{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1. Amazon SageMaker 학습 스크립트\n",
    "\n",
    "<p>이 예제는 LG에서 개발한 AI chip에서 동작할 수 있도록, Tensorflow 1.X, python2.7 버전에서 학습하기 위한 코드입니다. </p>\n",
    "<p>이 코드는 <strong><a href=\"https://github.com/tensorflow/models/tree/master/research/slim\" target=\"_blank\" class ='btn-default'>TensorFlow-Slim image classification model library</a></strong>를 참고하여 Sagemaker에서 학습할 수 있는 실행 스크립트로 수정하여 작성하였습니다. Amazon SageMaker로 실행 스크립트를 구성하는 이유는 노트북의 스크립트에서 일부 파라미터 수정으로 동일 모델 아키텍처에 대해 hyperparamter가 변경된 다양한 모델을 원하는 형태의 다수 인프라에서 동시에 학습 수행이 가능하며, 가장 높은 성능의 모델을 노트북 스크립트 내 명령어로 바로 hosting 서비스가 가능한 Endpoint 생성을 할 수 있습니다.</p>\n",
    "\n",
    "<p>이번 실습에서는 Amazon SageMaker가 어떤 방식으로 학습이 되는지 설명되는 구조와 함께 학습하는 방법을 간단하게 체험해 보는 시간을 갖도록 하겠습니다.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sagemaker notebook 설명\n",
    "<p>Sagemaker notebook은 완전 관리형 서비스로 컨테이너 기반으로 구성되어 있습니다. 사용자가 직접 컨테이너를 볼 수 없지만, 내부적으로는 아래와 같은 원리로 동작합니다. </p>\n",
    "<p><img src=\"./imgs/fig00.png\" width=\"700\", height=\"70\"></p>\n",
    "\n",
    "- **S3 (Simple Storage Serivce)** : Object Storage로서 학습할 데이터 파일과 학습 결과인 model, checkpoint, tensorboard를 위한 event 파일, 로그 정보 등을 저장하는데 사용합니다.\n",
    "- **SageMaker Notebook** : 학습을 위한 스크립트 작성과 디버깅, 그리고 실제 학습을 수행하기 위한 Python을 개발하기 위한 환경을 제공합니다.\n",
    "- **Amazon Elastic Container Registry(ECR)** :  Docker 컨테이너 이미지를 손쉽게 저장, 관리 및 배포할 수 있게 해주는 완전관리형 Docker 컨테이너 레지스트리입니다. Sagemaker는 기본적인 컨테이너를 제공하기 때문에 별도 ECR에 컨테이너 이미지를 등록할 필요는 없습니다. 하지만, 별도의 학습 및 배포 환경이 필요한 경우 custom 컨테이너 이미지를 만들어서 ECR에 등록한 후 이 환경을 활용할 수 있습니다.\n",
    "\n",
    "<p>학습과 추론을 하는 hosting 서비스는 각각 다른 컨테이너 환경에서 수행할 수 있으며, 쉽게 다량으로 컨테이너 환경을 확장할 수 있으므로 다량의 학습과 hosting이 동시에 가능합니다.   \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 환경 설정\n",
    "\n",
    "<p>Sagemaker 학습에 필요한 기본적인 package를 import 합니다. </p>\n",
    "<p>boto3는 HTTP API 호출을 숨기는 편한 추상화 모델을 가지고 있고, Amazon EC2 인스턴스 및 S3 버켓과 같은 AWS 리소스와 동작하는 파이선 클래스를 제공합니다. </p>\n",
    "<p>sagemaker python sdk는 Amazon SageMaker에서 기계 학습 모델을 교육 및 배포하기 위한 오픈 소스 라이브러리입니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install --upgrade pip\n",
    "# !{sys.executable} -m pip install tensorflow_gpu==1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sagemaker\n",
    "import boto3\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>SageMaker에서 앞으로 사용할 SageMaker Session 설정, Role 정보를 설정합니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. S3의 저장 데이터 위치 가져오기\n",
    "<p> 데이터를 정하기 위한 S3의 bucket 위치는 아래 data_bucket 이름으로 생성하며, 기본적으로 SageMaker에서 학습한 모델과 로그 정보를 남기는 위치는 자동으로 생성되는 bucket 이름으로 저장됩니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n"
     ]
    }
   ],
   "source": [
    "# create a s3 bucket to hold data, note that your account might already created a bucket with the same name\n",
    "account_id = sess.client('sts').get_caller_identity()[\"Account\"]\n",
    "data_bucket = 'sagemaker-experiments-{}-{}'.format(sess.region_name, account_id)\n",
    "bucket = 'sagemaker-{}-{}'.format(sess.region_name, account_id)\n",
    "\n",
    "try:\n",
    "    if sess.region_name == \"us-east-1\":\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket)\n",
    "    else:\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket, \n",
    "                                        CreateBucketConfiguration={'LocationConstraint': sess.region_name})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 이미지를 TFRecord 변경하기\n",
    "<p>이미지 파일을 학습하기 위해 SageMaker Notebook 환경으로 upload를 합니다. 폴더 구조는 아래와 같은 형태로 구성되어야 합니다. </p>\n",
    "<pre>\n",
    "<div style='line-height:80%'>\n",
    "    image_path/class1/Aimage_1<br>\n",
    "                      Aimage_2<br>\n",
    "                       ...<br>\n",
    "                      Aimage_N<br>\n",
    "    image_path/class2/Bimage_1<br>\n",
    "                      Bimage_2<br>\n",
    "                       ...<br>\n",
    "                      Bimage_M<br>\n",
    "</div>\n",
    "</pre>\n",
    "<p>생성된 TFRecord 파일은 아래 정의하신 dataset_dir에 저장이 됩니다. train과 test의 데이터 수는 향후 학습에서 활용하기 위해 train_num_data, test_num_data 변수에 저장합니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/ec2-user/SageMaker/PUBLIC-IOT-ML/src_dir/')\n",
    "sys.path.append('/home/ec2-user/SageMaker/git_dir/PUBLIC-IOT-ML/src_dir/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import image_to_tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/home/ec2-user/SageMaker/PUBLIC-IOT-ML/img_datasets'\n",
    "image_path = '/home/ec2-user/SageMaker/PUBLIC-IOT-ML/data'\n",
    "\n",
    "dataset_dir = '/home/ec2-user/SageMaker/git_dir/PUBLIC-IOT-ML/img_datasets'\n",
    "image_path = '/home/ec2-user/SageMaker/git_dir/PUBLIC-IOT-ML/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/git_dir/PUBLIC-IOT-ML/src_dir/datasets/image_to_tfrecord.py:157: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/git_dir/PUBLIC-IOT-ML/src_dir/datasets/image_to_tfrecord.py:161: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/git_dir/PUBLIC-IOT-ML/src_dir/datasets/dataset_utils.py:176: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/git_dir/PUBLIC-IOT-ML/src_dir/datasets/image_to_tfrecord.py:196: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/git_dir/PUBLIC-IOT-ML/src_dir/datasets/image_to_tfrecord.py:73: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/git_dir/PUBLIC-IOT-ML/src_dir/datasets/image_to_tfrecord.py:76: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "\n",
      "Finished converting the image dataset!\n"
     ]
    }
   ],
   "source": [
    "train_num_data, test_num_data = image_to_tfrecord.run(image_path, dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TFRecord를 S3에 upload 하기\n",
    "\n",
    "<p>SageMaker 학습을 위해 TFRecord 파일을 S3에 upload합니다. TFRecord 은 이전에 지정한 data_bucket 내 prefix 하위 폴더에 저장됩니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: img_datasets/labels.txt to s3://sagemaker-experiments-us-east-2-322537213286/captured_data/tfrecord/labels.txt\n",
      "upload: img_datasets/captureddata_val.tfrecord to s3://sagemaker-experiments-us-east-2-322537213286/captured_data/tfrecord/captureddata_val.tfrecord\n",
      "upload: img_datasets/captureddata_train.tfrecord to s3://sagemaker-experiments-us-east-2-322537213286/captured_data/tfrecord/captureddata_train.tfrecord\n"
     ]
    }
   ],
   "source": [
    "prefix = 'captured_data/tfrecord'\n",
    "!aws s3 cp $dataset_dir s3://{data_bucket}/{prefix}/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 학습 스크립트 코딩하기\n",
    "\n",
    "<p>SageMaker에서 학습하는 것이 아니더라도 실제 모델 아키텍처와 학습을 위한 optimizer와 loss 함수 등을 정의하는 python 파일을 구성하게 됩니다. SageMaker에서 활용하는 python 파일도 동일한 python 파일을 사용하게 됩니다. 연계되는 다른 소스코드 파일이 있는 경우에도 별도 소스코드 수정 없이 학습이 가능하며, SageMaker에서 사용하기 위해서는 기존 python 파일에 SageMaker 학습에 사용할 수 있는 환경변수들만 추가하면 됩니다. 예를 들어, 환경변수 중 <code>SM_MODEL_DIR</code>은 컨테이너 환경에서는 <code>/opt/ml/model</code>를 의미합니다. 다양한 환경변수는 <strong><a href=\"https://github.com/aws/sagemaker-containers\" target=\"_blank\" class ='btn-default'>SageMaker Containers-IMPORTANT ENVIRONMENT VARIABLES</a></strong>를 참고하시기 바랍니다. </p><p>SageMaker 학습이 끝나면 자동은 컨테이너 환경은 삭제가 됩니다. 따라서, 학습이 완료된 모델 산출물과 다양한 output 파일은 S3로 저장해야 합니다. SageMaker는 자동으로 <code>SM_MODEL_DIR</code>에 저장된 최종 모델 파일을 학습이 끝난 다음 model.tar.gz로 압축하여 컨테이너 환경에서 S3의 특정 bucket에 저장하게 됩니다.</p><p> 별도 bucket을 설정하지 않으며, 기본적으로 생성되는 bucket에 저장됩니다. 이외 학습에 이용되는 python source code는 SageMaker 학습이 시작되면서 S3에 저장되며, 별도로 <code>SM_MODEL_DIR</code>에 checkpoint 또는 log 파일을 저장하게 되면 학습이 끝난 이후 자동으로 컨테이너 환경에서 S3로 저장된 파일들이 이동하게 됩니다. 이런 과정을 이해한다면 학습 시 저장되는 다양한 정보들을 저장한 다음 학습이 끝난 후 S3에서 download 받아 활용할 수 있습니다. </p>\n",
    "\n",
    "<p>아래는 시간 관계 상 미리 작성한 python 학습 스크립트 코드 입니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;49;00m\r\n",
      "\u001b[37m# you may not use this file except in compliance with the License.\u001b[39;49;00m\r\n",
      "\u001b[37m# You may obtain a copy of the License at\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# http://www.apache.org/licenses/LICENSE-2.0\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# Unless required by applicable law or agreed to in writing, software\u001b[39;49;00m\r\n",
      "\u001b[37m# distributed under the License is distributed on an \"AS IS\" BASIS,\u001b[39;49;00m\r\n",
      "\u001b[37m# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[39;49;00m\r\n",
      "\u001b[37m# See the License for the specific language governing permissions and\u001b[39;49;00m\r\n",
      "\u001b[37m# limitations under the License.\u001b[39;49;00m\r\n",
      "\u001b[37m# ==============================================================================\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"Generic training script that trains a model using a given dataset.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m absolute_import, division, print_function\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcodecs\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mPIL\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mfreeze_graph\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mfg\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m dataset_factory\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdeployment\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m model_deploy\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mnets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m nets_factory\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m preprocessing_factory\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcontrib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m quantize \u001b[34mas\u001b[39;49;00m contrib_quantize\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcontrib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m slim \u001b[34mas\u001b[39;49;00m contrib_slim\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcore\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mprotobuf\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m saver_pb2\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpython\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mplatform\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gfile\r\n",
      "\r\n",
      "slim = contrib_slim\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m###############################\u001b[39;49;00m\r\n",
      "    \u001b[37m# SageMaker Default Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m###############################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mDirectory where checkpoints and event logs are written to.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dataset_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m,\r\n",
      "                        default=json.loads(os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-config\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=json.loads,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_INPUT_DATA_CONFIG\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--fw-params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=json.loads,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_FRAMEWORK_PARAMS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "\r\n",
      "    \u001b[37m###############################\u001b[39;49;00m\r\n",
      "    \u001b[37m#  Default Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m###############################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--master\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe address of the TensorFlow master to use.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--warmup_epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mLinearly warmup learning rate from 0 to learning_rate over this many epochs.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_clones\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mNumber of model clones to deploy. Note For historical reasons\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mloss from all clones averaged out and learning rate decay happen per clone epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--clone_on_cpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mUse CPUs to deploy clones.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--worker_replicas\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m1\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mNumber of worker replicas.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_ps_tasks\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of parameter servers. If the value is 0,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mthen the parameters are handled locally by the worker.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--task\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mTask id of the replica running the training.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--save_interval_secs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m600\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe frequency with which the model is saved, in seconds.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--save_summaries_secs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m600\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe frequency with which summaries are saved, in seconds.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--log_every_n_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe frequency with which logs are print.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_preprocessing_threads\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of threads used to create the batches.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_readers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of parallel readers that read data from the dataset.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m##########################\u001b[39;49;00m\r\n",
      "    \u001b[37m# Optimization Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m##########################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--adam_beta1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe exponential decay rate for the 1st moment estimates.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--adam_beta2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.999\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe exponential decay rate for the 2nd moment estimates.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--adagrad_initial_accumulator_value\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.1\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mStarting value for the AdaGrad accumulators.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--adadelta_rho\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.95\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe decay rate for adadelta.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--optimizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mrmsprop\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe name of the optimizer, one of \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33madadelta\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33madagrad\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mftrl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mmomentum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33msgd\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m or \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mrmsprop\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--weight_decay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.00004\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe weight decay on the model weights.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--opt_epsilon\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m1.0\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mEpsilon term for the optimizer.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ftrl_learning_rate_power\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=-\u001b[34m0.5\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe learning rate power.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ftrl_initial_accumulator_value\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.1\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mStarting value for the FTRL accumulators.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ftrl_l1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe FTRL l1 regularization strength.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ftrl_l2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe FTRL l2 regularization strength.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe momentum for the MomentumOptimizer and RMSPropOptimizer.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--rmsprop_momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.9\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mMomentum.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--rmsprop_decay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.9\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mDecay term for RMSProp.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--quantize_delay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=-\u001b[34m1\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mNumber of steps to start quantized training.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mSet to -1 would disable quantized training.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m###########################\u001b[39;49;00m\r\n",
      "    \u001b[37m# Learning Rate Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m###########################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate_decay_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mexponential\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mSpecifies how the learning rate is decayed. One of \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mfixed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mexponential\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m or \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mpolynomial\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.01\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mInitial learning rate.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--end_learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.01\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mInitial learning rate.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--label_smoothing\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.1\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe amount of label smoothing.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate_decay_factor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.94\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mLearning rate decay factor.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_epochs_per_decay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m2.0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mNumber of epochs after which learning rate decays. \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mNote: this flag counts epochs per clone but aggregates per sync replicas.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mSo 1.0 means that each clone will go over full epoch individually, \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mbut replicas will go once across all replicas.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sync_replicas\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhether or not to synchronize the replicas during training.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--replicas_to_aggregate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m1\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe Number of gradients to collect before updating params.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--moving_average_decay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe decay to use for the moving average.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33m If left as None, then moving averages are not used.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m#####################\u001b[39;49;00m\r\n",
      "    \u001b[37m# Dataset Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m#####################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dataset_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[33m'\u001b[39;49;00m\u001b[33mimagenet\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe name of the dataset to load.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[37m# parser.add_argument('--dataset_split_name', type=str,\u001b[39;49;00m\r\n",
      "    \u001b[37m#                     default='train', help='The name of the train/test split.')\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--labels_offset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mAn offset for the labels in the dataset.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mThis flag is primarily used to evaluate the VGG and ResNet \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33marchitectures which do not use a background class for the ImageNet dataset.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[33m'\u001b[39;49;00m\u001b[33minception_v3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe name of the architecture to train.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--preprocessing_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe name of the preprocessing to use.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mIf left as `None`, then the model_name flag is used.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m32\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of samples in each batch.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--image_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mTrain image size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_number_of_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe maximum number of training steps.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_grayscale\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhether to convert input images to grayscale.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m#########################\u001b[39;49;00m\r\n",
      "    \u001b[37m# Fine-Tuning Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m#########################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--finetune_checkpoint_path\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe path to a checkpoint from which to fine-tune.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--checkpoint_exclude_scopes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of scopes of variables\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mto exclude when restoring from a checkpoint.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--trainable_scopes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of scopes to filter the set\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mof variables to train. By default, None would train all the variables.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ignore_missing_vars\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhen restoring a checkpoint would ignore missing variables.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m#########################\u001b[39;49;00m\r\n",
      "    \u001b[37m# evaluation Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m#########################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m100\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of samples in each batch in evaluation.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_num_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of train samples\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_num_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of test samples\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_eval_num_batches\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mMax number of batches to evaluate by default use all.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[37m# parser.add_argument('--num_preprocessing_threads', type=int,\u001b[39;49;00m\r\n",
      "    \u001b[37m#                     default=4, help='The number of threads used to create the batches.')\u001b[39;49;00m\r\n",
      "    \u001b[37m# parser.add_argument('--eval_image_size', type=int,\u001b[39;49;00m\r\n",
      "    \u001b[37m#                     default=None, help='Eval image size')\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--quantize\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mEval image size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--is_training\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhether to save out a training-focused version of the model.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--is_video_model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mwhether to use 5-D inputs for video model.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_frames\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of frames to use. Only used if is_video_model is True.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--write_text_graphdef\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhether to write a text version of graphdef.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    return_value = parser.parse_known_args()\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mparser.parse_known_args() : \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(return_value))\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m return_value\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_configure_learning_rate\u001b[39;49;00m(args, num_samples_per_epoch, global_step):\r\n",
      "    \u001b[33m\"\"\"Configures the learning rate.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m      num_samples_per_epoch: The number of samples in each epoch of training.\u001b[39;49;00m\r\n",
      "\u001b[33m      global_step: The global_step tensor.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m      A `Tensor` representing the learning rate.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Raises:\u001b[39;49;00m\r\n",
      "\u001b[33m      ValueError: if\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[37m# Note: when num_clones is > 1, this will actually have each clone to go\u001b[39;49;00m\r\n",
      "    \u001b[37m# over each epoch args.num_epochs_per_decay times. This is different\u001b[39;49;00m\r\n",
      "    \u001b[37m# behavior from sync replicas and is expected to produce different results.\u001b[39;49;00m\r\n",
      "    steps_per_epoch = num_samples_per_epoch / args.batch_size\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.sync_replicas:\r\n",
      "        steps_per_epoch /= args.replicas_to_aggregate\r\n",
      "\r\n",
      "    decay_steps = \u001b[36mint\u001b[39;49;00m(steps_per_epoch * args.num_epochs_per_decay)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.learning_rate_decay_type == \u001b[33m'\u001b[39;49;00m\u001b[33mexponential\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        learning_rate = tf.train.exponential_decay(\r\n",
      "            args.learning_rate,\r\n",
      "            global_step,\r\n",
      "            decay_steps,\r\n",
      "            args.learning_rate_decay_factor,\r\n",
      "            staircase=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "            name=\u001b[33m'\u001b[39;49;00m\u001b[33mexponential_decay_learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.learning_rate_decay_type == \u001b[33m'\u001b[39;49;00m\u001b[33mfixed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        learning_rate = tf.constant(\r\n",
      "            args.learning_rate, name=\u001b[33m'\u001b[39;49;00m\u001b[33mfixed_learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.learning_rate_decay_type == \u001b[33m'\u001b[39;49;00m\u001b[33mpolynomial\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        learning_rate = tf.train.polynomial_decay(\r\n",
      "            args.learning_rate,\r\n",
      "            global_step,\r\n",
      "            decay_steps,\r\n",
      "            args.end_learning_rate,\r\n",
      "            power=\u001b[34m1.0\u001b[39;49;00m,\r\n",
      "            cycle=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "            name=\u001b[33m'\u001b[39;49;00m\u001b[33mpolynomial_decay_learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate_decay_type [\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m] was not recognized\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m %\r\n",
      "                         args.learning_rate_decay_type)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.warmup_epochs:\r\n",
      "        warmup_lr = (\r\n",
      "            args.learning_rate * tf.cast(global_step, tf.float32) /\r\n",
      "            (steps_per_epoch * args.warmup_epochs))\r\n",
      "        learning_rate = tf.minimum(warmup_lr, learning_rate)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m learning_rate\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_configure_optimizer\u001b[39;49;00m(args, learning_rate):\r\n",
      "    \u001b[33m\"\"\"Configures the optimizer used for training.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m      learning_rate: A scalar or `Tensor` learning rate.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m      An instance of an optimizer.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Raises:\u001b[39;49;00m\r\n",
      "\u001b[33m      ValueError: if args.optimizer is not recognized.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33madadelta\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.AdadeltaOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            rho=args.adadelta_rho,\r\n",
      "            epsilon=args.opt_epsilon)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33madagrad\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.AdagradOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            initial_accumulator_value=args.adagrad_initial_accumulator_value)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.AdamOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            beta1=args.adam_beta1,\r\n",
      "            beta2=args.adam_beta2,\r\n",
      "            epsilon=args.opt_epsilon)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33mftrl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.FtrlOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            learning_rate_power=args.ftrl_learning_rate_power,\r\n",
      "            initial_accumulator_value=args.ftrl_initial_accumulator_value,\r\n",
      "            l1_regularization_strength=args.ftrl_l1,\r\n",
      "            l2_regularization_strength=args.ftrl_l2)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33mmomentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.MomentumOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            momentum=args.momentum,\r\n",
      "            name=\u001b[33m'\u001b[39;49;00m\u001b[33mMomentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33mrmsprop\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.RMSPropOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            decay=args.rmsprop_decay,\r\n",
      "            momentum=args.rmsprop_momentum,\r\n",
      "            epsilon=args.opt_epsilon)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33msgd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mOptimizer [\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m] was not recognized\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % args.optimizer)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m optimizer\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_init_fn\u001b[39;49;00m(args):\r\n",
      "    \u001b[33m\"\"\"Returns a function run by the chief worker to warm-start the training.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Note that the init_fn is only run when initializing the model during the very\u001b[39;49;00m\r\n",
      "\u001b[33m    first global step.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m      An init function run by the supervisor.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.finetune_checkpoint_path \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Warn the user if a checkpoint exists in the train_dir. Then we'll be\u001b[39;49;00m\r\n",
      "    \u001b[37m# ignoring the checkpoint anyway.\u001b[39;49;00m\r\n",
      "    \u001b[37m# if tf.train.latest_checkpoint(args.finetune_checkpoint_path):\u001b[39;49;00m\r\n",
      "    \u001b[37m#     tf.logging.info(\u001b[39;49;00m\r\n",
      "    \u001b[37m#         'Ignoring --checkpoint_path because a checkpoint already exists in %s'\u001b[39;49;00m\r\n",
      "    \u001b[37m#         % args.finetune_checkpoint_path)\u001b[39;49;00m\r\n",
      "    \u001b[37m#     return None\u001b[39;49;00m\r\n",
      "\r\n",
      "    exclusions = []\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.checkpoint_exclude_scopes:\r\n",
      "        exclusions = [scope.strip()\r\n",
      "                      \u001b[34mfor\u001b[39;49;00m scope \u001b[35min\u001b[39;49;00m args.checkpoint_exclude_scopes.split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)]\r\n",
      "\r\n",
      "    \u001b[37m# TODO(sguada) variables.filter_variables()\u001b[39;49;00m\r\n",
      "    variables_to_restore = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m var \u001b[35min\u001b[39;49;00m slim.get_model_variables():\r\n",
      "        \u001b[34mfor\u001b[39;49;00m exclusion \u001b[35min\u001b[39;49;00m exclusions:\r\n",
      "            \u001b[34mif\u001b[39;49;00m var.op.name.startswith(exclusion):\r\n",
      "                \u001b[34mbreak\u001b[39;49;00m\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            variables_to_restore.append(var)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m tf.gfile.IsDirectory(args.finetune_checkpoint_path):\r\n",
      "        checkpoint_path = tf.train.latest_checkpoint(\r\n",
      "            args.finetune_checkpoint_path)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        checkpoint_path = args.finetune_checkpoint_path\r\n",
      "\r\n",
      "    tf.logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mFine-tuning from \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % checkpoint_path)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m slim.assign_from_checkpoint_fn(\r\n",
      "        checkpoint_path,\r\n",
      "        variables_to_restore,\r\n",
      "        ignore_missing_vars=args.ignore_missing_vars)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_variables_to_train\u001b[39;49;00m(args):\r\n",
      "    \u001b[33m\"\"\"Returns a list of variables to train.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m      A list of variables to train by the optimizer.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.trainable_scopes \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m tf.trainable_variables()\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        scopes = [scope.strip() \u001b[34mfor\u001b[39;49;00m scope \u001b[35min\u001b[39;49;00m args.trainable_scopes.split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)]\r\n",
      "\r\n",
      "    variables_to_train = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m scope \u001b[35min\u001b[39;49;00m scopes:\r\n",
      "        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\r\n",
      "        variables_to_train.extend(variables)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m variables_to_train\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mevaluation\u001b[39;49;00m(args):\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.dataset_dir:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mYou must supply the dataset directory with --dataset_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    tf.logging.set_verbosity(tf.logging.INFO)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.Graph().as_default():\r\n",
      "        tf_global_step = slim.get_or_create_global_step()\r\n",
      "\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the dataset #\u001b[39;49;00m\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        dataset = dataset_factory.get_dataset(\r\n",
      "            args.dataset_name, \u001b[33m'\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.dataset_dir, args.test_num_data)\r\n",
      "\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the model #\u001b[39;49;00m\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        network_fn = nets_factory.get_network_fn(\r\n",
      "            args.model_name,\r\n",
      "            num_classes=(dataset.num_classes - args.labels_offset),\r\n",
      "            is_training=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m##############################################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Create a dataset provider that loads data from the dataset #\u001b[39;49;00m\r\n",
      "        \u001b[37m##############################################################\u001b[39;49;00m\r\n",
      "        provider = slim.dataset_data_provider.DatasetDataProvider(\r\n",
      "            dataset,\r\n",
      "            shuffle=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "            common_queue_capacity=\u001b[34m2\u001b[39;49;00m * args.eval_batch_size,\r\n",
      "            common_queue_min=args.eval_batch_size)\r\n",
      "        [image, label] = provider.get([\u001b[33m'\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "        label -= args.labels_offset\r\n",
      "\r\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the preprocessing function #\u001b[39;49;00m\r\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\r\n",
      "        preprocessing_name = args.preprocessing_name \u001b[35mor\u001b[39;49;00m args.model_name\r\n",
      "        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33meval_args.use_grayscale : \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.use_grayscale))\r\n",
      "        \r\n",
      "        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\r\n",
      "            preprocessing_name,\r\n",
      "            is_training=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "            use_grayscale=args.use_grayscale)\r\n",
      "\r\n",
      "        eval_image_size = args.image_size \u001b[35mor\u001b[39;49;00m network_fn.default_image_size\r\n",
      "\r\n",
      "        image = image_preprocessing_fn(image, eval_image_size, eval_image_size)\r\n",
      "\r\n",
      "        images, labels = tf.train.batch(\r\n",
      "            [image, label],\r\n",
      "            batch_size=args.eval_batch_size,\r\n",
      "            num_threads=args.num_preprocessing_threads,\r\n",
      "            capacity=\u001b[34m5\u001b[39;49;00m * args.eval_batch_size)\r\n",
      "\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Define the model #\u001b[39;49;00m\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        logits, _ = network_fn(images)\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.quantize:\r\n",
      "            contrib_quantize.create_eval_graph()\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.moving_average_decay:\r\n",
      "            variable_averages = tf.train.ExponentialMovingAverage(\r\n",
      "                args.moving_average_decay, tf_global_step)\r\n",
      "            variables_to_restore = variable_averages.variables_to_restore(\r\n",
      "                slim.get_model_variables())\r\n",
      "            variables_to_restore[tf_global_step.op.name] = tf_global_step\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            variables_to_restore = slim.get_variables_to_restore()\r\n",
      "\r\n",
      "        predictions = tf.argmax(logits, \u001b[34m1\u001b[39;49;00m)\r\n",
      "        labels = tf.squeeze(labels)\r\n",
      "\r\n",
      "        \u001b[37m# Define the metrics:\u001b[39;49;00m\r\n",
      "        names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mAccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: slim.metrics.streaming_accuracy(predictions, labels),\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mRecall_5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: slim.metrics.streaming_recall_at_k(\r\n",
      "                logits, labels, \u001b[34m5\u001b[39;49;00m),\r\n",
      "        })\r\n",
      "\r\n",
      "        \u001b[37m# Print the summaries to screen.\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m name, value \u001b[35min\u001b[39;49;00m names_to_values.items():\r\n",
      "            summary_name = \u001b[33m'\u001b[39;49;00m\u001b[33meval/\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % name\r\n",
      "            op = tf.summary.scalar(summary_name, value, collections=[])\r\n",
      "            op = tf.Print(op, [value], summary_name)\r\n",
      "            tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\r\n",
      "\r\n",
      "        \u001b[37m# TODO(sguada) use num_epochs=1\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.max_eval_num_batches:\r\n",
      "            num_batches = args.max_eval_num_batches\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[37m# This ensures that we make a single pass over all of the data.\u001b[39;49;00m\r\n",
      "            num_batches = math.ceil(\r\n",
      "                dataset.num_samples / \u001b[36mfloat\u001b[39;49;00m(args.eval_batch_size))\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m tf.gfile.IsDirectory(args.train_dir):\r\n",
      "            checkpoint_path = tf.train.latest_checkpoint(args.train_dir)\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            checkpoint_path = args.train_dir\r\n",
      "\r\n",
      "        tf.logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mEvaluating \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % checkpoint_path)\r\n",
      "\r\n",
      "        slim.evaluation.evaluate_once(\r\n",
      "            master=args.master,\r\n",
      "            checkpoint_path=checkpoint_path,\r\n",
      "            logdir=args.train_dir,\r\n",
      "            num_evals=num_batches,\r\n",
      "            eval_op=\u001b[36mlist\u001b[39;49;00m(names_to_updates.values()),\r\n",
      "            variables_to_restore=variables_to_restore)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mexport_inference_graph\u001b[39;49;00m(args):\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.train_dir:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mYou must supply the path to save to with --train_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.is_video_model \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.num_frames:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mNumber of frames must be specified for video models with --num_frames\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    tf.logging.set_verbosity(tf.logging.INFO)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.Graph().as_default() \u001b[34mas\u001b[39;49;00m graph:\r\n",
      "        dataset = dataset_factory.get_dataset(args.dataset_name, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                              args.dataset_dir, args.train_num_data)\r\n",
      "        network_fn = nets_factory.get_network_fn(\r\n",
      "            args.model_name,\r\n",
      "            num_classes=(dataset.num_classes - args.labels_offset),\r\n",
      "            is_training=args.is_training)\r\n",
      "        image_size = args.image_size \u001b[35mor\u001b[39;49;00m network_fn.default_image_size\r\n",
      "        num_channels = \u001b[34m1\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m args.use_grayscale \u001b[34melse\u001b[39;49;00m \u001b[34m3\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.is_video_model:\r\n",
      "            input_shape = [\r\n",
      "                \u001b[34m1\u001b[39;49;00m, args.num_frames, image_size, image_size,\r\n",
      "                num_channels\r\n",
      "            ]\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            input_shape = [\u001b[34m1\u001b[39;49;00m,\r\n",
      "                           image_size, image_size, num_channels]\r\n",
      "        placeholder = tf.placeholder(name=\u001b[33m'\u001b[39;49;00m\u001b[33minput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, dtype=tf.float32,\r\n",
      "                                     shape=input_shape)\r\n",
      "        network_fn(placeholder)\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.quantize:\r\n",
      "            contrib_quantize.create_eval_graph()\r\n",
      "\r\n",
      "        graph_def = graph.as_graph_def()\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.write_text_graphdef:\r\n",
      "            tf.io.write_graph(\r\n",
      "                graph_def,\r\n",
      "                os.path.dirname(args.train_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/inference_graph.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "                os.path.basename(args.train_dir),\r\n",
      "                as_text=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[34mwith\u001b[39;49;00m gfile.GFile(args.train_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/inference_graph.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "                f.write(graph_def.SerializeToString())\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mfreeze_graph\u001b[39;49;00m(args):\r\n",
      "    checkpoint_version = saver_pb2.SaverDef.V2\r\n",
      "    input_graph = args.train_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/inference_graph.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    input_checkpoint = tf.train.latest_checkpoint(args.train_dir)\r\n",
      "    input_binary = \u001b[34mTrue\u001b[39;49;00m\r\n",
      "    output_graph = args.train_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/inference_graph_frozen.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    output_node_names = \u001b[33m'\u001b[39;49;00m\u001b[33mMobilenetV1/Predictions/Reshape_1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    input_saved_model_dir = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    saved_model_tags = \u001b[33m\"\u001b[39;49;00m\u001b[33mserve\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    input_meta_graph = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    variable_names_blacklist = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    variable_names_whitelist = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    initializer_nodes = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    clear_devices = \u001b[34mTrue\u001b[39;49;00m\r\n",
      "    filename_tensor_name = \u001b[33m\"\u001b[39;49;00m\u001b[33msave/Const:0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    restore_op_name = \u001b[33m\"\u001b[39;49;00m\u001b[33msave/restore_all\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    input_saver = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mfreeze_graph input_checkpoint : \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(input_checkpoint))\r\n",
      "\r\n",
      "    fg.freeze_graph(input_graph, input_saver, input_binary,\r\n",
      "                    input_checkpoint, output_node_names,\r\n",
      "                    restore_op_name, filename_tensor_name,\r\n",
      "                    output_graph, clear_devices, initializer_nodes,\r\n",
      "                    variable_names_whitelist, variable_names_blacklist,\r\n",
      "                    input_meta_graph, input_saved_model_dir,\r\n",
      "                    saved_model_tags, checkpoint_version)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\r\n",
      "    args, unknown = parse_args()\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.dataset_dir:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mYou must supply the dataset directory with --dataset_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    tf.logging.set_verbosity(tf.logging.INFO)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.Graph().as_default():\r\n",
      "        \u001b[37m#######################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Config model_deploy #\u001b[39;49;00m\r\n",
      "        \u001b[37m#######################\u001b[39;49;00m\r\n",
      "        deploy_config = model_deploy.DeploymentConfig(\r\n",
      "            num_clones=args.num_clones,\r\n",
      "            clone_on_cpu=args.clone_on_cpu,\r\n",
      "            replica_id=args.task,\r\n",
      "            num_replicas=args.worker_replicas,\r\n",
      "            num_ps_tasks=args.num_ps_tasks)\r\n",
      "\r\n",
      "        \u001b[37m# Create global_step\u001b[39;49;00m\r\n",
      "        \u001b[34mwith\u001b[39;49;00m tf.device(deploy_config.variables_device()):\r\n",
      "            global_step = slim.create_global_step()\r\n",
      "\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the dataset #\u001b[39;49;00m\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        dataset = dataset_factory.get_dataset(\r\n",
      "            args.dataset_name, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.dataset_dir, args.train_num_data)\r\n",
      "\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the network #\u001b[39;49;00m\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        network_fn = nets_factory.get_network_fn(\r\n",
      "            args.model_name,\r\n",
      "            num_classes=(dataset.num_classes - args.labels_offset),\r\n",
      "            weight_decay=args.weight_decay,\r\n",
      "            is_training=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the preprocessing function #\u001b[39;49;00m\r\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\r\n",
      "        preprocessing_name = args.preprocessing_name \u001b[35mor\u001b[39;49;00m args.model_name\r\n",
      "        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_args.use_grayscale : \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.use_grayscale))\r\n",
      "        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\r\n",
      "            preprocessing_name,\r\n",
      "            is_training=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "            use_grayscale=args.use_grayscale)\r\n",
      "\r\n",
      "        \u001b[37m##############################################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Create a dataset provider that loads data from the dataset #\u001b[39;49;00m\r\n",
      "        \u001b[37m##############################################################\u001b[39;49;00m\r\n",
      "        \u001b[34mwith\u001b[39;49;00m tf.device(deploy_config.inputs_device()):\r\n",
      "            provider = slim.dataset_data_provider.DatasetDataProvider(\r\n",
      "                dataset,\r\n",
      "                num_readers=args.num_readers,\r\n",
      "                common_queue_capacity=\u001b[34m20\u001b[39;49;00m * args.batch_size,\r\n",
      "                common_queue_min=\u001b[34m10\u001b[39;49;00m * args.batch_size)\r\n",
      "            [image, label] = provider.get([\u001b[33m'\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "            label -= args.labels_offset\r\n",
      "\r\n",
      "            train_image_size = args.image_size \u001b[35mor\u001b[39;49;00m network_fn.default_image_size\r\n",
      "\r\n",
      "            image = image_preprocessing_fn(\r\n",
      "                image, train_image_size, train_image_size)\r\n",
      "\r\n",
      "            images, labels = tf.train.batch(\r\n",
      "                [image, label],\r\n",
      "                batch_size=args.batch_size,\r\n",
      "                num_threads=args.num_preprocessing_threads,\r\n",
      "                capacity=\u001b[34m5\u001b[39;49;00m * args.batch_size)\r\n",
      "            labels = slim.one_hot_encoding(\r\n",
      "                labels, dataset.num_classes - args.labels_offset)\r\n",
      "            batch_queue = slim.prefetch_queue.prefetch_queue(\r\n",
      "                [images, labels], capacity=\u001b[34m2\u001b[39;49;00m * deploy_config.num_clones)\r\n",
      "\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Define the model #\u001b[39;49;00m\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32mclone_fn\u001b[39;49;00m(args, batch_queue):\r\n",
      "            \u001b[33m\"\"\"Allows data parallelism by creating multiple clones of network_fn.\"\"\"\u001b[39;49;00m\r\n",
      "            images, labels = batch_queue.dequeue()\r\n",
      "            logits, end_points = network_fn(images)\r\n",
      "\r\n",
      "            \u001b[37m#############################\u001b[39;49;00m\r\n",
      "            \u001b[37m# Specify the loss function #\u001b[39;49;00m\r\n",
      "            \u001b[37m#############################\u001b[39;49;00m\r\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mAuxLogits\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m end_points:\r\n",
      "                slim.losses.softmax_cross_entropy(\r\n",
      "                    end_points[\u001b[33m'\u001b[39;49;00m\u001b[33mAuxLogits\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], labels,\r\n",
      "                    label_smoothing=args.label_smoothing, weights=\u001b[34m0.4\u001b[39;49;00m,\r\n",
      "                    scope=\u001b[33m'\u001b[39;49;00m\u001b[33maux_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            slim.losses.softmax_cross_entropy(\r\n",
      "                logits, labels, label_smoothing=args.label_smoothing, weights=\u001b[34m1.0\u001b[39;49;00m)\r\n",
      "            \u001b[34mreturn\u001b[39;49;00m end_points\r\n",
      "\r\n",
      "        \u001b[37m# Gather initial summaries.\u001b[39;49;00m\r\n",
      "        summaries = \u001b[36mset\u001b[39;49;00m(tf.get_collection(tf.GraphKeys.SUMMARIES))\r\n",
      "\r\n",
      "        clones = model_deploy.create_clones(\r\n",
      "            deploy_config, clone_fn, [args, batch_queue])\r\n",
      "        first_clone_scope = deploy_config.clone_scope(\u001b[34m0\u001b[39;49;00m)\r\n",
      "        \u001b[37m# Gather update_ops from the first clone. These contain, for example,\u001b[39;49;00m\r\n",
      "        \u001b[37m# the updates for the batch_norm variables created by network_fn.\u001b[39;49;00m\r\n",
      "        update_ops = tf.get_collection(\r\n",
      "            tf.GraphKeys.UPDATE_OPS, first_clone_scope)\r\n",
      "\r\n",
      "        \u001b[37m# Add summaries for end_points.\u001b[39;49;00m\r\n",
      "        end_points = clones[\u001b[34m0\u001b[39;49;00m].outputs\r\n",
      "        \u001b[34mfor\u001b[39;49;00m end_point \u001b[35min\u001b[39;49;00m end_points:\r\n",
      "            x = end_points[end_point]\r\n",
      "            summaries.add(tf.summary.histogram(\u001b[33m'\u001b[39;49;00m\u001b[33mactivations/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + end_point, x))\r\n",
      "            summaries.add(tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33msparsity/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + end_point,\r\n",
      "                                            tf.nn.zero_fraction(x)))\r\n",
      "\r\n",
      "        \u001b[37m# Add summaries for losses.\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m loss \u001b[35min\u001b[39;49;00m tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\r\n",
      "            summaries.add(tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mlosses/\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % loss.op.name, loss))\r\n",
      "\r\n",
      "        \u001b[37m# Add summaries for variables.\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m variable \u001b[35min\u001b[39;49;00m slim.get_model_variables():\r\n",
      "            summaries.add(tf.summary.histogram(variable.op.name, variable))\r\n",
      "\r\n",
      "        \u001b[37m#################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Configure the moving averages #\u001b[39;49;00m\r\n",
      "        \u001b[37m#################################\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.moving_average_decay:\r\n",
      "            moving_average_variables = slim.get_model_variables()\r\n",
      "            variable_averages = tf.train.ExponentialMovingAverage(\r\n",
      "                args.moving_average_decay, global_step)\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            moving_average_variables, variable_averages = \u001b[34mNone\u001b[39;49;00m, \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.quantize_delay >= \u001b[34m0\u001b[39;49;00m:\r\n",
      "            contrib_quantize.create_training_graph(\r\n",
      "                quant_delay=args.quantize_delay)\r\n",
      "\r\n",
      "        \u001b[37m#########################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Configure the optimization procedure. #\u001b[39;49;00m\r\n",
      "        \u001b[37m#########################################\u001b[39;49;00m\r\n",
      "        \u001b[34mwith\u001b[39;49;00m tf.device(deploy_config.optimizer_device()):\r\n",
      "            learning_rate = _configure_learning_rate(args,\r\n",
      "                                                     dataset.num_samples, global_step)\r\n",
      "            optimizer = _configure_optimizer(args, learning_rate)\r\n",
      "            summaries.add(tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, learning_rate))\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.sync_replicas:\r\n",
      "            \u001b[37m# If sync_replicas is enabled, the averaging will be done in the chief\u001b[39;49;00m\r\n",
      "            \u001b[37m# queue runner.\u001b[39;49;00m\r\n",
      "            optimizer = tf.train.SyncReplicasOptimizer(\r\n",
      "                opt=optimizer,\r\n",
      "                replicas_to_aggregate=args.replicas_to_aggregate,\r\n",
      "                total_num_replicas=args.worker_replicas,\r\n",
      "                variable_averages=variable_averages,\r\n",
      "                variables_to_average=moving_average_variables)\r\n",
      "        \u001b[34melif\u001b[39;49;00m args.moving_average_decay:\r\n",
      "            \u001b[37m# Update ops executed locally by trainer.\u001b[39;49;00m\r\n",
      "            update_ops.append(variable_averages.apply(\r\n",
      "                moving_average_variables))\r\n",
      "\r\n",
      "        \u001b[37m# Variables to train.\u001b[39;49;00m\r\n",
      "        variables_to_train = _get_variables_to_train(args)\r\n",
      "\r\n",
      "        \u001b[37m#  and returns a train_tensor and summary_op\u001b[39;49;00m\r\n",
      "        total_loss, clones_gradients = model_deploy.optimize_clones(\r\n",
      "            clones,\r\n",
      "            optimizer,\r\n",
      "            var_list=variables_to_train)\r\n",
      "        \u001b[37m# Add total_loss to summary.\u001b[39;49;00m\r\n",
      "        summaries.add(tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mtotal_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, total_loss))\r\n",
      "\r\n",
      "        \u001b[37m# Create gradient updates.\u001b[39;49;00m\r\n",
      "        grad_updates = optimizer.apply_gradients(clones_gradients,\r\n",
      "                                                 global_step=global_step)\r\n",
      "        update_ops.append(grad_updates)\r\n",
      "\r\n",
      "        update_op = tf.group(*update_ops)\r\n",
      "        \u001b[34mwith\u001b[39;49;00m tf.control_dependencies([update_op]):\r\n",
      "            train_tensor = tf.identity(total_loss, name=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_op\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m# Add the summaries from the first clone. These contain the summaries\u001b[39;49;00m\r\n",
      "        \u001b[37m# created by model_fn and either optimize_clones() or _gather_clone_loss().\u001b[39;49;00m\r\n",
      "        summaries |= \u001b[36mset\u001b[39;49;00m(tf.get_collection(tf.GraphKeys.SUMMARIES,\r\n",
      "                                           first_clone_scope))\r\n",
      "\r\n",
      "        \u001b[37m# Merge all summaries together.\u001b[39;49;00m\r\n",
      "        summary_op = tf.summary.merge(\u001b[36mlist\u001b[39;49;00m(summaries), name=\u001b[33m'\u001b[39;49;00m\u001b[33msummary_op\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m###########################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Kicks off the training. #\u001b[39;49;00m\r\n",
      "        \u001b[37m###########################\u001b[39;49;00m\r\n",
      "        slim.learning.train(\r\n",
      "            train_tensor,\r\n",
      "            logdir=args.train_dir,\r\n",
      "            master=args.master,\r\n",
      "            is_chief=(args.task == \u001b[34m0\u001b[39;49;00m),\r\n",
      "            init_fn=_get_init_fn(args),\r\n",
      "            summary_op=summary_op,\r\n",
      "            number_of_steps=args.max_number_of_steps,\r\n",
      "            log_every_n_steps=args.log_every_n_steps,\r\n",
      "            save_summaries_secs=args.save_summaries_secs,\r\n",
      "            save_interval_secs=args.save_interval_secs,\r\n",
      "            sync_optimizer=optimizer \u001b[34mif\u001b[39;49;00m args.sync_replicas \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m)\r\n",
      "\r\n",
      "    evaluation(args)\r\n",
      "    export_inference_graph(args)\r\n",
      "    freeze_graph(args)\r\n",
      "\r\n",
      "    converter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n",
      "        args.train_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/inference_graph_frozen.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33minput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], [\u001b[33m'\u001b[39;49;00m\u001b[33mMobilenetV1/Predictions/Reshape_1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    tflite_model = converter.convert()\r\n",
      "    \u001b[36mopen\u001b[39;49;00m(args.train_dir + \u001b[33m\"\u001b[39;49;00m\u001b[33m/mobilenetv1_model.tflite\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "         \u001b[33m\"\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).write(tflite_model)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize './src_dir/image_classifier.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. `TensorFlow` estimator를 이용한 training job 생성하기\n",
    "\n",
    "\n",
    "<p><strong><code>sagemaker.tensorflow.TensorFlow</code></strong> estimator는 처음 실행하는 스크립트 위치와 다양한 연계 코드들이 위치한 디렉토리 정보를 찾아서 스크립트를 S3에 upload하고 SageMaker의 training job을 수행하게 됩니다. training job은 학습을 수행한 단위입니다. 학습을 1번 돌리면 training job이 1개 생성됩니다. 몇 가지 중요 파라미터를 아래와 같이 설명드립니다. </p>\n",
    "\n",
    "- **entry_point** : 학습을 처음 실행하는 Python 소스 파일의 절대 또는 상대 경로이며, source_dir이 지정된 경우 entry_point는 source_dir 내 파일이 됩니다.\n",
    "- **source_dir** : 학습에 연계되는 다양한 소스코드 파일이 들어 있는 디렉토리 위치이며, 절대, 상대 경로 또는 S3 URI가 모두 가능하며,source_dir이 S3 URI 인 경우 tar.gz 파일이 됩니다.\n",
    "- **role** : Amazon SageMaker가 사용자를 대신해 작업(예: S3 버킷에서 모델 결과물이라고 하는 훈련 결과 읽기 및 Amazon S3에 훈련 결과 쓰기)을 수행하는 AWS Identity and Access Management(IAM) 역할입니다.\n",
    "- **train_instance_count** : 학습을 수행하는 instance 개수를 정의할 수 있습니다.\n",
    "- **train_instance_type** : 학습을 수행하는 instance 타입을 정의할 수 있습니다.\n",
    "- **train_volume_size** : 학습 인스턴스에 연결할 Amazon Elastic Block Store(Amazon EBS) 스토리지 볼륨의 크기(GB)입니다. File 모드를 사용할 경우 이 값이 훈련 데이터를 충분히 저장할 수 있는 크기여야 합니다(File 모드가 기본값)\n",
    "- **train_use_spot_instances** : 학습에서 SageMaker Managed Spot 인스턴스를 사용할지 여부를 지정합니다. 활성화되면 train_max_wait도 설정해야 합니다.\n",
    "- **train_max_run** : 최대 학습 시간을 설정할 수 있으며, 이 시간이 지나면 Amazon SageMaker는 현재 상태에 관계없이 작업을 종료합니다. (기본값 : 24 * 60 * 60)\n",
    "- **train_max_wait** : SageMaker Managed Spot 인스턴스를 기다리는 초 단위의 시간을 의미하는 것으로, 이 시간이 지나면 Amazon SageMaker는 스팟 인스턴스가 사용 가능해지기를 기다리는 것을 중지하며 결과는 fail이 됩니다.\n",
    "- **framework_version** : 학습에 사용될 특정 Tensorflow 버전을 정의할 수 있습니다.\n",
    "- **py_version** : 컨테이너 환경이 python3일 경우 py3, python2일 경우 py2로 설정하면 됩니다. python2는 지원이 중단되었지만 기존 python2로 구성된 파일들을 지원하기 위해 현재 계속 사용할 수 있습니다.\n",
    "- **hyperparameters** : 학습에 사용할 하이퍼 파라미터를 정의할 수 있으며, 정의된 하이퍼 파라미터 값들은 모두 학습 컨테이너로 전송이 됩니다.\n",
    "\n",
    "<p> 추가적으로 분산/ 멀티 GPU 학습도 가능합니다. SageMaker는 <strong><a href=\"https://github.com/horovod/horovod\" target=\"_blank\" class ='btn-default'>Horovod</a></strong>에 최적화된 환경을 제공하고 있으며, 자세한 내용은 <strong><a href=\"https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/tensorflow#distributed-training\" target=\"_blank\" class ='btn-default'>여기</a></strong>에서 확인이 가능합니다. 이번 학습에서는 분산/멀티 GPU 학습은 제외하였습니다.(단, 기존과 동일하게 python 소스코드에 분산/멀티 학습이 가능하도록 구성 필요) </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>S3에 저장된 TFRecord 파일의 위치를 다시 확인합니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-experiments-us-east-2-322537213286/captured_data/tfrecord'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Dataset 위치\n",
    "inputs= 's3://{}/{}'.format(data_bucket, prefix)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        'dataset_name' : 'captured_dataset',\n",
    "        'model_name' : 'mobilenet_v1_025',\n",
    "        'preprocessing_name' : 'mobilenet_v1',\n",
    "        'learning_rate_decay_type' : 'exponential',    ## \"fixed\", \"exponential\" or \"polynomial\"\n",
    "        'learning_rate_decay_factor' : 0.98,          ## in case of exponential\n",
    "        'learning_rate' : 0.001,\n",
    "        'image_size' : 224,\n",
    "        'save_summaries_secs' : 300,\n",
    "        'num_epochs_per_decay' : 2.5,\n",
    "        'moving_average_decay' : 0.9999,\n",
    "        'batch_size' : 128,\n",
    "        'max_number_of_steps' : 30000,\n",
    "        'eval_batch_size' : 1000,\n",
    "        'train_num_data' : train_num_data,\n",
    "        'test_num_data': test_num_data,\n",
    "        'finetune_checkpoint_path' : 'fine_tune_checkpoint/model.ckpt-25000',\n",
    "#         'finetune_checkpoint_path' : 'fine_tune_checkpoint/mobilenet_v1_0.25_128.ckpt',\n",
    "#         'checkpoint_exclude_scopes' : 'MobilenetV1/Logits,MobilenetV1/AuxLogits',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = \"{}-img-classifier-training-job\".format(int(time.time()))\n",
    "estimator = TensorFlow(entry_point='image_classifier.py',\n",
    "                       source_dir='src_dir',\n",
    "                       role=role,\n",
    "                       train_instance_count=1,\n",
    "                       train_instance_type='ml.p3.2xlarge',\n",
    "                       train_use_spot_instances=True,  # spot instance 활용\n",
    "                       train_volume_size=400,\n",
    "                       train_max_run=12*60*60,\n",
    "                       train_max_wait=12*60*60,\n",
    "#                        train_instance_type='local_gpu',\n",
    "                       framework_version='1.14.0',\n",
    "                       py_version='py2',\n",
    "                       hyperparameters=hyperparameters\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fit 함수로 학습 시작하기 \n",
    "\n",
    "<p>학습을 시작하는 것은 <strong><code>estimator.fit (training_data_uri)</code></strong>이 호출되는 경우입니다. 여기에서 실제 데이터가 있는 S3의 위치가 입력으로 사용됩니다. <code>fit</code>에서는 <code>training</code>라는 기본 채널을 생성하며, 이 위치의 데이터는 S3에서 실제 컨테이너 환경에서는 <code>SM_CHANNEL_TRAINING</code> 위치로 복사되어 학습에 활용이 가능합니다. <code>fit</code>은 몇 가지 다른 유형의 입력도 허용하는데 자세한 내용은 <strong><a href=\"https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.EstimatorBase.fit\" target=\"_blank\" class ='btn-default'>API 문서</a></strong>를 참고하실 수 있습니다.</p>\n",
    "<p> 학습이 시작되면 Tensorflow 컨테이너에서는 <code>image_classifier.py</code>를 실행되며, <code>Estimator</code>에서 <code>hyperparameters</code> 와 <code>model_dir</code>을 스크립트의 파라미터로 전달합니다. <code>model_dir</code>을 별도로 전달하지 않으며, 기본값은<strong>s3://[DEFAULT_BUCKET]/[TRAINING_JOB_NAME] </strong>이 되며 실제 스크립트 실행은 다음과 같습니다. </p>\n",
    "    \n",
    "```bash\n",
    "python image_classifier.py --model_dir s3://[DEFAULT_BUCKET]/[TRAINING_JOB_NAME]\n",
    "```\n",
    "<p>학습이 완료되면 training job은 Tensorflow serving을 위해 saved model을 S3에 upload합니다.</p>\n",
    "<p><code>fit</code>에서 <strong>wait=True</strong>로 설정할 경우 <strong>Synchronous</strong> 방식으로 동직하게 되며, <strong>wait=False</strong>일 경우 <strong>Aynchronous</strong> 방식으로 동작되어 여러 개의 Training job을 동시에 실행할 수 있습니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_job_name : 1593562844-img-classifier-training-job\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(\n",
    "    inputs = {'training': inputs},\n",
    "    job_name=training_job_name,\n",
    "    logs='All',\n",
    "    wait=False\n",
    ")\n",
    "print(\"training_job_name : {}\".format(training_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Aynchronous</strong>로 진행된 Training job은 아래와 같은 방법으로 진행상황을 실시간으로 확인할 수 있습니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-01 00:20:48 Starting - Starting the training job...\n",
      "2020-07-01 00:20:49 Starting - Launching requested ML instances.........\n",
      "2020-07-01 00:22:20 Starting - Preparing the instances for training......\n",
      "2020-07-01 00:23:28 Downloading - Downloading input data...\n",
      "2020-07-01 00:24:02 Training - Downloading the training image...\n",
      "2020-07-01 00:24:30 Training - Training image download completed. Training in progress.\u001b[34mparser.parse_known_args() : (Namespace(adadelta_rho=0.95, adagrad_initial_accumulator_value=0.1, adam_beta1=0.9, adam_beta2=0.999, batch_size=128, checkpoint_exclude_scopes=None, clone_on_cpu=False, current_host='algo-1', data_config={u'training': {u'TrainingInputMode': u'File', u'RecordWrapperType': u'None', u'S3DistributionType': u'FullyReplicated'}}, dataset_dir='/opt/ml/input/data/training', dataset_name='captured_dataset', end_learning_rate=0.01, eval_batch_size=1000, finetune_checkpoint_path='fine_tune_checkpoint/model.ckpt-25000', ftrl_initial_accumulator_value=0.1, ftrl_l1=0.0, ftrl_l2=0.0, ftrl_learning_rate_power=-0.5, fw_params={}, hosts=[u'algo-1'], ignore_missing_vars=False, image_size=224, is_training=False, is_video_model=False, label_smoothing=0.1, labels_offset=0, learning_rate=0.001, learning_rate_decay_factor=0.98, learning_rate_decay_type='exponential', log_every_n_steps=10, master='', max_eval_num_batches=None, max_number_of_steps=30000, model_dir='s3://sagemaker-us-east-2-322537213286/1593562844-img-classifier-training-job/model', model_name='mobilenet_v1_025', momentum=0.9, moving_average_decay=0.9999, num_clones=1, num_epochs_per_decay=2.5, num_frames=None, num_gpus='1', num_preprocessing_threads=4, num_ps_tasks=0, num_readers=4, opt_epsilon=1.0, optimizer='rmsprop', output_data_dir='/opt/ml/output/data', output_dir='/opt/ml/output', preprocessing_name='mobilenet_v1', quantize=False, quantize_delay=-1, replicas_to_aggregate=1, rmsprop_decay=0.9, rmsprop_momentum=0.9, save_interval_secs=600, save_summaries_secs=300, sync_replicas=False, task=0, test_num_data=738, train_dir='/opt/ml/model', train_num_data=6650, trainable_scopes=None, use_grayscale=False, warmup_epochs=0, weight_decay=4e-05, worker_replicas=1, write_text_graphdef=False), [])\u001b[0m\n",
      "\u001b[34mWARNING: Logging before flag parsing goes to stderr.\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.520514 139735246075648 deprecation_wrapper.py:119] From image_classifier.py:603: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.520807 139735246075648 deprecation_wrapper.py:119] From image_classifier.py:603: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.523161 139735246075648 deprecation.py:323] From image_classifier.py:617: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease switch to tf.train.create_global_step\u001b[0m\n",
      "\u001b[34mfile_pattern : None\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.528970 139735246075648 deprecation_wrapper.py:119] From /opt/ml/code/datasets/captured_dataset.py:72: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.529738 139735246075648 deprecation_wrapper.py:119] From /opt/ml/code/datasets/captured_dataset.py:88: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.529933 139735246075648 deprecation_wrapper.py:119] From /opt/ml/code/datasets/dataset_utils.py:206: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\u001b[0m\n",
      "\u001b[34mlabels_to_names : {0: u'Abyssinian', 1: u'Bengal', 2: u'Birman', 3: u'Bombay', 4: u'British_Shorthair', 5: u'Egyptian_Mau', 6: u'Maine_Coon', 7: u'Persian', 8: u'Ragdoll', 9: u'Russian_Blue', 10: u'Siamese', 11: u'Sphynx', 12: u'american_bulldog', 13: u'american_pit_bull_terrier', 14: u'basset_hound', 15: u'beagle', 16: u'boxer', 17: u'chihuahua', 18: u'english_cocker_spaniel', 19: u'english_setter', 20: u'german_shorthaired', 21: u'great_pyrenees', 22: u'havanese', 23: u'japanese_chin', 24: u'keeshond', 25: u'leonberger', 26: u'miniature_pinscher', 27: u'newfoundland', 28: u'pomeranian', 29: u'pug', 30: u'saint_bernard', 31: u'samoyed', 32: u'scottish_terrier', 33: u'shiba_inu', 34: u'staffordshire_bull_terrier', 35: u'wheaten_terrier', 36: u'yorkshire_terrier'}\u001b[0m\n",
      "\u001b[34mtrain_args.use_grayscale : False\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.530733 139735246075648 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py:246: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.535991 139735246075648 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.536781 139735246075648 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.538286 139735246075648 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:199: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.539283 139735246075648 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.546564 139735246075648 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py:95: __init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.650984 139735246075648 deprecation_wrapper.py:119] From /opt/ml/code/preprocessing/inception_preprocessing.py:206: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.653100 139735246075648 deprecation.py:323] From /opt/ml/code/preprocessing/inception_preprocessing.py:148: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34m`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.658612 139735246075648 deprecation_wrapper.py:119] From /opt/ml/code/preprocessing/inception_preprocessing.py:38: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.662632 139735246075648 deprecation_wrapper.py:119] From /opt/ml/code/preprocessing/inception_preprocessing.py:230: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.702887 139735246075648 deprecation.py:323] From image_classifier.py:666: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.719985 139735246075648 deprecation_wrapper.py:119] From image_classifier.py:693: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.720180 139735246075648 deprecation_wrapper.py:119] From image_classifier.py:693: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:37.720571 139735246075648 deprecation_wrapper.py:119] From /opt/ml/code/deployment/model_deploy.py:192: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:39.344160 139735246075648 deprecation.py:323] From image_classifier.py:689: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\u001b[0m\n",
      "\u001b[34mW0701 00:24:39.351062 139735246075648 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:373: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\n",
      "\u001b[0m\n",
      "\u001b[34mFuture major versions of TensorFlow will allow gradients to flow\u001b[0m\n",
      "\u001b[34minto the labels input on backprop by default.\n",
      "\u001b[0m\n",
      "\u001b[34mSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:39.386529 139735246075648 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:374: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.losses.compute_weighted_loss instead.\u001b[0m\n",
      "\u001b[34mW0701 00:24:39.394965 139735246075648 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:152: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mDeprecated in favor of operator or tf.math.divide.\u001b[0m\n",
      "\u001b[34mW0701 00:24:39.396737 139735246075648 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:154: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.where in 2.0, which has the same broadcast rule as np.where\u001b[0m\n",
      "\u001b[34mW0701 00:24:39.404624 139735246075648 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:121: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.losses.add_loss instead.\u001b[0m\n",
      "\u001b[34mW0701 00:24:39.405086 139735246075648 deprecation_wrapper.py:119] From image_classifier.py:707: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:39.406246 139735246075648 deprecation_wrapper.py:119] From image_classifier.py:708: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:40.088082 139735246075648 deprecation_wrapper.py:119] From image_classifier.py:262: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:40.095562 139735246075648 deprecation_wrapper.py:119] From image_classifier.py:333: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:40.097034 139735246075648 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/moving_averages.py:433: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\u001b[0m\n",
      "\u001b[34mW0701 00:24:43.849781 139735246075648 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/rmsprop.py:119: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m\n",
      "\u001b[34mW0701 00:24:44.823000 139735246075648 deprecation_wrapper.py:119] From image_classifier.py:782: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:44.829771 139735246075648 deprecation_wrapper.py:119] From image_classifier.py:379: The name tf.gfile.IsDirectory is deprecated. Please use tf.io.gfile.isdir instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0701 00:24:44.829977 139735246075648 deprecation_wrapper.py:119] From image_classifier.py:385: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\u001b[0m\n",
      "\u001b[34mI0701 00:24:44.830063 139735246075648 image_classifier.py:385] Fine-tuning from fine_tune_checkpoint/model.ckpt-25000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mW0701 00:24:45.649071 139735246075648 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:742: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease switch to tf.train.MonitoredTrainingSession\u001b[0m\n",
      "\u001b[34m2020-07-01 00:24:49.306319: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34mW0701 00:24:49.803543 139735246075648 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1282: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse standard file APIs to check for files with this prefix.\u001b[0m\n",
      "\u001b[34mI0701 00:24:49.805938 139735246075648 saver.py:1286] Restoring parameters from fine_tune_checkpoint/model.ckpt-25000\u001b[0m\n",
      "\u001b[34mI0701 00:24:50.085700 139735246075648 session_manager.py:500] Running local_init_op.\u001b[0m\n",
      "\u001b[34mI0701 00:24:50.203140 139735246075648 session_manager.py:502] Done running local_init_op.\u001b[0m\n",
      "\u001b[34mI0701 00:24:54.285770 139735246075648 learning.py:754] Starting Session.\u001b[0m\n",
      "\u001b[34mI0701 00:24:54.398499 139729488037632 supervisor.py:1117] Saving checkpoint to path /opt/ml/model/model.ckpt\u001b[0m\n",
      "\u001b[34mI0701 00:24:54.404134 139735246075648 learning.py:768] Starting Queues.\u001b[0m\n",
      "\u001b[34mI0701 00:24:56.448605 139729479644928 supervisor.py:1099] global_step/sec: 0\u001b[0m\n",
      "\u001b[34mI0701 00:25:04.148833 139729471252224 supervisor.py:1050] Recording summary at step 1.\u001b[0m\n",
      "\u001b[34mI0701 00:25:06.374345 139735246075648 learning.py:507] global step 10: loss = 1.4490 (0.247 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:08.728377 139735246075648 learning.py:507] global step 20: loss = 1.4968 (0.227 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:11.087121 139735246075648 learning.py:507] global step 30: loss = 1.5143 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:13.464364 139735246075648 learning.py:507] global step 40: loss = 1.5523 (0.242 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:15.841994 139735246075648 learning.py:507] global step 50: loss = 1.6640 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:18.166994 139735246075648 learning.py:507] global step 60: loss = 1.4794 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:20.516459 139735246075648 learning.py:507] global step 70: loss = 1.6559 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:22.873583 139735246075648 learning.py:507] global step 80: loss = 1.5426 (0.251 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:25.230546 139735246075648 learning.py:507] global step 90: loss = 1.5841 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:27.603290 139735246075648 learning.py:507] global step 100: loss = 1.4485 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:29.961551 139735246075648 learning.py:507] global step 110: loss = 1.5638 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:32.332870 139735246075648 learning.py:507] global step 120: loss = 1.5504 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:34.647772 139735246075648 learning.py:507] global step 130: loss = 1.5545 (0.221 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:36.986381 139735246075648 learning.py:507] global step 140: loss = 1.5151 (0.226 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:39.378705 139735246075648 learning.py:507] global step 150: loss = 1.4837 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:41.727615 139735246075648 learning.py:507] global step 160: loss = 1.5127 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:44.085736 139735246075648 learning.py:507] global step 170: loss = 1.5921 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:46.437752 139735246075648 learning.py:507] global step 180: loss = 1.5287 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:48.812429 139735246075648 learning.py:507] global step 190: loss = 1.5322 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:51.161508 139735246075648 learning.py:507] global step 200: loss = 1.6738 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:53.533731 139735246075648 learning.py:507] global step 210: loss = 1.5792 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:55.888530 139735246075648 learning.py:507] global step 220: loss = 1.4868 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:25:58.249043 139735246075648 learning.py:507] global step 230: loss = 1.4753 (0.226 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:00.623575 139735246075648 learning.py:507] global step 240: loss = 1.6132 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:02.974558 139735246075648 learning.py:507] global step 250: loss = 1.5194 (0.240 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:05.337888 139735246075648 learning.py:507] global step 260: loss = 1.6637 (0.243 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:07.692401 139735246075648 learning.py:507] global step 270: loss = 1.5218 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:10.041932 139735246075648 learning.py:507] global step 280: loss = 1.5421 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:12.414644 139735246075648 learning.py:507] global step 290: loss = 1.6590 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:14.741516 139735246075648 learning.py:507] global step 300: loss = 1.5485 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:17.098332 139735246075648 learning.py:507] global step 310: loss = 1.5961 (0.247 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:19.480648 139735246075648 learning.py:507] global step 320: loss = 1.7225 (0.223 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:21.866709 139735246075648 learning.py:507] global step 330: loss = 1.4629 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:24.223498 139735246075648 learning.py:507] global step 340: loss = 1.4793 (0.225 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:26.607228 139735246075648 learning.py:507] global step 350: loss = 1.5379 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:28.956338 139735246075648 learning.py:507] global step 360: loss = 1.5816 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:31.302171 139735246075648 learning.py:507] global step 370: loss = 1.5466 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:33.663239 139735246075648 learning.py:507] global step 380: loss = 1.5614 (0.247 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:36.084501 139735246075648 learning.py:507] global step 390: loss = 1.4222 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:38.448124 139735246075648 learning.py:507] global step 400: loss = 1.5273 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:40.820837 139735246075648 learning.py:507] global step 410: loss = 1.4802 (0.232 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:43.175684 139735246075648 learning.py:507] global step 420: loss = 1.5548 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:45.545510 139735246075648 learning.py:507] global step 430: loss = 1.5703 (0.244 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:47.911478 139735246075648 learning.py:507] global step 440: loss = 1.5536 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:50.271553 139735246075648 learning.py:507] global step 450: loss = 1.4263 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:52.666661 139735246075648 learning.py:507] global step 460: loss = 1.4821 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:55.020284 139735246075648 learning.py:507] global step 470: loss = 1.5978 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:57.370322 139735246075648 learning.py:507] global step 480: loss = 1.5499 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:26:59.732702 139735246075648 learning.py:507] global step 490: loss = 1.4456 (0.246 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:02.085035 139735246075648 learning.py:507] global step 500: loss = 1.5910 (0.241 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:04.461241 139735246075648 learning.py:507] global step 510: loss = 1.4715 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:06.798753 139735246075648 learning.py:507] global step 520: loss = 1.5291 (0.225 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:09.151892 139735246075648 learning.py:507] global step 530: loss = 1.4872 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:11.493256 139735246075648 learning.py:507] global step 540: loss = 1.4605 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:13.868135 139735246075648 learning.py:507] global step 550: loss = 1.5759 (0.225 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:16.250566 139735246075648 learning.py:507] global step 560: loss = 1.6133 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:18.612952 139735246075648 learning.py:507] global step 570: loss = 1.7148 (0.246 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:20.950089 139735246075648 learning.py:507] global step 580: loss = 1.5350 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:23.323549 139735246075648 learning.py:507] global step 590: loss = 1.4479 (0.241 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:25.662857 139735246075648 learning.py:507] global step 600: loss = 1.6075 (0.239 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0701 00:27:27.976017 139735246075648 learning.py:507] global step 610: loss = 1.5476 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:30.330969 139735246075648 learning.py:507] global step 620: loss = 1.5546 (0.240 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:32.677287 139735246075648 learning.py:507] global step 630: loss = 1.4961 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:35.005853 139735246075648 learning.py:507] global step 640: loss = 1.4910 (0.225 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:37.369421 139735246075648 learning.py:507] global step 650: loss = 1.4802 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:39.739717 139735246075648 learning.py:507] global step 660: loss = 1.4876 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:42.072590 139735246075648 learning.py:507] global step 670: loss = 1.5303 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:44.440766 139735246075648 learning.py:507] global step 680: loss = 1.5468 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:46.789722 139735246075648 learning.py:507] global step 690: loss = 1.5251 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:49.156495 139735246075648 learning.py:507] global step 700: loss = 1.3682 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:51.525984 139735246075648 learning.py:507] global step 710: loss = 1.5458 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:53.859600 139735246075648 learning.py:507] global step 720: loss = 1.5258 (0.240 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:56.239665 139735246075648 learning.py:507] global step 730: loss = 1.5386 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:27:58.612313 139735246075648 learning.py:507] global step 740: loss = 1.6310 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:00.967843 139735246075648 learning.py:507] global step 750: loss = 1.3995 (0.240 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:03.350248 139735246075648 learning.py:507] global step 760: loss = 1.4329 (0.248 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:05.709922 139735246075648 learning.py:507] global step 770: loss = 1.5395 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:08.084408 139735246075648 learning.py:507] global step 780: loss = 1.4740 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:10.448075 139735246075648 learning.py:507] global step 790: loss = 1.4675 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:12.809900 139735246075648 learning.py:507] global step 800: loss = 1.5946 (0.243 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:15.178862 139735246075648 learning.py:507] global step 810: loss = 1.5718 (0.242 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:17.606439 139735246075648 learning.py:507] global step 820: loss = 1.5201 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:20.018368 139735246075648 learning.py:507] global step 830: loss = 1.5064 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:22.404460 139735246075648 learning.py:507] global step 840: loss = 1.5472 (0.245 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:24.771940 139735246075648 learning.py:507] global step 850: loss = 1.4478 (0.224 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:27.138437 139735246075648 learning.py:507] global step 860: loss = 1.4908 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:29.507885 139735246075648 learning.py:507] global step 870: loss = 1.5347 (0.241 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:31.878146 139735246075648 learning.py:507] global step 880: loss = 1.7600 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:34.233850 139735246075648 learning.py:507] global step 890: loss = 1.4932 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:36.585421 139735246075648 learning.py:507] global step 900: loss = 1.6801 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:38.942863 139735246075648 learning.py:507] global step 910: loss = 1.4998 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:41.284537 139735246075648 learning.py:507] global step 920: loss = 1.4894 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:43.664160 139735246075648 learning.py:507] global step 930: loss = 1.4939 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:46.056417 139735246075648 learning.py:507] global step 940: loss = 1.4960 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:48.405111 139735246075648 learning.py:507] global step 950: loss = 1.5702 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:50.780026 139735246075648 learning.py:507] global step 960: loss = 1.4163 (0.245 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:53.136415 139735246075648 learning.py:507] global step 970: loss = 1.5304 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:55.496057 139735246075648 learning.py:507] global step 980: loss = 1.6108 (0.240 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:28:57.871998 139735246075648 learning.py:507] global step 990: loss = 1.5397 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:00.256855 139735246075648 learning.py:507] global step 1000: loss = 1.5559 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:02.618038 139735246075648 learning.py:507] global step 1010: loss = 1.5660 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:04.965615 139735246075648 learning.py:507] global step 1020: loss = 1.4914 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:07.342283 139735246075648 learning.py:507] global step 1030: loss = 1.5037 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:09.709124 139735246075648 learning.py:507] global step 1040: loss = 1.5650 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:12.063261 139735246075648 learning.py:507] global step 1050: loss = 1.4949 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:14.405292 139735246075648 learning.py:507] global step 1060: loss = 1.5304 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:16.786309 139735246075648 learning.py:507] global step 1070: loss = 1.5045 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:19.141731 139735246075648 learning.py:507] global step 1080: loss = 1.5231 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:21.526515 139735246075648 learning.py:507] global step 1090: loss = 1.5399 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:23.884618 139735246075648 learning.py:507] global step 1100: loss = 1.5261 (0.241 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:26.230406 139735246075648 learning.py:507] global step 1110: loss = 1.4671 (0.240 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:28.610588 139735246075648 learning.py:507] global step 1120: loss = 1.4649 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:30.939730 139735246075648 learning.py:507] global step 1130: loss = 1.4670 (0.222 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:33.309014 139735246075648 learning.py:507] global step 1140: loss = 1.5683 (0.241 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:35.693672 139735246075648 learning.py:507] global step 1150: loss = 1.5358 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:38.053692 139735246075648 learning.py:507] global step 1160: loss = 1.4825 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:40.456091 139735246075648 learning.py:507] global step 1170: loss = 1.5771 (0.240 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:42.850300 139735246075648 learning.py:507] global step 1180: loss = 1.4618 (0.226 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:45.230748 139735246075648 learning.py:507] global step 1190: loss = 1.5096 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:47.592076 139735246075648 learning.py:507] global step 1200: loss = 1.5505 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:49.992471 139735246075648 learning.py:507] global step 1210: loss = 1.5236 (0.248 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:52.364029 139735246075648 learning.py:507] global step 1220: loss = 1.5059 (0.232 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:55.442091 139735246075648 learning.py:507] global step 1230: loss = 1.4965 (0.247 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:29:55.865159 139729479644928 supervisor.py:1099] global_step/sec: 4.108\u001b[0m\n",
      "\u001b[34mI0701 00:29:56.138128 139729471252224 supervisor.py:1050] Recording summary at step 1231.\u001b[0m\n",
      "\u001b[34mI0701 00:29:58.190321 139735246075648 learning.py:507] global step 1240: loss = 1.3514 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:00.564107 139735246075648 learning.py:507] global step 1250: loss = 1.4958 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:02.946779 139735246075648 learning.py:507] global step 1260: loss = 1.5530 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:05.301881 139735246075648 learning.py:507] global step 1270: loss = 1.5659 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:07.686788 139735246075648 learning.py:507] global step 1280: loss = 1.4686 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:10.040173 139735246075648 learning.py:507] global step 1290: loss = 1.6031 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:12.388236 139735246075648 learning.py:507] global step 1300: loss = 1.4610 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:14.744952 139735246075648 learning.py:507] global step 1310: loss = 1.5485 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:17.126674 139735246075648 learning.py:507] global step 1320: loss = 1.5148 (0.245 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:19.543694 139735246075648 learning.py:507] global step 1330: loss = 1.6231 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:21.912683 139735246075648 learning.py:507] global step 1340: loss = 1.3823 (0.232 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:24.265635 139735246075648 learning.py:507] global step 1350: loss = 1.6036 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:26.616981 139735246075648 learning.py:507] global step 1360: loss = 1.6538 (0.227 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:29.000422 139735246075648 learning.py:507] global step 1370: loss = 1.4543 (0.239 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0701 00:30:31.366954 139735246075648 learning.py:507] global step 1380: loss = 1.5474 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:33.725805 139735246075648 learning.py:507] global step 1390: loss = 1.4293 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:36.080322 139735246075648 learning.py:507] global step 1400: loss = 1.5612 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:38.395329 139735246075648 learning.py:507] global step 1410: loss = 1.4569 (0.224 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:40.757076 139735246075648 learning.py:507] global step 1420: loss = 1.5664 (0.232 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:43.148196 139735246075648 learning.py:507] global step 1430: loss = 1.5627 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:45.512945 139735246075648 learning.py:507] global step 1440: loss = 1.4811 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:47.877816 139735246075648 learning.py:507] global step 1450: loss = 1.5141 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:50.229621 139735246075648 learning.py:507] global step 1460: loss = 1.4330 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:52.586297 139735246075648 learning.py:507] global step 1470: loss = 1.4075 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:54.978056 139735246075648 learning.py:507] global step 1480: loss = 1.5411 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:57.335596 139735246075648 learning.py:507] global step 1490: loss = 1.4713 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:30:59.666104 139735246075648 learning.py:507] global step 1500: loss = 1.5383 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:01.986325 139735246075648 learning.py:507] global step 1510: loss = 1.4906 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:04.372622 139735246075648 learning.py:507] global step 1520: loss = 1.6400 (0.241 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:06.743642 139735246075648 learning.py:507] global step 1530: loss = 1.4634 (0.244 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:09.149285 139735246075648 learning.py:507] global step 1540: loss = 1.4629 (0.247 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:11.521316 139735246075648 learning.py:507] global step 1550: loss = 1.5563 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:13.872498 139735246075648 learning.py:507] global step 1560: loss = 1.4798 (0.243 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:16.201154 139735246075648 learning.py:507] global step 1570: loss = 1.5004 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:18.567667 139735246075648 learning.py:507] global step 1580: loss = 1.4938 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:20.918502 139735246075648 learning.py:507] global step 1590: loss = 1.5934 (0.232 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:23.308773 139735246075648 learning.py:507] global step 1600: loss = 1.4245 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:25.649688 139735246075648 learning.py:507] global step 1610: loss = 1.5141 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:28.036027 139735246075648 learning.py:507] global step 1620: loss = 1.5846 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:30.389841 139735246075648 learning.py:507] global step 1630: loss = 1.5122 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:32.737562 139735246075648 learning.py:507] global step 1640: loss = 1.4263 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:35.118879 139735246075648 learning.py:507] global step 1650: loss = 1.5341 (0.248 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:37.491959 139735246075648 learning.py:507] global step 1660: loss = 1.5475 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:39.845055 139735246075648 learning.py:507] global step 1670: loss = 1.5119 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:42.193823 139735246075648 learning.py:507] global step 1680: loss = 1.5018 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:44.556401 139735246075648 learning.py:507] global step 1690: loss = 1.5539 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:46.923336 139735246075648 learning.py:507] global step 1700: loss = 1.5297 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:49.298278 139735246075648 learning.py:507] global step 1710: loss = 1.4854 (0.240 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:51.626703 139735246075648 learning.py:507] global step 1720: loss = 1.5428 (0.247 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:53.973211 139735246075648 learning.py:507] global step 1730: loss = 1.5992 (0.232 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:56.320174 139735246075648 learning.py:507] global step 1740: loss = 1.5498 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:31:58.682388 139735246075648 learning.py:507] global step 1750: loss = 1.4802 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:01.050559 139735246075648 learning.py:507] global step 1760: loss = 1.6386 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:03.426456 139735246075648 learning.py:507] global step 1770: loss = 1.7068 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:05.831820 139735246075648 learning.py:507] global step 1780: loss = 1.5452 (0.245 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:08.198580 139735246075648 learning.py:507] global step 1790: loss = 1.4270 (0.243 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:10.565704 139735246075648 learning.py:507] global step 1800: loss = 1.3623 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:12.939095 139735246075648 learning.py:507] global step 1810: loss = 1.6419 (0.247 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:15.315902 139735246075648 learning.py:507] global step 1820: loss = 1.6413 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:17.690687 139735246075648 learning.py:507] global step 1830: loss = 1.4274 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:20.084400 139735246075648 learning.py:507] global step 1840: loss = 1.4896 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:22.436748 139735246075648 learning.py:507] global step 1850: loss = 1.5442 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:24.815098 139735246075648 learning.py:507] global step 1860: loss = 1.5439 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:27.182751 139735246075648 learning.py:507] global step 1870: loss = 1.5959 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:29.587352 139735246075648 learning.py:507] global step 1880: loss = 1.5204 (0.240 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:31.967513 139735246075648 learning.py:507] global step 1890: loss = 1.4949 (0.245 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:34.324902 139735246075648 learning.py:507] global step 1900: loss = 1.4756 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:36.681574 139735246075648 learning.py:507] global step 1910: loss = 1.5824 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:39.041253 139735246075648 learning.py:507] global step 1920: loss = 1.5278 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:41.408654 139735246075648 learning.py:507] global step 1930: loss = 1.4880 (0.225 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:43.765571 139735246075648 learning.py:507] global step 1940: loss = 1.5361 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:46.139003 139735246075648 learning.py:507] global step 1950: loss = 1.5634 (0.243 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:48.477040 139735246075648 learning.py:507] global step 1960: loss = 1.4916 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:50.838001 139735246075648 learning.py:507] global step 1970: loss = 1.4820 (0.240 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:53.212896 139735246075648 learning.py:507] global step 1980: loss = 1.4641 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:55.568677 139735246075648 learning.py:507] global step 1990: loss = 1.4886 (0.232 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:32:57.949233 139735246075648 learning.py:507] global step 2000: loss = 1.5721 (0.246 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:00.278911 139735246075648 learning.py:507] global step 2010: loss = 1.5951 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:02.634641 139735246075648 learning.py:507] global step 2020: loss = 1.4253 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:04.979085 139735246075648 learning.py:507] global step 2030: loss = 1.4277 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:07.318514 139735246075648 learning.py:507] global step 2040: loss = 1.4547 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:09.675957 139735246075648 learning.py:507] global step 2050: loss = 1.5277 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:12.073945 139735246075648 learning.py:507] global step 2060: loss = 1.4306 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:14.424740 139735246075648 learning.py:507] global step 2070: loss = 1.4387 (0.226 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:16.788351 139735246075648 learning.py:507] global step 2080: loss = 1.5132 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:19.179487 139735246075648 learning.py:507] global step 2090: loss = 1.5088 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:21.530072 139735246075648 learning.py:507] global step 2100: loss = 1.6386 (0.226 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:23.880526 139735246075648 learning.py:507] global step 2110: loss = 1.5861 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:26.250961 139735246075648 learning.py:507] global step 2120: loss = 1.4668 (0.224 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:28.625941 139735246075648 learning.py:507] global step 2130: loss = 1.4808 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:30.972738 139735246075648 learning.py:507] global step 2140: loss = 1.4498 (0.234 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0701 00:33:33.350574 139735246075648 learning.py:507] global step 2150: loss = 1.5229 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:35.812448 139735246075648 learning.py:507] global step 2160: loss = 1.4428 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:38.162539 139735246075648 learning.py:507] global step 2170: loss = 1.4360 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:40.527194 139735246075648 learning.py:507] global step 2180: loss = 1.3491 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:42.896497 139735246075648 learning.py:507] global step 2190: loss = 1.5268 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:45.261260 139735246075648 learning.py:507] global step 2200: loss = 1.6098 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:47.631556 139735246075648 learning.py:507] global step 2210: loss = 1.4733 (0.226 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:49.989438 139735246075648 learning.py:507] global step 2220: loss = 1.7007 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:52.346627 139735246075648 learning.py:507] global step 2230: loss = 1.4506 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:54.691342 139735246075648 learning.py:507] global step 2240: loss = 1.4230 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:57.078017 139735246075648 learning.py:507] global step 2250: loss = 1.4706 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:33:59.453495 139735246075648 learning.py:507] global step 2260: loss = 1.3656 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:01.828041 139735246075648 learning.py:507] global step 2270: loss = 1.4801 (0.241 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:04.225373 139735246075648 learning.py:507] global step 2280: loss = 1.4789 (0.247 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:06.626348 139735246075648 learning.py:507] global step 2290: loss = 1.3910 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:08.976243 139735246075648 learning.py:507] global step 2300: loss = 1.4762 (0.232 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:11.362643 139735246075648 learning.py:507] global step 2310: loss = 1.3740 (0.245 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:13.733239 139735246075648 learning.py:507] global step 2320: loss = 1.4645 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:16.096050 139735246075648 learning.py:507] global step 2330: loss = 1.3616 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:18.451026 139735246075648 learning.py:507] global step 2340: loss = 1.4973 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:20.785307 139735246075648 learning.py:507] global step 2350: loss = 1.4179 (0.222 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:23.135593 139735246075648 learning.py:507] global step 2360: loss = 1.5106 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:25.540155 139735246075648 learning.py:507] global step 2370: loss = 1.5097 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:27.897121 139735246075648 learning.py:507] global step 2380: loss = 1.5100 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:30.246234 139735246075648 learning.py:507] global step 2390: loss = 1.4107 (0.242 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:32.592962 139735246075648 learning.py:507] global step 2400: loss = 1.4606 (0.248 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:34.977055 139735246075648 learning.py:507] global step 2410: loss = 1.5758 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:37.344229 139735246075648 learning.py:507] global step 2420: loss = 1.3938 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:39.711666 139735246075648 learning.py:507] global step 2430: loss = 1.5604 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:42.066732 139735246075648 learning.py:507] global step 2440: loss = 1.5199 (0.227 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:44.429629 139735246075648 learning.py:507] global step 2450: loss = 1.5683 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:46.831090 139735246075648 learning.py:507] global step 2460: loss = 1.4690 (0.240 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:49.200401 139735246075648 learning.py:507] global step 2470: loss = 1.2835 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:51.549736 139735246075648 learning.py:507] global step 2480: loss = 1.4343 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:53.929928 139735246075648 learning.py:507] global step 2490: loss = 1.4211 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:54.398334 139729488037632 supervisor.py:1117] Saving checkpoint to path /opt/ml/model/model.ckpt\u001b[0m\n",
      "\u001b[34mI0701 00:34:55.510971 139729479644928 supervisor.py:1099] global_step/sec: 4.21163\u001b[0m\n",
      "\u001b[34mI0701 00:34:55.799849 139729471252224 supervisor.py:1050] Recording summary at step 2493.\u001b[0m\n",
      "\u001b[34mI0701 00:34:57.388391 139735246075648 learning.py:507] global step 2500: loss = 1.6570 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:34:59.708046 139735246075648 learning.py:507] global step 2510: loss = 1.3549 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:02.064104 139735246075648 learning.py:507] global step 2520: loss = 1.4738 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:04.389358 139735246075648 learning.py:507] global step 2530: loss = 1.5216 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:06.759779 139735246075648 learning.py:507] global step 2540: loss = 1.5372 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:09.091084 139735246075648 learning.py:507] global step 2550: loss = 1.4492 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:11.480777 139735246075648 learning.py:507] global step 2560: loss = 1.5140 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:13.838329 139735246075648 learning.py:507] global step 2570: loss = 1.4540 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:16.164196 139735246075648 learning.py:507] global step 2580: loss = 1.3957 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:18.501852 139735246075648 learning.py:507] global step 2590: loss = 1.5210 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:20.866446 139735246075648 learning.py:507] global step 2600: loss = 1.4188 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:23.235768 139735246075648 learning.py:507] global step 2610: loss = 1.5125 (0.227 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:25.597815 139735246075648 learning.py:507] global step 2620: loss = 1.4356 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:27.957567 139735246075648 learning.py:507] global step 2630: loss = 1.4769 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:30.321163 139735246075648 learning.py:507] global step 2640: loss = 1.4279 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:32.688508 139735246075648 learning.py:507] global step 2650: loss = 1.4480 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:35.050338 139735246075648 learning.py:507] global step 2660: loss = 1.4828 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:37.435441 139735246075648 learning.py:507] global step 2670: loss = 1.5136 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:39.788888 139735246075648 learning.py:507] global step 2680: loss = 1.3460 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:42.116700 139735246075648 learning.py:507] global step 2690: loss = 1.5901 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:44.455475 139735246075648 learning.py:507] global step 2700: loss = 1.4497 (0.223 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:46.828331 139735246075648 learning.py:507] global step 2710: loss = 1.4854 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:49.193532 139735246075648 learning.py:507] global step 2720: loss = 1.4445 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:51.561170 139735246075648 learning.py:507] global step 2730: loss = 1.2684 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:53.937345 139735246075648 learning.py:507] global step 2740: loss = 1.4937 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:56.317346 139735246075648 learning.py:507] global step 2750: loss = 1.3974 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:35:58.668661 139735246075648 learning.py:507] global step 2760: loss = 1.5678 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:01.041996 139735246075648 learning.py:507] global step 2770: loss = 1.4485 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:03.407438 139735246075648 learning.py:507] global step 2780: loss = 1.5267 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:05.765109 139735246075648 learning.py:507] global step 2790: loss = 1.3799 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:08.143436 139735246075648 learning.py:507] global step 2800: loss = 1.3834 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:10.499799 139735246075648 learning.py:507] global step 2810: loss = 1.3681 (0.232 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:12.877599 139735246075648 learning.py:507] global step 2820: loss = 1.3833 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:15.264919 139735246075648 learning.py:507] global step 2830: loss = 1.3874 (0.247 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:17.635991 139735246075648 learning.py:507] global step 2840: loss = 1.4429 (0.240 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:19.980459 139735246075648 learning.py:507] global step 2850: loss = 1.4039 (0.239 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0701 00:36:22.354020 139735246075648 learning.py:507] global step 2860: loss = 1.4669 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:24.723222 139735246075648 learning.py:507] global step 2870: loss = 1.4979 (0.242 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:27.081810 139735246075648 learning.py:507] global step 2880: loss = 1.5242 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:29.425035 139735246075648 learning.py:507] global step 2890: loss = 1.4862 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:31.798669 139735246075648 learning.py:507] global step 2900: loss = 1.4361 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:34.196156 139735246075648 learning.py:507] global step 2910: loss = 1.5536 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:36.538780 139735246075648 learning.py:507] global step 2920: loss = 1.4416 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:38.918688 139735246075648 learning.py:507] global step 2930: loss = 1.4006 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:41.298794 139735246075648 learning.py:507] global step 2940: loss = 1.4849 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:43.672133 139735246075648 learning.py:507] global step 2950: loss = 1.3740 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:46.016258 139735246075648 learning.py:507] global step 2960: loss = 1.4601 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:48.394341 139735246075648 learning.py:507] global step 2970: loss = 1.4241 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:50.758845 139735246075648 learning.py:507] global step 2980: loss = 1.5116 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:53.126140 139735246075648 learning.py:507] global step 2990: loss = 1.5467 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:55.468732 139735246075648 learning.py:507] global step 3000: loss = 1.4998 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:36:57.863495 139735246075648 learning.py:507] global step 3010: loss = 1.4964 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:00.208796 139735246075648 learning.py:507] global step 3020: loss = 1.4354 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:02.561633 139735246075648 learning.py:507] global step 3030: loss = 1.4304 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:04.898878 139735246075648 learning.py:507] global step 3040: loss = 1.3965 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:07.334738 139735246075648 learning.py:507] global step 3050: loss = 1.4984 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:09.696038 139735246075648 learning.py:507] global step 3060: loss = 1.4360 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:12.055641 139735246075648 learning.py:507] global step 3070: loss = 1.3846 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:14.441293 139735246075648 learning.py:507] global step 3080: loss = 1.4901 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:17.304734 139735246075648 learning.py:507] global step 3090: loss = 1.4075 (0.240 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:19.684953 139735246075648 learning.py:507] global step 3100: loss = 1.3932 (0.240 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:22.066471 139735246075648 learning.py:507] global step 3110: loss = 1.5080 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:24.429305 139735246075648 learning.py:507] global step 3120: loss = 1.4633 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:26.785053 139735246075648 learning.py:507] global step 3130: loss = 1.4817 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:29.155740 139735246075648 learning.py:507] global step 3140: loss = 1.3723 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:31.497879 139735246075648 learning.py:507] global step 3150: loss = 1.5100 (0.225 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:33.862783 139735246075648 learning.py:507] global step 3160: loss = 1.3349 (0.241 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:36.213134 139735246075648 learning.py:507] global step 3170: loss = 1.4463 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:38.576384 139735246075648 learning.py:507] global step 3180: loss = 1.4301 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:40.976238 139735246075648 learning.py:507] global step 3190: loss = 1.4161 (0.244 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:43.381541 139735246075648 learning.py:507] global step 3200: loss = 1.5053 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:45.757848 139735246075648 learning.py:507] global step 3210: loss = 1.5042 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:48.082448 139735246075648 learning.py:507] global step 3220: loss = 1.4597 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:50.442589 139735246075648 learning.py:507] global step 3230: loss = 1.4947 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:52.915245 139735246075648 learning.py:507] global step 3240: loss = 1.4931 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:55.277398 139735246075648 learning.py:507] global step 3250: loss = 1.4412 (0.247 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:37:57.640275 139735246075648 learning.py:507] global step 3260: loss = 1.5235 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:00.035603 139735246075648 learning.py:507] global step 3270: loss = 1.3843 (0.247 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:02.407326 139735246075648 learning.py:507] global step 3280: loss = 1.5338 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:04.760669 139735246075648 learning.py:507] global step 3290: loss = 1.4641 (0.224 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:07.120273 139735246075648 learning.py:507] global step 3300: loss = 1.4219 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:09.503943 139735246075648 learning.py:507] global step 3310: loss = 1.5104 (0.239 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:11.867230 139735246075648 learning.py:507] global step 3320: loss = 1.6473 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:14.252192 139735246075648 learning.py:507] global step 3330: loss = 1.5757 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:16.600220 139735246075648 learning.py:507] global step 3340: loss = 1.4501 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:18.926908 139735246075648 learning.py:507] global step 3350: loss = 1.4976 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:21.268560 139735246075648 learning.py:507] global step 3360: loss = 1.4213 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:23.657819 139735246075648 learning.py:507] global step 3370: loss = 1.5448 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:26.025522 139735246075648 learning.py:507] global step 3380: loss = 1.4941 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:28.507159 139735246075648 learning.py:507] global step 3390: loss = 1.4878 (0.227 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:30.855586 139735246075648 learning.py:507] global step 3400: loss = 1.3496 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:33.226717 139735246075648 learning.py:507] global step 3410: loss = 1.5386 (0.242 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:35.555949 139735246075648 learning.py:507] global step 3420: loss = 1.5264 (0.226 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:37.947679 139735246075648 learning.py:507] global step 3430: loss = 1.4540 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:40.336694 139735246075648 learning.py:507] global step 3440: loss = 1.4954 (0.240 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:42.733649 139735246075648 learning.py:507] global step 3450: loss = 1.5283 (0.246 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:45.095909 139735246075648 learning.py:507] global step 3460: loss = 1.4818 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:47.461291 139735246075648 learning.py:507] global step 3470: loss = 1.4883 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:49.828288 139735246075648 learning.py:507] global step 3480: loss = 1.4436 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:52.186151 139735246075648 learning.py:507] global step 3490: loss = 1.4708 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:54.551101 139735246075648 learning.py:507] global step 3500: loss = 1.5927 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0701 00:38:56.927700 139735246075648 learning.py:507] global step 3510: loss = 1.3979 (0.236 sec/step)\u001b[0m\n",
      "\n",
      "2020-07-01 00:59:45 Interrupted - Training job interrupted\n",
      "2020-07-01 01:02:42 Starting - Launching requested ML instances\n",
      "2020-07-01 01:03:46 Starting - Preparing the instances for training\n",
      "2020-07-01 01:04:40 Downloading - Downloading input data\n",
      "2020-07-01 01:04:53 Training - Downloading the training image\n",
      "2020-07-01 01:05:45 Training - Training image download completed. Training in progress.\u001b[32mparser.parse_known_args() : (Namespace(adadelta_rho=0.95, adagrad_initial_accumulator_value=0.1, adam_beta1=0.9, adam_beta2=0.999, batch_size=128, checkpoint_exclude_scopes=None, clone_on_cpu=False, current_host='algo-1', data_config={u'training': {u'TrainingInputMode': u'File', u'RecordWrapperType': u'None', u'S3DistributionType': u'FullyReplicated'}}, dataset_dir='/opt/ml/input/data/training', dataset_name='captured_dataset', end_learning_rate=0.01, eval_batch_size=1000, finetune_checkpoint_path='fine_tune_checkpoint/model.ckpt-25000', ftrl_initial_accumulator_value=0.1, ftrl_l1=0.0, ftrl_l2=0.0, ftrl_learning_rate_power=-0.5, fw_params={}, hosts=[u'algo-1'], ignore_missing_vars=False, image_size=224, is_training=False, is_video_model=False, label_smoothing=0.1, labels_offset=0, learning_rate=0.001, learning_rate_decay_factor=0.98, learning_rate_decay_type='exponential', log_every_n_steps=10, master='', max_eval_num_batches=None, max_number_of_steps=30000, model_dir='s3://sagemaker-us-east-2-322537213286/1593562844-img-classifier-training-job/model', model_name='mobilenet_v1_025', momentum=0.9, moving_average_decay=0.9999, num_clones=1, num_epochs_per_decay=2.5, num_frames=None, num_gpus='1', num_preprocessing_threads=4, num_ps_tasks=0, num_readers=4, opt_epsilon=1.0, optimizer='rmsprop', output_data_dir='/opt/ml/output/data', output_dir='/opt/ml/output', preprocessing_name='mobilenet_v1', quantize=False, quantize_delay=-1, replicas_to_aggregate=1, rmsprop_decay=0.9, rmsprop_momentum=0.9, save_interval_secs=600, save_summaries_secs=300, sync_replicas=False, task=0, test_num_data=738, train_dir='/opt/ml/model', train_num_data=6650, trainable_scopes=None, use_grayscale=False, warmup_epochs=0, weight_decay=4e-05, worker_replicas=1, write_text_graphdef=False), [])\u001b[0m\n",
      "\u001b[32mWARNING: Logging before flag parsing goes to stderr.\u001b[0m\n",
      "\u001b[32mW0701 01:05:51.895298 140367827080960 deprecation_wrapper.py:119] From image_classifier.py:603: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:51.895550 140367827080960 deprecation_wrapper.py:119] From image_classifier.py:603: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:51.897871 140367827080960 deprecation.py:323] From image_classifier.py:617: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mPlease switch to tf.train.create_global_step\u001b[0m\n",
      "\u001b[32mfile_pattern : None\u001b[0m\n",
      "\u001b[32mW0701 01:05:51.903671 140367827080960 deprecation_wrapper.py:119] From /opt/ml/code/datasets/captured_dataset.py:72: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:51.904398 140367827080960 deprecation_wrapper.py:119] From /opt/ml/code/datasets/captured_dataset.py:88: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:51.904553 140367827080960 deprecation_wrapper.py:119] From /opt/ml/code/datasets/dataset_utils.py:206: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\u001b[0m\n",
      "\u001b[32mlabels_to_names : {0: u'Abyssinian', 1: u'Bengal', 2: u'Birman', 3: u'Bombay', 4: u'British_Shorthair', 5: u'Egyptian_Mau', 6: u'Maine_Coon', 7: u'Persian', 8: u'Ragdoll', 9: u'Russian_Blue', 10: u'Siamese', 11: u'Sphynx', 12: u'american_bulldog', 13: u'american_pit_bull_terrier', 14: u'basset_hound', 15: u'beagle', 16: u'boxer', 17: u'chihuahua', 18: u'english_cocker_spaniel', 19: u'english_setter', 20: u'german_shorthaired', 21: u'great_pyrenees', 22: u'havanese', 23: u'japanese_chin', 24: u'keeshond', 25: u'leonberger', 26: u'miniature_pinscher', 27: u'newfoundland', 28: u'pomeranian', 29: u'pug', 30: u'saint_bernard', 31: u'samoyed', 32: u'scottish_terrier', 33: u'shiba_inu', 34: u'staffordshire_bull_terrier', 35: u'wheaten_terrier', 36: u'yorkshire_terrier'}\u001b[0m\n",
      "\u001b[32mtrain_args.use_grayscale : False\u001b[0m\n",
      "\u001b[32mW0701 01:05:51.905272 140367827080960 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py:246: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\u001b[0m\n",
      "\u001b[32mW0701 01:05:51.910594 140367827080960 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\u001b[0m\n",
      "\u001b[32mW0701 01:05:51.911380 140367827080960 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\u001b[0m\n",
      "\u001b[32mW0701 01:05:51.912909 140367827080960 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:199: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[32mW0701 01:05:51.913935 140367827080960 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[32mW0701 01:05:51.921364 140367827080960 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py:95: __init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\u001b[0m\n",
      "\u001b[32mW0701 01:05:52.027386 140367827080960 deprecation_wrapper.py:119] From /opt/ml/code/preprocessing/inception_preprocessing.py:206: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:52.029454 140367827080960 deprecation.py:323] From /opt/ml/code/preprocessing/inception_preprocessing.py:148: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32m`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\u001b[0m\n",
      "\u001b[32mW0701 01:05:52.034881 140367827080960 deprecation_wrapper.py:119] From /opt/ml/code/preprocessing/inception_preprocessing.py:38: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:52.038800 140367827080960 deprecation_wrapper.py:119] From /opt/ml/code/preprocessing/inception_preprocessing.py:230: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:52.078449 140367827080960 deprecation.py:323] From image_classifier.py:666: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\u001b[0m\n",
      "\u001b[32mW0701 01:05:52.095829 140367827080960 deprecation_wrapper.py:119] From image_classifier.py:693: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:52.095979 140367827080960 deprecation_wrapper.py:119] From image_classifier.py:693: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:52.096355 140367827080960 deprecation_wrapper.py:119] From /opt/ml/code/deployment/model_deploy.py:192: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:53.739583 140367827080960 deprecation.py:323] From image_classifier.py:689: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mUse tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\u001b[0m\n",
      "\u001b[32mW0701 01:05:53.746592 140367827080960 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:373: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\n",
      "\u001b[0m\n",
      "\u001b[32mFuture major versions of TensorFlow will allow gradients to flow\u001b[0m\n",
      "\u001b[32minto the labels input on backprop by default.\n",
      "\u001b[0m\n",
      "\u001b[32mSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:53.780847 140367827080960 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:374: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mUse tf.losses.compute_weighted_loss instead.\u001b[0m\n",
      "\u001b[32mW0701 01:05:53.789499 140367827080960 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:152: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mDeprecated in favor of operator or tf.math.divide.\u001b[0m\n",
      "\u001b[32mW0701 01:05:53.791292 140367827080960 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:154: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mUse tf.where in 2.0, which has the same broadcast rule as np.where\u001b[0m\n",
      "\u001b[32mW0701 01:05:53.799449 140367827080960 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:121: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mUse tf.losses.add_loss instead.\u001b[0m\n",
      "\u001b[32mW0701 01:05:53.799979 140367827080960 deprecation_wrapper.py:119] From image_classifier.py:707: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:53.801178 140367827080960 deprecation_wrapper.py:119] From image_classifier.py:708: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:54.498862 140367827080960 deprecation_wrapper.py:119] From image_classifier.py:262: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:54.504287 140367827080960 deprecation_wrapper.py:119] From image_classifier.py:333: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:54.505676 140367827080960 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/moving_averages.py:433: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\u001b[0m\n",
      "\u001b[32mW0701 01:05:58.284095 140367827080960 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/rmsprop.py:119: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m\n",
      "\u001b[32mW0701 01:05:59.264858 140367827080960 deprecation_wrapper.py:119] From image_classifier.py:782: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:59.271650 140367827080960 deprecation_wrapper.py:119] From image_classifier.py:379: The name tf.gfile.IsDirectory is deprecated. Please use tf.io.gfile.isdir instead.\n",
      "\u001b[0m\n",
      "\u001b[32mW0701 01:05:59.271872 140367827080960 deprecation_wrapper.py:119] From image_classifier.py:385: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\u001b[0m\n",
      "\u001b[32mI0701 01:05:59.271962 140367827080960 image_classifier.py:385] Fine-tuning from fine_tune_checkpoint/model.ckpt-25000\u001b[0m\n",
      "\u001b[32mW0701 01:06:00.108688 140367827080960 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:742: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mPlease switch to tf.train.MonitoredTrainingSession\u001b[0m\n",
      "\u001b[32m2020-07-01 01:06:03.567511: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[32mW0701 01:06:04.043914 140367827080960 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1282: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mUse standard file APIs to check for files with this prefix.\u001b[0m\n",
      "\u001b[32mI0701 01:06:04.045963 140367827080960 saver.py:1286] Restoring parameters from fine_tune_checkpoint/model.ckpt-25000\u001b[0m\n",
      "\u001b[32mI0701 01:06:04.317832 140367827080960 session_manager.py:500] Running local_init_op.\u001b[0m\n",
      "\u001b[32mI0701 01:06:04.411531 140367827080960 session_manager.py:502] Done running local_init_op.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:06:08.440344 140367827080960 learning.py:754] Starting Session.\u001b[0m\n",
      "\u001b[32mI0701 01:06:08.552064 140361938757376 supervisor.py:1117] Saving checkpoint to path /opt/ml/model/model.ckpt\u001b[0m\n",
      "\u001b[32mI0701 01:06:08.557351 140367827080960 learning.py:768] Starting Queues.\u001b[0m\n",
      "\u001b[32mI0701 01:06:09.560029 140361930364672 supervisor.py:1099] global_step/sec: 0\u001b[0m\n",
      "\u001b[32mI0701 01:06:17.518559 140361921971968 supervisor.py:1050] Recording summary at step 1.\u001b[0m\n",
      "\u001b[32mI0701 01:06:19.704246 140367827080960 learning.py:507] global step 10: loss = 1.4531 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:22.113562 140367827080960 learning.py:507] global step 20: loss = 1.4690 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:24.505810 140367827080960 learning.py:507] global step 30: loss = 1.6307 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:26.911880 140367827080960 learning.py:507] global step 40: loss = 1.6451 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:29.301457 140367827080960 learning.py:507] global step 50: loss = 1.5252 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:31.664398 140367827080960 learning.py:507] global step 60: loss = 1.5692 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:34.036607 140367827080960 learning.py:507] global step 70: loss = 1.4749 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:36.447139 140367827080960 learning.py:507] global step 80: loss = 1.5741 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:38.866463 140367827080960 learning.py:507] global step 90: loss = 1.5525 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:41.273219 140367827080960 learning.py:507] global step 100: loss = 1.7108 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:43.630337 140367827080960 learning.py:507] global step 110: loss = 1.5813 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:46.011765 140367827080960 learning.py:507] global step 120: loss = 1.5098 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:48.398516 140367827080960 learning.py:507] global step 130: loss = 1.5282 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:50.770853 140367827080960 learning.py:507] global step 140: loss = 1.5538 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:53.144618 140367827080960 learning.py:507] global step 150: loss = 1.5691 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:55.504601 140367827080960 learning.py:507] global step 160: loss = 1.5915 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:06:57.867203 140367827080960 learning.py:507] global step 170: loss = 1.7995 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:00.211921 140367827080960 learning.py:507] global step 180: loss = 1.4573 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:02.563986 140367827080960 learning.py:507] global step 190: loss = 1.5190 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:04.957207 140367827080960 learning.py:507] global step 200: loss = 1.7137 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:07.298266 140367827080960 learning.py:507] global step 210: loss = 1.4705 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:09.657027 140367827080960 learning.py:507] global step 220: loss = 1.6641 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:12.038957 140367827080960 learning.py:507] global step 230: loss = 1.4346 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:14.420455 140367827080960 learning.py:507] global step 240: loss = 1.4995 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:16.804271 140367827080960 learning.py:507] global step 250: loss = 1.5009 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:19.196115 140367827080960 learning.py:507] global step 260: loss = 1.4505 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:21.579765 140367827080960 learning.py:507] global step 270: loss = 1.6907 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:23.947774 140367827080960 learning.py:507] global step 280: loss = 1.5734 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:26.370793 140367827080960 learning.py:507] global step 290: loss = 1.7376 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:28.735438 140367827080960 learning.py:507] global step 300: loss = 1.6139 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:31.097176 140367827080960 learning.py:507] global step 310: loss = 1.4672 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:33.457148 140367827080960 learning.py:507] global step 320: loss = 1.6402 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:35.812460 140367827080960 learning.py:507] global step 330: loss = 1.5093 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:38.174220 140367827080960 learning.py:507] global step 340: loss = 1.5676 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:40.570343 140367827080960 learning.py:507] global step 350: loss = 1.5352 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:42.926881 140367827080960 learning.py:507] global step 360: loss = 1.5789 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:45.271203 140367827080960 learning.py:507] global step 370: loss = 1.5533 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:47.670661 140367827080960 learning.py:507] global step 380: loss = 1.4670 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:50.024188 140367827080960 learning.py:507] global step 390: loss = 1.4898 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:52.401465 140367827080960 learning.py:507] global step 400: loss = 1.6475 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:54.782315 140367827080960 learning.py:507] global step 410: loss = 1.5108 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:57.153973 140367827080960 learning.py:507] global step 420: loss = 1.4035 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:07:59.550995 140367827080960 learning.py:507] global step 430: loss = 1.5717 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:01.935820 140367827080960 learning.py:507] global step 440: loss = 1.4888 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:04.327034 140367827080960 learning.py:507] global step 450: loss = 1.4032 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:06.666198 140367827080960 learning.py:507] global step 460: loss = 1.4886 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:09.043714 140367827080960 learning.py:507] global step 470: loss = 1.6390 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:11.425213 140367827080960 learning.py:507] global step 480: loss = 1.6134 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:13.791209 140367827080960 learning.py:507] global step 490: loss = 1.5861 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:16.185842 140367827080960 learning.py:507] global step 500: loss = 1.4981 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:18.563647 140367827080960 learning.py:507] global step 510: loss = 1.6196 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:20.956135 140367827080960 learning.py:507] global step 520: loss = 1.5191 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:23.355743 140367827080960 learning.py:507] global step 530: loss = 1.4941 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:25.736202 140367827080960 learning.py:507] global step 540: loss = 1.4486 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:28.129220 140367827080960 learning.py:507] global step 550: loss = 1.5245 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:30.511703 140367827080960 learning.py:507] global step 560: loss = 1.6232 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:32.896677 140367827080960 learning.py:507] global step 570: loss = 1.5470 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:35.274763 140367827080960 learning.py:507] global step 580: loss = 1.5930 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:37.652719 140367827080960 learning.py:507] global step 590: loss = 1.6398 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:40.038387 140367827080960 learning.py:507] global step 600: loss = 1.6606 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:42.412755 140367827080960 learning.py:507] global step 610: loss = 1.6443 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:44.787303 140367827080960 learning.py:507] global step 620: loss = 1.4892 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:47.165622 140367827080960 learning.py:507] global step 630: loss = 1.4618 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:49.559263 140367827080960 learning.py:507] global step 640: loss = 1.5775 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:51.939680 140367827080960 learning.py:507] global step 650: loss = 1.4750 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:54.334935 140367827080960 learning.py:507] global step 660: loss = 1.5291 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:56.718918 140367827080960 learning.py:507] global step 670: loss = 1.4669 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:08:59.103755 140367827080960 learning.py:507] global step 680: loss = 1.6563 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:01.459146 140367827080960 learning.py:507] global step 690: loss = 1.5335 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:03.856709 140367827080960 learning.py:507] global step 700: loss = 1.4445 (0.240 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:09:06.242714 140367827080960 learning.py:507] global step 710: loss = 1.4501 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:08.618967 140367827080960 learning.py:507] global step 720: loss = 1.4182 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:10.998249 140367827080960 learning.py:507] global step 730: loss = 1.6087 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:13.370057 140367827080960 learning.py:507] global step 740: loss = 1.4693 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:15.751003 140367827080960 learning.py:507] global step 750: loss = 1.3897 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:18.159667 140367827080960 learning.py:507] global step 760: loss = 1.4677 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:20.527493 140367827080960 learning.py:507] global step 770: loss = 1.4808 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:22.931441 140367827080960 learning.py:507] global step 780: loss = 1.5765 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:25.301990 140367827080960 learning.py:507] global step 790: loss = 1.5478 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:27.680239 140367827080960 learning.py:507] global step 800: loss = 1.5349 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:30.050668 140367827080960 learning.py:507] global step 810: loss = 1.4577 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:32.435694 140367827080960 learning.py:507] global step 820: loss = 1.5690 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:34.821012 140367827080960 learning.py:507] global step 830: loss = 1.6675 (0.253 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:37.182410 140367827080960 learning.py:507] global step 840: loss = 1.4440 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:39.543658 140367827080960 learning.py:507] global step 850: loss = 1.4821 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:41.929546 140367827080960 learning.py:507] global step 860: loss = 1.3956 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:44.338692 140367827080960 learning.py:507] global step 870: loss = 1.5594 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:46.712382 140367827080960 learning.py:507] global step 880: loss = 1.5854 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:49.098675 140367827080960 learning.py:507] global step 890: loss = 1.5446 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:51.465935 140367827080960 learning.py:507] global step 900: loss = 1.4645 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:53.830400 140367827080960 learning.py:507] global step 910: loss = 1.4523 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:56.171868 140367827080960 learning.py:507] global step 920: loss = 1.4800 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:09:58.519231 140367827080960 learning.py:507] global step 930: loss = 1.5401 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:00.906745 140367827080960 learning.py:507] global step 940: loss = 1.7064 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:03.291254 140367827080960 learning.py:507] global step 950: loss = 1.4809 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:05.644965 140367827080960 learning.py:507] global step 960: loss = 1.5999 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:08.011327 140367827080960 learning.py:507] global step 970: loss = 1.4462 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:10.396740 140367827080960 learning.py:507] global step 980: loss = 1.4617 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:12.772485 140367827080960 learning.py:507] global step 990: loss = 1.5321 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:15.157124 140367827080960 learning.py:507] global step 1000: loss = 1.5165 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:17.543839 140367827080960 learning.py:507] global step 1010: loss = 1.5683 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:19.915479 140367827080960 learning.py:507] global step 1020: loss = 1.4993 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:22.278959 140367827080960 learning.py:507] global step 1030: loss = 1.5260 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:24.617777 140367827080960 learning.py:507] global step 1040: loss = 1.4647 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:27.010765 140367827080960 learning.py:507] global step 1050: loss = 1.5097 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:29.358809 140367827080960 learning.py:507] global step 1060: loss = 1.5073 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:31.739017 140367827080960 learning.py:507] global step 1070: loss = 1.6061 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:34.110903 140367827080960 learning.py:507] global step 1080: loss = 1.5410 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:36.482688 140367827080960 learning.py:507] global step 1090: loss = 1.6790 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:38.840699 140367827080960 learning.py:507] global step 1100: loss = 1.4654 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:41.238090 140367827080960 learning.py:507] global step 1110: loss = 1.5797 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:43.604028 140367827080960 learning.py:507] global step 1120: loss = 1.4274 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:45.981822 140367827080960 learning.py:507] global step 1130: loss = 1.5141 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:48.358789 140367827080960 learning.py:507] global step 1140: loss = 1.6347 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:50.736083 140367827080960 learning.py:507] global step 1150: loss = 1.4178 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:53.115161 140367827080960 learning.py:507] global step 1160: loss = 1.5887 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:55.532768 140367827080960 learning.py:507] global step 1170: loss = 1.5491 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:10:57.911847 140367827080960 learning.py:507] global step 1180: loss = 1.4098 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:00.287456 140367827080960 learning.py:507] global step 1190: loss = 1.3211 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:02.670308 140367827080960 learning.py:507] global step 1200: loss = 1.5898 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:05.030972 140367827080960 learning.py:507] global step 1210: loss = 1.5265 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:07.439496 140367827080960 learning.py:507] global step 1220: loss = 1.4967 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:08.553111 140361930364672 supervisor.py:1099] global_step/sec: 4.09374\u001b[0m\n",
      "\u001b[32mI0701 01:11:10.401632 140361921971968 supervisor.py:1050] Recording summary at step 1227.\u001b[0m\n",
      "\u001b[32mI0701 01:11:10.969728 140367827080960 learning.py:507] global step 1230: loss = 1.5425 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:13.332117 140367827080960 learning.py:507] global step 1240: loss = 1.6963 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:15.697439 140367827080960 learning.py:507] global step 1250: loss = 1.5137 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:18.034989 140367827080960 learning.py:507] global step 1260: loss = 1.4804 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:20.424585 140367827080960 learning.py:507] global step 1270: loss = 1.3991 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:22.796190 140367827080960 learning.py:507] global step 1280: loss = 1.5180 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:25.150726 140367827080960 learning.py:507] global step 1290: loss = 1.4790 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:27.544276 140367827080960 learning.py:507] global step 1300: loss = 1.5406 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:29.945271 140367827080960 learning.py:507] global step 1310: loss = 1.5647 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:32.344444 140367827080960 learning.py:507] global step 1320: loss = 1.5102 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:34.708762 140367827080960 learning.py:507] global step 1330: loss = 1.4745 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:37.083297 140367827080960 learning.py:507] global step 1340: loss = 1.5212 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:39.507209 140367827080960 learning.py:507] global step 1350: loss = 1.5482 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:41.889889 140367827080960 learning.py:507] global step 1360: loss = 1.6108 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:44.254646 140367827080960 learning.py:507] global step 1370: loss = 1.4400 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:46.635662 140367827080960 learning.py:507] global step 1380: loss = 1.5354 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:49.038778 140367827080960 learning.py:507] global step 1390: loss = 1.6356 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:51.441129 140367827080960 learning.py:507] global step 1400: loss = 1.4564 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:53.843251 140367827080960 learning.py:507] global step 1410: loss = 1.5346 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:56.227720 140367827080960 learning.py:507] global step 1420: loss = 1.5037 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:11:58.607671 140367827080960 learning.py:507] global step 1430: loss = 1.4455 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:00.990346 140367827080960 learning.py:507] global step 1440: loss = 1.5606 (0.232 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:12:03.386049 140367827080960 learning.py:507] global step 1450: loss = 1.5000 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:05.725855 140367827080960 learning.py:507] global step 1460: loss = 1.5593 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:08.093163 140367827080960 learning.py:507] global step 1470: loss = 1.6935 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:10.477116 140367827080960 learning.py:507] global step 1480: loss = 1.4750 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:12.837841 140367827080960 learning.py:507] global step 1490: loss = 1.5834 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:15.227312 140367827080960 learning.py:507] global step 1500: loss = 1.5428 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:17.596560 140367827080960 learning.py:507] global step 1510: loss = 1.4488 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:19.987706 140367827080960 learning.py:507] global step 1520: loss = 1.4706 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:22.360666 140367827080960 learning.py:507] global step 1530: loss = 1.5906 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:24.752887 140367827080960 learning.py:507] global step 1540: loss = 1.6452 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:27.125961 140367827080960 learning.py:507] global step 1550: loss = 1.4936 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:29.519902 140367827080960 learning.py:507] global step 1560: loss = 1.5324 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:31.931755 140367827080960 learning.py:507] global step 1570: loss = 1.4491 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:34.310019 140367827080960 learning.py:507] global step 1580: loss = 1.5697 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:36.684215 140367827080960 learning.py:507] global step 1590: loss = 1.4980 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:39.045618 140367827080960 learning.py:507] global step 1600: loss = 1.4630 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:41.411001 140367827080960 learning.py:507] global step 1610: loss = 1.5746 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:43.772294 140367827080960 learning.py:507] global step 1620: loss = 1.5159 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:46.130565 140367827080960 learning.py:507] global step 1630: loss = 1.4680 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:48.510040 140367827080960 learning.py:507] global step 1640: loss = 1.5506 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:50.891469 140367827080960 learning.py:507] global step 1650: loss = 1.4047 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:53.252801 140367827080960 learning.py:507] global step 1660: loss = 1.5515 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:55.623785 140367827080960 learning.py:507] global step 1670: loss = 1.5581 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:12:58.024943 140367827080960 learning.py:507] global step 1680: loss = 1.4279 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:00.430126 140367827080960 learning.py:507] global step 1690: loss = 1.5403 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:02.782702 140367827080960 learning.py:507] global step 1700: loss = 1.6195 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:05.153462 140367827080960 learning.py:507] global step 1710: loss = 1.4462 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:07.525441 140367827080960 learning.py:507] global step 1720: loss = 1.5113 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:09.867117 140367827080960 learning.py:507] global step 1730: loss = 1.6308 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:12.216167 140367827080960 learning.py:507] global step 1740: loss = 1.4481 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:14.564035 140367827080960 learning.py:507] global step 1750: loss = 1.5463 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:16.948940 140367827080960 learning.py:507] global step 1760: loss = 1.4628 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:19.290019 140367827080960 learning.py:507] global step 1770: loss = 1.5000 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:21.694523 140367827080960 learning.py:507] global step 1780: loss = 1.4487 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:24.086353 140367827080960 learning.py:507] global step 1790: loss = 1.4772 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:26.466276 140367827080960 learning.py:507] global step 1800: loss = 1.6498 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:28.852942 140367827080960 learning.py:507] global step 1810: loss = 1.4946 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:31.218658 140367827080960 learning.py:507] global step 1820: loss = 1.3961 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:33.601551 140367827080960 learning.py:507] global step 1830: loss = 1.3788 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:35.985677 140367827080960 learning.py:507] global step 1840: loss = 1.5473 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:38.369267 140367827080960 learning.py:507] global step 1850: loss = 1.4724 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:40.743031 140367827080960 learning.py:507] global step 1860: loss = 1.5255 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:43.096159 140367827080960 learning.py:507] global step 1870: loss = 1.4918 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:45.488689 140367827080960 learning.py:507] global step 1880: loss = 1.4830 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:47.882117 140367827080960 learning.py:507] global step 1890: loss = 1.5137 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:50.251703 140367827080960 learning.py:507] global step 1900: loss = 1.4238 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:52.627567 140367827080960 learning.py:507] global step 1910: loss = 1.5651 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:55.037355 140367827080960 learning.py:507] global step 1920: loss = 1.4690 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:57.420445 140367827080960 learning.py:507] global step 1930: loss = 1.5717 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:13:59.783510 140367827080960 learning.py:507] global step 1940: loss = 1.4948 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:02.154934 140367827080960 learning.py:507] global step 1950: loss = 1.5321 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:04.531802 140367827080960 learning.py:507] global step 1960: loss = 1.4172 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:06.913347 140367827080960 learning.py:507] global step 1970: loss = 1.4375 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:09.293092 140367827080960 learning.py:507] global step 1980: loss = 1.4734 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:11.647367 140367827080960 learning.py:507] global step 1990: loss = 1.4926 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:14.041722 140367827080960 learning.py:507] global step 2000: loss = 1.4766 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:16.412034 140367827080960 learning.py:507] global step 2010: loss = 1.4748 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:18.806113 140367827080960 learning.py:507] global step 2020: loss = 1.4540 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:21.176312 140367827080960 learning.py:507] global step 2030: loss = 1.6117 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:23.547766 140367827080960 learning.py:507] global step 2040: loss = 1.5428 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:25.902574 140367827080960 learning.py:507] global step 2050: loss = 1.4625 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:28.265331 140367827080960 learning.py:507] global step 2060: loss = 1.4840 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:30.687570 140367827080960 learning.py:507] global step 2070: loss = 1.5850 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:33.071935 140367827080960 learning.py:507] global step 2080: loss = 1.5612 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:35.441431 140367827080960 learning.py:507] global step 2090: loss = 1.5468 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:37.781265 140367827080960 learning.py:507] global step 2100: loss = 1.5535 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:40.155385 140367827080960 learning.py:507] global step 2110: loss = 1.3905 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:42.549226 140367827080960 learning.py:507] global step 2120: loss = 1.5217 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:44.913781 140367827080960 learning.py:507] global step 2130: loss = 1.5300 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:47.304723 140367827080960 learning.py:507] global step 2140: loss = 1.4747 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:49.663054 140367827080960 learning.py:507] global step 2150: loss = 1.4332 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:52.079845 140367827080960 learning.py:507] global step 2160: loss = 1.4175 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:54.423393 140367827080960 learning.py:507] global step 2170: loss = 1.4600 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:56.779859 140367827080960 learning.py:507] global step 2180: loss = 1.5796 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:14:59.184763 140367827080960 learning.py:507] global step 2190: loss = 1.5123 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:01.597641 140367827080960 learning.py:507] global step 2200: loss = 1.5322 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:03.940963 140367827080960 learning.py:507] global step 2210: loss = 1.5036 (0.250 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:15:06.339648 140367827080960 learning.py:507] global step 2220: loss = 1.4723 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:08.697695 140367827080960 learning.py:507] global step 2230: loss = 1.5058 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:11.084142 140367827080960 learning.py:507] global step 2240: loss = 1.6195 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:13.466917 140367827080960 learning.py:507] global step 2250: loss = 1.5645 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:15.833642 140367827080960 learning.py:507] global step 2260: loss = 1.4441 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:18.173367 140367827080960 learning.py:507] global step 2270: loss = 1.6079 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:20.550326 140367827080960 learning.py:507] global step 2280: loss = 1.5767 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:22.915978 140367827080960 learning.py:507] global step 2290: loss = 1.6251 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:25.285830 140367827080960 learning.py:507] global step 2300: loss = 1.5318 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:27.687809 140367827080960 learning.py:507] global step 2310: loss = 1.4298 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:30.050132 140367827080960 learning.py:507] global step 2320: loss = 1.4572 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:32.421915 140367827080960 learning.py:507] global step 2330: loss = 1.5002 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:34.801223 140367827080960 learning.py:507] global step 2340: loss = 1.5133 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:37.179348 140367827080960 learning.py:507] global step 2350: loss = 1.5491 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:39.556035 140367827080960 learning.py:507] global step 2360: loss = 1.4681 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:41.901946 140367827080960 learning.py:507] global step 2370: loss = 1.5738 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:44.281096 140367827080960 learning.py:507] global step 2380: loss = 1.4442 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:46.645092 140367827080960 learning.py:507] global step 2390: loss = 1.4830 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:49.038423 140367827080960 learning.py:507] global step 2400: loss = 1.5026 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:51.411468 140367827080960 learning.py:507] global step 2410: loss = 1.4368 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:53.793534 140367827080960 learning.py:507] global step 2420: loss = 1.4653 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:56.182017 140367827080960 learning.py:507] global step 2430: loss = 1.5415 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:15:58.531819 140367827080960 learning.py:507] global step 2440: loss = 1.5419 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:00.904620 140367827080960 learning.py:507] global step 2450: loss = 1.4643 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:03.291121 140367827080960 learning.py:507] global step 2460: loss = 1.4922 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:05.697813 140367827080960 learning.py:507] global step 2470: loss = 1.6069 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:08.082056 140367827080960 learning.py:507] global step 2480: loss = 1.4489 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:08.552041 140361938757376 supervisor.py:1117] Saving checkpoint to path /opt/ml/model/model.ckpt\u001b[0m\n",
      "\u001b[32mI0701 01:16:08.553523 140361930364672 supervisor.py:1099] global_step/sec: 4.18999\u001b[0m\n",
      "\u001b[32mI0701 01:16:10.089807 140361921971968 supervisor.py:1050] Recording summary at step 2484.\u001b[0m\n",
      "\u001b[32mI0701 01:16:11.480958 140367827080960 learning.py:507] global step 2490: loss = 1.4615 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:13.874604 140367827080960 learning.py:507] global step 2500: loss = 1.5297 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:16.264967 140367827080960 learning.py:507] global step 2510: loss = 1.4977 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:18.644464 140367827080960 learning.py:507] global step 2520: loss = 1.5037 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:21.052933 140367827080960 learning.py:507] global step 2530: loss = 1.6000 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:23.452318 140367827080960 learning.py:507] global step 2540: loss = 1.5203 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:25.826680 140367827080960 learning.py:507] global step 2550: loss = 1.4775 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:28.194612 140367827080960 learning.py:507] global step 2560: loss = 1.4728 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:30.574170 140367827080960 learning.py:507] global step 2570: loss = 1.5161 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:32.979912 140367827080960 learning.py:507] global step 2580: loss = 1.3252 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:35.343993 140367827080960 learning.py:507] global step 2590: loss = 1.4777 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:37.736934 140367827080960 learning.py:507] global step 2600: loss = 1.4855 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:40.186100 140367827080960 learning.py:507] global step 2610: loss = 1.4682 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:42.571069 140367827080960 learning.py:507] global step 2620: loss = 1.4321 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:44.946893 140367827080960 learning.py:507] global step 2630: loss = 1.5002 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:47.336307 140367827080960 learning.py:507] global step 2640: loss = 1.4971 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:49.705760 140367827080960 learning.py:507] global step 2650: loss = 1.3396 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:52.067095 140367827080960 learning.py:507] global step 2660: loss = 1.4736 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:54.476170 140367827080960 learning.py:507] global step 2670: loss = 1.5610 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:56.846817 140367827080960 learning.py:507] global step 2680: loss = 1.5075 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:16:59.217441 140367827080960 learning.py:507] global step 2690: loss = 1.5876 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:01.610704 140367827080960 learning.py:507] global step 2700: loss = 1.4169 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:03.984282 140367827080960 learning.py:507] global step 2710: loss = 1.3941 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:06.365472 140367827080960 learning.py:507] global step 2720: loss = 1.4544 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:08.761662 140367827080960 learning.py:507] global step 2730: loss = 1.5008 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:11.116844 140367827080960 learning.py:507] global step 2740: loss = 1.6158 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:13.518850 140367827080960 learning.py:507] global step 2750: loss = 1.5235 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:15.866007 140367827080960 learning.py:507] global step 2760: loss = 1.4466 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:18.229363 140367827080960 learning.py:507] global step 2770: loss = 1.4046 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:20.626602 140367827080960 learning.py:507] global step 2780: loss = 1.4561 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:22.973124 140367827080960 learning.py:507] global step 2790: loss = 1.5414 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:25.337356 140367827080960 learning.py:507] global step 2800: loss = 1.5533 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:27.724562 140367827080960 learning.py:507] global step 2810: loss = 1.4691 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:30.122757 140367827080960 learning.py:507] global step 2820: loss = 1.6115 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:32.501940 140367827080960 learning.py:507] global step 2830: loss = 1.4006 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:34.874737 140367827080960 learning.py:507] global step 2840: loss = 1.4248 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:37.260128 140367827080960 learning.py:507] global step 2850: loss = 1.5021 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:39.619617 140367827080960 learning.py:507] global step 2860: loss = 1.4189 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:41.974145 140367827080960 learning.py:507] global step 2870: loss = 1.5161 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:44.350224 140367827080960 learning.py:507] global step 2880: loss = 1.5818 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:46.702954 140367827080960 learning.py:507] global step 2890: loss = 1.4823 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:49.058692 140367827080960 learning.py:507] global step 2900: loss = 1.4388 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:51.442367 140367827080960 learning.py:507] global step 2910: loss = 1.5665 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:53.835160 140367827080960 learning.py:507] global step 2920: loss = 1.5335 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:56.217833 140367827080960 learning.py:507] global step 2930: loss = 1.4453 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:17:58.593350 140367827080960 learning.py:507] global step 2940: loss = 1.4648 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:00.986131 140367827080960 learning.py:507] global step 2950: loss = 1.5643 (0.240 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:18:03.383128 140367827080960 learning.py:507] global step 2960: loss = 1.4559 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:05.752002 140367827080960 learning.py:507] global step 2970: loss = 1.5315 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:08.127285 140367827080960 learning.py:507] global step 2980: loss = 1.3887 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:10.488058 140367827080960 learning.py:507] global step 2990: loss = 1.5295 (0.220 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:12.872689 140367827080960 learning.py:507] global step 3000: loss = 1.4215 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:15.249557 140367827080960 learning.py:507] global step 3010: loss = 1.4889 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:17.609390 140367827080960 learning.py:507] global step 3020: loss = 1.4214 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:19.974962 140367827080960 learning.py:507] global step 3030: loss = 1.6105 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:22.338599 140367827080960 learning.py:507] global step 3040: loss = 1.4441 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:24.732861 140367827080960 learning.py:507] global step 3050: loss = 1.5570 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:27.110606 140367827080960 learning.py:507] global step 3060: loss = 1.3381 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:29.498317 140367827080960 learning.py:507] global step 3070: loss = 1.5305 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:31.890666 140367827080960 learning.py:507] global step 3080: loss = 1.5053 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:34.265575 140367827080960 learning.py:507] global step 3090: loss = 1.3451 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:36.742533 140367827080960 learning.py:507] global step 3100: loss = 1.3627 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:39.112448 140367827080960 learning.py:507] global step 3110: loss = 1.4671 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:41.447109 140367827080960 learning.py:507] global step 3120: loss = 1.5039 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:43.828649 140367827080960 learning.py:507] global step 3130: loss = 1.3815 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:46.214306 140367827080960 learning.py:507] global step 3140: loss = 1.3916 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:48.576792 140367827080960 learning.py:507] global step 3150: loss = 1.3335 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:50.932276 140367827080960 learning.py:507] global step 3160: loss = 1.5766 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:53.315299 140367827080960 learning.py:507] global step 3170: loss = 1.4615 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:55.715519 140367827080960 learning.py:507] global step 3180: loss = 1.4466 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:18:58.085339 140367827080960 learning.py:507] global step 3190: loss = 1.4523 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:00.465496 140367827080960 learning.py:507] global step 3200: loss = 1.4653 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:02.836806 140367827080960 learning.py:507] global step 3210: loss = 1.4784 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:05.218206 140367827080960 learning.py:507] global step 3220: loss = 1.5469 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:07.612739 140367827080960 learning.py:507] global step 3230: loss = 1.5212 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:09.989327 140367827080960 learning.py:507] global step 3240: loss = 1.4714 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:12.364737 140367827080960 learning.py:507] global step 3250: loss = 1.4835 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:14.763161 140367827080960 learning.py:507] global step 3260: loss = 1.5279 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:17.130969 140367827080960 learning.py:507] global step 3270: loss = 1.4339 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:19.503022 140367827080960 learning.py:507] global step 3280: loss = 1.5019 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:21.899174 140367827080960 learning.py:507] global step 3290: loss = 1.4999 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:24.282744 140367827080960 learning.py:507] global step 3300: loss = 1.3508 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:26.653269 140367827080960 learning.py:507] global step 3310: loss = 1.3788 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:29.025975 140367827080960 learning.py:507] global step 3320: loss = 1.4767 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:31.412744 140367827080960 learning.py:507] global step 3330: loss = 1.5501 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:33.786766 140367827080960 learning.py:507] global step 3340: loss = 1.5761 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:36.165390 140367827080960 learning.py:507] global step 3350: loss = 1.4294 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:38.525753 140367827080960 learning.py:507] global step 3360: loss = 1.3639 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:40.909818 140367827080960 learning.py:507] global step 3370: loss = 1.5088 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:43.259524 140367827080960 learning.py:507] global step 3380: loss = 1.3980 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:45.655184 140367827080960 learning.py:507] global step 3390: loss = 1.4816 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:48.036092 140367827080960 learning.py:507] global step 3400: loss = 1.4905 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:50.396756 140367827080960 learning.py:507] global step 3410: loss = 1.5522 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:52.785089 140367827080960 learning.py:507] global step 3420: loss = 1.3722 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:55.168488 140367827080960 learning.py:507] global step 3430: loss = 1.4941 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:57.538700 140367827080960 learning.py:507] global step 3440: loss = 1.4962 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:19:59.907205 140367827080960 learning.py:507] global step 3450: loss = 1.5307 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:02.287921 140367827080960 learning.py:507] global step 3460: loss = 1.5157 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:04.664352 140367827080960 learning.py:507] global step 3470: loss = 1.4932 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:07.078815 140367827080960 learning.py:507] global step 3480: loss = 1.6024 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:09.465651 140367827080960 learning.py:507] global step 3490: loss = 1.3778 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:11.831433 140367827080960 learning.py:507] global step 3500: loss = 1.4743 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:14.209606 140367827080960 learning.py:507] global step 3510: loss = 1.5026 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:16.543370 140367827080960 learning.py:507] global step 3520: loss = 1.3633 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:18.933583 140367827080960 learning.py:507] global step 3530: loss = 1.4875 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:21.315962 140367827080960 learning.py:507] global step 3540: loss = 1.5075 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:23.720124 140367827080960 learning.py:507] global step 3550: loss = 1.5113 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:26.086572 140367827080960 learning.py:507] global step 3560: loss = 1.3891 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:28.478576 140367827080960 learning.py:507] global step 3570: loss = 1.4379 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:30.875840 140367827080960 learning.py:507] global step 3580: loss = 1.4760 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:33.221262 140367827080960 learning.py:507] global step 3590: loss = 1.5050 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:35.597659 140367827080960 learning.py:507] global step 3600: loss = 1.5316 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:37.993944 140367827080960 learning.py:507] global step 3610: loss = 1.3940 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:40.381412 140367827080960 learning.py:507] global step 3620: loss = 1.6050 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:42.772325 140367827080960 learning.py:507] global step 3630: loss = 1.4616 (0.248 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:45.147046 140367827080960 learning.py:507] global step 3640: loss = 1.4756 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:47.501389 140367827080960 learning.py:507] global step 3650: loss = 1.4193 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:49.870752 140367827080960 learning.py:507] global step 3660: loss = 1.6092 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:52.244267 140367827080960 learning.py:507] global step 3670: loss = 1.5336 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:54.620286 140367827080960 learning.py:507] global step 3680: loss = 1.3907 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:56.981142 140367827080960 learning.py:507] global step 3690: loss = 1.3622 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:20:59.362798 140367827080960 learning.py:507] global step 3700: loss = 1.4678 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:01.722613 140367827080960 learning.py:507] global step 3710: loss = 1.5972 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:04.091089 140367827080960 learning.py:507] global step 3720: loss = 1.4556 (0.239 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:21:06.437680 140367827080960 learning.py:507] global step 3730: loss = 1.4747 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:08.579910 140361930364672 supervisor.py:1099] global_step/sec: 4.18963\u001b[0m\n",
      "\u001b[32mI0701 01:21:09.797982 140367827080960 learning.py:507] global step 3740: loss = 1.4233 (0.404 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:09.855422 140361921971968 supervisor.py:1050] Recording summary at step 3740.\u001b[0m\n",
      "\u001b[32mI0701 01:21:12.135967 140367827080960 learning.py:507] global step 3750: loss = 1.4923 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:14.524939 140367827080960 learning.py:507] global step 3760: loss = 1.4659 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:16.868747 140367827080960 learning.py:507] global step 3770: loss = 1.4586 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:19.260210 140367827080960 learning.py:507] global step 3780: loss = 1.4747 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:21.638789 140367827080960 learning.py:507] global step 3790: loss = 1.4393 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:24.003123 140367827080960 learning.py:507] global step 3800: loss = 1.5496 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:26.366432 140367827080960 learning.py:507] global step 3810: loss = 1.4103 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:28.739660 140367827080960 learning.py:507] global step 3820: loss = 1.3548 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:31.126334 140367827080960 learning.py:507] global step 3830: loss = 1.6096 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:33.507049 140367827080960 learning.py:507] global step 3840: loss = 1.4604 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:35.886441 140367827080960 learning.py:507] global step 3850: loss = 1.5334 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:38.232176 140367827080960 learning.py:507] global step 3860: loss = 1.4491 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:40.609178 140367827080960 learning.py:507] global step 3870: loss = 1.4853 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:42.977091 140367827080960 learning.py:507] global step 3880: loss = 1.4715 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:45.374912 140367827080960 learning.py:507] global step 3890: loss = 1.5048 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:47.760305 140367827080960 learning.py:507] global step 3900: loss = 1.6524 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:50.133183 140367827080960 learning.py:507] global step 3910: loss = 1.4411 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:52.513968 140367827080960 learning.py:507] global step 3920: loss = 1.5200 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:54.896739 140367827080960 learning.py:507] global step 3930: loss = 1.5592 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:57.269581 140367827080960 learning.py:507] global step 3940: loss = 1.5592 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:21:59.631170 140367827080960 learning.py:507] global step 3950: loss = 1.4617 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:02.007680 140367827080960 learning.py:507] global step 3960: loss = 1.4454 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:04.390701 140367827080960 learning.py:507] global step 3970: loss = 1.5227 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:06.733473 140367827080960 learning.py:507] global step 3980: loss = 1.4504 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:09.105986 140367827080960 learning.py:507] global step 3990: loss = 1.4309 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:11.481709 140367827080960 learning.py:507] global step 4000: loss = 1.3890 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:13.880369 140367827080960 learning.py:507] global step 4010: loss = 1.5345 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:16.267477 140367827080960 learning.py:507] global step 4020: loss = 1.3674 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:18.641513 140367827080960 learning.py:507] global step 4030: loss = 1.5301 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:21.042088 140367827080960 learning.py:507] global step 4040: loss = 1.4883 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:23.421638 140367827080960 learning.py:507] global step 4050: loss = 1.4591 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:25.804265 140367827080960 learning.py:507] global step 4060: loss = 1.5239 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:28.173470 140367827080960 learning.py:507] global step 4070: loss = 1.4626 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:30.553642 140367827080960 learning.py:507] global step 4080: loss = 1.4745 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:32.960375 140367827080960 learning.py:507] global step 4090: loss = 1.3973 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:35.356045 140367827080960 learning.py:507] global step 4100: loss = 1.3341 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:37.703006 140367827080960 learning.py:507] global step 4110: loss = 1.4873 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:40.075180 140367827080960 learning.py:507] global step 4120: loss = 1.3932 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:42.439289 140367827080960 learning.py:507] global step 4130: loss = 1.4199 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:44.829524 140367827080960 learning.py:507] global step 4140: loss = 1.4019 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:47.197144 140367827080960 learning.py:507] global step 4150: loss = 1.5782 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:49.546215 140367827080960 learning.py:507] global step 4160: loss = 1.4795 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:51.920968 140367827080960 learning.py:507] global step 4170: loss = 1.5775 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:54.290957 140367827080960 learning.py:507] global step 4180: loss = 1.3525 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:56.673213 140367827080960 learning.py:507] global step 4190: loss = 1.5047 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:22:59.028423 140367827080960 learning.py:507] global step 4200: loss = 1.5518 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:01.416629 140367827080960 learning.py:507] global step 4210: loss = 1.4638 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:03.805780 140367827080960 learning.py:507] global step 4220: loss = 1.5536 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:06.177578 140367827080960 learning.py:507] global step 4230: loss = 1.4160 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:08.538461 140367827080960 learning.py:507] global step 4240: loss = 1.3925 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:10.910708 140367827080960 learning.py:507] global step 4250: loss = 1.3885 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:13.278095 140367827080960 learning.py:507] global step 4260: loss = 1.3903 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:15.682838 140367827080960 learning.py:507] global step 4270: loss = 1.3878 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:18.075706 140367827080960 learning.py:507] global step 4280: loss = 1.5075 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:20.454505 140367827080960 learning.py:507] global step 4290: loss = 1.4566 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:22.815886 140367827080960 learning.py:507] global step 4300: loss = 1.4385 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:25.196979 140367827080960 learning.py:507] global step 4310: loss = 1.5073 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:27.575155 140367827080960 learning.py:507] global step 4320: loss = 1.4441 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:29.944128 140367827080960 learning.py:507] global step 4330: loss = 1.5330 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:32.347664 140367827080960 learning.py:507] global step 4340: loss = 1.5986 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:34.712346 140367827080960 learning.py:507] global step 4350: loss = 1.4331 (0.253 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:37.077521 140367827080960 learning.py:507] global step 4360: loss = 1.5088 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:39.462125 140367827080960 learning.py:507] global step 4370: loss = 1.3383 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:41.851074 140367827080960 learning.py:507] global step 4380: loss = 1.5289 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:44.235749 140367827080960 learning.py:507] global step 4390: loss = 1.4781 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:46.614559 140367827080960 learning.py:507] global step 4400: loss = 1.4848 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:48.981477 140367827080960 learning.py:507] global step 4410: loss = 1.4301 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:51.359169 140367827080960 learning.py:507] global step 4420: loss = 1.3719 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:53.704961 140367827080960 learning.py:507] global step 4430: loss = 1.2957 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:23:56.102889 140367827080960 learning.py:507] global step 4440: loss = 1.4062 (0.244 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:23:58.459320 140367827080960 learning.py:507] global step 4450: loss = 1.3727 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:00.838200 140367827080960 learning.py:507] global step 4460: loss = 1.4726 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:03.212636 140367827080960 learning.py:507] global step 4470: loss = 1.4755 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:05.570899 140367827080960 learning.py:507] global step 4480: loss = 1.4353 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:07.944591 140367827080960 learning.py:507] global step 4490: loss = 1.4507 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:10.335042 140367827080960 learning.py:507] global step 4500: loss = 1.3758 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:12.703613 140367827080960 learning.py:507] global step 4510: loss = 1.5363 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:15.086867 140367827080960 learning.py:507] global step 4520: loss = 1.4932 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:17.484003 140367827080960 learning.py:507] global step 4530: loss = 1.4730 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:19.864043 140367827080960 learning.py:507] global step 4540: loss = 1.5224 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:22.223687 140367827080960 learning.py:507] global step 4550: loss = 1.4654 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:24.596775 140367827080960 learning.py:507] global step 4560: loss = 1.5650 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:26.960073 140367827080960 learning.py:507] global step 4570: loss = 1.4261 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:29.312673 140367827080960 learning.py:507] global step 4580: loss = 1.4797 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:31.708050 140367827080960 learning.py:507] global step 4590: loss = 1.6128 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:34.077174 140367827080960 learning.py:507] global step 4600: loss = 1.4575 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:36.452799 140367827080960 learning.py:507] global step 4610: loss = 1.2705 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:38.840919 140367827080960 learning.py:507] global step 4620: loss = 1.4713 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:41.229310 140367827080960 learning.py:507] global step 4630: loss = 1.5346 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:43.598948 140367827080960 learning.py:507] global step 4640: loss = 1.3997 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:46.002768 140367827080960 learning.py:507] global step 4650: loss = 1.3867 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:48.405257 140367827080960 learning.py:507] global step 4660: loss = 1.5084 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:50.798357 140367827080960 learning.py:507] global step 4670: loss = 1.4696 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:53.146437 140367827080960 learning.py:507] global step 4680: loss = 1.4347 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:55.504405 140367827080960 learning.py:507] global step 4690: loss = 1.4290 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:24:57.895273 140367827080960 learning.py:507] global step 4700: loss = 1.4704 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:00.258203 140367827080960 learning.py:507] global step 4710: loss = 1.5052 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:02.659019 140367827080960 learning.py:507] global step 4720: loss = 1.4535 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:05.028270 140367827080960 learning.py:507] global step 4730: loss = 1.4366 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:07.390343 140367827080960 learning.py:507] global step 4740: loss = 1.4829 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:09.765285 140367827080960 learning.py:507] global step 4750: loss = 1.3897 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:12.098197 140367827080960 learning.py:507] global step 4760: loss = 1.4301 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:14.474101 140367827080960 learning.py:507] global step 4770: loss = 1.5496 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:16.854722 140367827080960 learning.py:507] global step 4780: loss = 1.4158 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:19.250291 140367827080960 learning.py:507] global step 4790: loss = 1.3750 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:21.644654 140367827080960 learning.py:507] global step 4800: loss = 1.4852 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:24.022559 140367827080960 learning.py:507] global step 4810: loss = 1.4854 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:26.396977 140367827080960 learning.py:507] global step 4820: loss = 1.5119 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:28.784421 140367827080960 learning.py:507] global step 4830: loss = 1.4263 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:31.179258 140367827080960 learning.py:507] global step 4840: loss = 1.4781 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:33.549570 140367827080960 learning.py:507] global step 4850: loss = 1.4100 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:35.919594 140367827080960 learning.py:507] global step 4860: loss = 1.3877 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:38.292649 140367827080960 learning.py:507] global step 4870: loss = 1.4546 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:40.648606 140367827080960 learning.py:507] global step 4880: loss = 1.4012 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:43.043210 140367827080960 learning.py:507] global step 4890: loss = 1.4354 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:45.432207 140367827080960 learning.py:507] global step 4900: loss = 1.4696 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:47.809557 140367827080960 learning.py:507] global step 4910: loss = 1.5112 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:50.211647 140367827080960 learning.py:507] global step 4920: loss = 1.4734 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:52.596198 140367827080960 learning.py:507] global step 4930: loss = 1.4176 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:54.986890 140367827080960 learning.py:507] global step 4940: loss = 1.4075 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:57.365086 140367827080960 learning.py:507] global step 4950: loss = 1.3239 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:25:59.749941 140367827080960 learning.py:507] global step 4960: loss = 1.4782 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:02.130178 140367827080960 learning.py:507] global step 4970: loss = 1.3639 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:04.509710 140367827080960 learning.py:507] global step 4980: loss = 1.3456 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:06.875677 140367827080960 learning.py:507] global step 4990: loss = 1.4187 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:08.552118 140361938757376 supervisor.py:1117] Saving checkpoint to path /opt/ml/model/model.ckpt\u001b[0m\n",
      "\u001b[32mI0701 01:26:08.553527 140361930364672 supervisor.py:1099] global_step/sec: 4.1937\u001b[0m\n",
      "\u001b[32mI0701 01:26:10.094717 140361921971968 supervisor.py:1050] Recording summary at step 4998.\u001b[0m\n",
      "\u001b[32mI0701 01:26:10.356497 140367827080960 learning.py:507] global step 5000: loss = 1.4825 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:12.752671 140367827080960 learning.py:507] global step 5010: loss = 1.3709 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:15.129501 140367827080960 learning.py:507] global step 5020: loss = 1.4024 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:17.510142 140367827080960 learning.py:507] global step 5030: loss = 1.5271 (0.222 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:19.875504 140367827080960 learning.py:507] global step 5040: loss = 1.3749 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:22.244776 140367827080960 learning.py:507] global step 5050: loss = 1.4837 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:24.644309 140367827080960 learning.py:507] global step 5060: loss = 1.3655 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:27.004776 140367827080960 learning.py:507] global step 5070: loss = 1.2356 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:29.386787 140367827080960 learning.py:507] global step 5080: loss = 1.3886 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:31.784547 140367827080960 learning.py:507] global step 5090: loss = 1.4740 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:34.158966 140367827080960 learning.py:507] global step 5100: loss = 1.3791 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:36.520172 140367827080960 learning.py:507] global step 5110: loss = 1.3527 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:38.910053 140367827080960 learning.py:507] global step 5120: loss = 1.3952 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:41.284224 140367827080960 learning.py:507] global step 5130: loss = 1.4835 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:43.660269 140367827080960 learning.py:507] global step 5140: loss = 1.4347 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:46.036433 140367827080960 learning.py:507] global step 5150: loss = 1.4173 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:48.410152 140367827080960 learning.py:507] global step 5160: loss = 1.3568 (0.236 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:26:50.788121 140367827080960 learning.py:507] global step 5170: loss = 1.3727 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:53.141916 140367827080960 learning.py:507] global step 5180: loss = 1.4776 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:55.508317 140367827080960 learning.py:507] global step 5190: loss = 1.5357 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:26:57.909327 140367827080960 learning.py:507] global step 5200: loss = 1.3621 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:00.278306 140367827080960 learning.py:507] global step 5210: loss = 1.5503 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:02.643315 140367827080960 learning.py:507] global step 5220: loss = 1.5466 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:05.022949 140367827080960 learning.py:507] global step 5230: loss = 1.4786 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:07.394824 140367827080960 learning.py:507] global step 5240: loss = 1.4242 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:09.785290 140367827080960 learning.py:507] global step 5250: loss = 1.5172 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:12.191879 140367827080960 learning.py:507] global step 5260: loss = 1.4620 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:14.572673 140367827080960 learning.py:507] global step 5270: loss = 1.4540 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:16.938441 140367827080960 learning.py:507] global step 5280: loss = 1.5357 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:19.270905 140367827080960 learning.py:507] global step 5290: loss = 1.4727 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:21.609797 140367827080960 learning.py:507] global step 5300: loss = 1.4912 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:23.989449 140367827080960 learning.py:507] global step 5310: loss = 1.3527 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:26.360418 140367827080960 learning.py:507] global step 5320: loss = 1.3848 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:28.723752 140367827080960 learning.py:507] global step 5330: loss = 1.4323 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:31.082248 140367827080960 learning.py:507] global step 5340: loss = 1.5087 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:33.462692 140367827080960 learning.py:507] global step 5350: loss = 1.4511 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:35.850712 140367827080960 learning.py:507] global step 5360: loss = 1.4434 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:38.247677 140367827080960 learning.py:507] global step 5370: loss = 1.4835 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:40.643755 140367827080960 learning.py:507] global step 5380: loss = 1.4342 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:43.011516 140367827080960 learning.py:507] global step 5390: loss = 1.3873 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:45.388926 140367827080960 learning.py:507] global step 5400: loss = 1.3836 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:47.731506 140367827080960 learning.py:507] global step 5410: loss = 1.4347 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:50.122713 140367827080960 learning.py:507] global step 5420: loss = 1.4158 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:52.516431 140367827080960 learning.py:507] global step 5430: loss = 1.5171 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:54.918059 140367827080960 learning.py:507] global step 5440: loss = 1.3904 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:57.294605 140367827080960 learning.py:507] global step 5450: loss = 1.4245 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:27:59.646797 140367827080960 learning.py:507] global step 5460: loss = 1.4098 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:02.034894 140367827080960 learning.py:507] global step 5470: loss = 1.4884 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:04.413049 140367827080960 learning.py:507] global step 5480: loss = 1.3614 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:06.797909 140367827080960 learning.py:507] global step 5490: loss = 1.4645 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:09.184730 140367827080960 learning.py:507] global step 5500: loss = 1.4018 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:11.554244 140367827080960 learning.py:507] global step 5510: loss = 1.4459 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:13.958225 140367827080960 learning.py:507] global step 5520: loss = 1.4316 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:16.320395 140367827080960 learning.py:507] global step 5530: loss = 1.3890 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:18.717828 140367827080960 learning.py:507] global step 5540: loss = 1.4121 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:21.103605 140367827080960 learning.py:507] global step 5550: loss = 1.4576 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:23.493987 140367827080960 learning.py:507] global step 5560: loss = 1.4920 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:25.861697 140367827080960 learning.py:507] global step 5570: loss = 1.4770 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:28.234358 140367827080960 learning.py:507] global step 5580: loss = 1.3612 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:30.593017 140367827080960 learning.py:507] global step 5590: loss = 1.5755 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:32.952986 140367827080960 learning.py:507] global step 5600: loss = 1.3071 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:35.310280 140367827080960 learning.py:507] global step 5610: loss = 1.4999 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:37.704231 140367827080960 learning.py:507] global step 5620: loss = 1.4369 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:40.072190 140367827080960 learning.py:507] global step 5630: loss = 1.4174 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:42.449127 140367827080960 learning.py:507] global step 5640: loss = 1.4296 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:44.847697 140367827080960 learning.py:507] global step 5650: loss = 1.3512 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:47.232422 140367827080960 learning.py:507] global step 5660: loss = 1.5035 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:49.612101 140367827080960 learning.py:507] global step 5670: loss = 1.4372 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:51.999080 140367827080960 learning.py:507] global step 5680: loss = 1.4774 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:54.399955 140367827080960 learning.py:507] global step 5690: loss = 1.5091 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:56.789979 140367827080960 learning.py:507] global step 5700: loss = 1.4053 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:28:59.179460 140367827080960 learning.py:507] global step 5710: loss = 1.3996 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:01.560353 140367827080960 learning.py:507] global step 5720: loss = 1.4862 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:03.981251 140367827080960 learning.py:507] global step 5730: loss = 1.4779 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:06.344465 140367827080960 learning.py:507] global step 5740: loss = 1.5396 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:08.725289 140367827080960 learning.py:507] global step 5750: loss = 1.5386 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:11.119138 140367827080960 learning.py:507] global step 5760: loss = 1.4699 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:13.474596 140367827080960 learning.py:507] global step 5770: loss = 1.3993 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:15.840842 140367827080960 learning.py:507] global step 5780: loss = 1.3676 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:18.209635 140367827080960 learning.py:507] global step 5790: loss = 1.4713 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:20.602686 140367827080960 learning.py:507] global step 5800: loss = 1.5327 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:22.993204 140367827080960 learning.py:507] global step 5810: loss = 1.3651 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:25.374999 140367827080960 learning.py:507] global step 5820: loss = 1.5352 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:27.733511 140367827080960 learning.py:507] global step 5830: loss = 1.4289 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:30.102008 140367827080960 learning.py:507] global step 5840: loss = 1.4773 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:32.448642 140367827080960 learning.py:507] global step 5850: loss = 1.4000 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:34.820492 140367827080960 learning.py:507] global step 5860: loss = 1.5182 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:37.177763 140367827080960 learning.py:507] global step 5870: loss = 1.4435 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:39.545070 140367827080960 learning.py:507] global step 5880: loss = 1.5105 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:41.952162 140367827080960 learning.py:507] global step 5890: loss = 1.4125 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:44.405179 140367827080960 learning.py:507] global step 5900: loss = 1.5439 (0.234 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:29:46.767529 140367827080960 learning.py:507] global step 5910: loss = 1.4806 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:49.152883 140367827080960 learning.py:507] global step 5920: loss = 1.4197 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:51.522228 140367827080960 learning.py:507] global step 5930: loss = 1.4433 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:53.900577 140367827080960 learning.py:507] global step 5940: loss = 1.5719 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:56.289901 140367827080960 learning.py:507] global step 5950: loss = 1.4514 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:29:58.677150 140367827080960 learning.py:507] global step 5960: loss = 1.3780 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:01.071516 140367827080960 learning.py:507] global step 5970: loss = 1.2913 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:03.463198 140367827080960 learning.py:507] global step 5980: loss = 1.4089 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:05.847067 140367827080960 learning.py:507] global step 5990: loss = 1.5886 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:08.249669 140367827080960 learning.py:507] global step 6000: loss = 1.4783 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:10.610902 140367827080960 learning.py:507] global step 6010: loss = 1.4791 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:12.998116 140367827080960 learning.py:507] global step 6020: loss = 1.4049 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:15.368880 140367827080960 learning.py:507] global step 6030: loss = 1.3178 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:17.743659 140367827080960 learning.py:507] global step 6040: loss = 1.3397 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:20.086312 140367827080960 learning.py:507] global step 6050: loss = 1.5280 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:22.462201 140367827080960 learning.py:507] global step 6060: loss = 1.4236 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:24.829617 140367827080960 learning.py:507] global step 6070: loss = 1.4504 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:27.230401 140367827080960 learning.py:507] global step 6080: loss = 1.4224 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:29.608962 140367827080960 learning.py:507] global step 6090: loss = 1.3560 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:31.981899 140367827080960 learning.py:507] global step 6100: loss = 1.4758 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:34.360676 140367827080960 learning.py:507] global step 6110: loss = 1.4163 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:36.730171 140367827080960 learning.py:507] global step 6120: loss = 1.2701 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:39.105933 140367827080960 learning.py:507] global step 6130: loss = 1.5428 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:41.500456 140367827080960 learning.py:507] global step 6140: loss = 1.3475 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:43.887833 140367827080960 learning.py:507] global step 6150: loss = 1.3733 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:46.256114 140367827080960 learning.py:507] global step 6160: loss = 1.3757 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:48.627481 140367827080960 learning.py:507] global step 6170: loss = 1.4785 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:51.012459 140367827080960 learning.py:507] global step 6180: loss = 1.4543 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:53.385699 140367827080960 learning.py:507] global step 6190: loss = 1.3964 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:55.783387 140367827080960 learning.py:507] global step 6200: loss = 1.4701 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:30:58.177175 140367827080960 learning.py:507] global step 6210: loss = 1.4436 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:00.558480 140367827080960 learning.py:507] global step 6220: loss = 1.3948 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:02.938895 140367827080960 learning.py:507] global step 6230: loss = 1.3568 (0.223 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:05.297744 140367827080960 learning.py:507] global step 6240: loss = 1.3081 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:07.690491 140367827080960 learning.py:507] global step 6250: loss = 1.5072 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:08.573216 140361930364672 supervisor.py:1099] global_step/sec: 4.18972\u001b[0m\n",
      "\u001b[32mI0701 01:31:10.120376 140361921971968 supervisor.py:1050] Recording summary at step 6256.\u001b[0m\n",
      "\u001b[32mI0701 01:31:11.036642 140367827080960 learning.py:507] global step 6260: loss = 1.4220 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:13.442889 140367827080960 learning.py:507] global step 6270: loss = 1.4648 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:15.854218 140367827080960 learning.py:507] global step 6280: loss = 1.3760 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:18.224899 140367827080960 learning.py:507] global step 6290: loss = 1.4925 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:20.624934 140367827080960 learning.py:507] global step 6300: loss = 1.5573 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:23.032015 140367827080960 learning.py:507] global step 6310: loss = 1.3030 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:25.432230 140367827080960 learning.py:507] global step 6320: loss = 1.4389 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:27.795469 140367827080960 learning.py:507] global step 6330: loss = 1.5285 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:30.176549 140367827080960 learning.py:507] global step 6340: loss = 1.3377 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:32.562056 140367827080960 learning.py:507] global step 6350: loss = 1.4188 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:34.946513 140367827080960 learning.py:507] global step 6360: loss = 1.4654 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:37.311956 140367827080960 learning.py:507] global step 6370: loss = 1.4787 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:39.719963 140367827080960 learning.py:507] global step 6380: loss = 1.4164 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:42.120625 140367827080960 learning.py:507] global step 6390: loss = 1.5299 (0.249 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:44.530112 140367827080960 learning.py:507] global step 6400: loss = 1.3747 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:46.911174 140367827080960 learning.py:507] global step 6410: loss = 1.5266 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:49.310497 140367827080960 learning.py:507] global step 6420: loss = 1.4327 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:51.665081 140367827080960 learning.py:507] global step 6430: loss = 1.4319 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:54.050345 140367827080960 learning.py:507] global step 6440: loss = 1.4406 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:56.432919 140367827080960 learning.py:507] global step 6450: loss = 1.5059 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:31:58.800651 140367827080960 learning.py:507] global step 6460: loss = 1.3281 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:01.170156 140367827080960 learning.py:507] global step 6470: loss = 1.4443 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:03.559921 140367827080960 learning.py:507] global step 6480: loss = 1.3968 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:05.953941 140367827080960 learning.py:507] global step 6490: loss = 1.5915 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:08.305979 140367827080960 learning.py:507] global step 6500: loss = 1.3369 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:10.683432 140367827080960 learning.py:507] global step 6510: loss = 1.4889 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:13.067748 140367827080960 learning.py:507] global step 6520: loss = 1.4125 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:15.449126 140367827080960 learning.py:507] global step 6530: loss = 1.3388 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:17.818070 140367827080960 learning.py:507] global step 6540: loss = 1.3973 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:20.244601 140367827080960 learning.py:507] global step 6550: loss = 1.4805 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:22.618347 140367827080960 learning.py:507] global step 6560: loss = 1.4067 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:24.996659 140367827080960 learning.py:507] global step 6570: loss = 1.4386 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:27.370275 140367827080960 learning.py:507] global step 6580: loss = 1.4535 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:29.756812 140367827080960 learning.py:507] global step 6590: loss = 1.4656 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:32.099415 140367827080960 learning.py:507] global step 6600: loss = 1.5295 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:34.460195 140367827080960 learning.py:507] global step 6610: loss = 1.3262 (0.228 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:32:36.821429 140367827080960 learning.py:507] global step 6620: loss = 1.2669 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:39.192507 140367827080960 learning.py:507] global step 6630: loss = 1.5819 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:41.550673 140367827080960 learning.py:507] global step 6640: loss = 1.4809 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:43.961066 140367827080960 learning.py:507] global step 6650: loss = 1.2712 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:46.345699 140367827080960 learning.py:507] global step 6660: loss = 1.4964 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:48.739522 140367827080960 learning.py:507] global step 6670: loss = 1.4125 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:51.115489 140367827080960 learning.py:507] global step 6680: loss = 1.4029 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:53.464020 140367827080960 learning.py:507] global step 6690: loss = 1.4451 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:55.821803 140367827080960 learning.py:507] global step 6700: loss = 1.5693 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:32:58.208398 140367827080960 learning.py:507] global step 6710: loss = 1.3150 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:00.607466 140367827080960 learning.py:507] global step 6720: loss = 1.4117 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:02.986824 140367827080960 learning.py:507] global step 6730: loss = 1.4533 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:05.377324 140367827080960 learning.py:507] global step 6740: loss = 1.4860 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:07.754344 140367827080960 learning.py:507] global step 6750: loss = 1.3518 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:10.159411 140367827080960 learning.py:507] global step 6760: loss = 1.4375 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:12.537710 140367827080960 learning.py:507] global step 6770: loss = 1.4370 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:14.942351 140367827080960 learning.py:507] global step 6780: loss = 1.4231 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:17.372956 140367827080960 learning.py:507] global step 6790: loss = 1.3901 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:19.791990 140367827080960 learning.py:507] global step 6800: loss = 1.4423 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:22.171889 140367827080960 learning.py:507] global step 6810: loss = 1.4000 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:24.547032 140367827080960 learning.py:507] global step 6820: loss = 1.5603 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:26.938206 140367827080960 learning.py:507] global step 6830: loss = 1.4208 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:29.322864 140367827080960 learning.py:507] global step 6840: loss = 1.5246 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:31.724035 140367827080960 learning.py:507] global step 6850: loss = 1.3509 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:34.102190 140367827080960 learning.py:507] global step 6860: loss = 1.4732 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:36.491334 140367827080960 learning.py:507] global step 6870: loss = 1.3313 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:38.896441 140367827080960 learning.py:507] global step 6880: loss = 1.4027 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:41.301423 140367827080960 learning.py:507] global step 6890: loss = 1.4099 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:43.680732 140367827080960 learning.py:507] global step 6900: loss = 1.3926 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:46.090470 140367827080960 learning.py:507] global step 6910: loss = 1.4064 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:48.480571 140367827080960 learning.py:507] global step 6920: loss = 1.5409 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:50.867640 140367827080960 learning.py:507] global step 6930: loss = 1.4585 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:53.249136 140367827080960 learning.py:507] global step 6940: loss = 1.5364 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:55.611634 140367827080960 learning.py:507] global step 6950: loss = 1.3835 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:33:57.968571 140367827080960 learning.py:507] global step 6960: loss = 1.3538 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:00.371627 140367827080960 learning.py:507] global step 6970: loss = 1.5794 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:02.755656 140367827080960 learning.py:507] global step 6980: loss = 1.4196 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:05.150587 140367827080960 learning.py:507] global step 6990: loss = 1.4904 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:07.541495 140367827080960 learning.py:507] global step 7000: loss = 1.5006 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:09.920319 140367827080960 learning.py:507] global step 7010: loss = 1.4091 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:12.299082 140367827080960 learning.py:507] global step 7020: loss = 1.4124 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:14.641947 140367827080960 learning.py:507] global step 7030: loss = 1.5517 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:17.023365 140367827080960 learning.py:507] global step 7040: loss = 1.4012 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:19.392726 140367827080960 learning.py:507] global step 7050: loss = 1.4654 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:21.768549 140367827080960 learning.py:507] global step 7060: loss = 1.4132 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:24.126451 140367827080960 learning.py:507] global step 7070: loss = 1.5419 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:26.517400 140367827080960 learning.py:507] global step 7080: loss = 1.4576 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:28.903887 140367827080960 learning.py:507] global step 7090: loss = 1.4954 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:31.286818 140367827080960 learning.py:507] global step 7100: loss = 1.3607 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:33.657176 140367827080960 learning.py:507] global step 7110: loss = 1.3859 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:36.030499 140367827080960 learning.py:507] global step 7120: loss = 1.3880 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:38.403381 140367827080960 learning.py:507] global step 7130: loss = 1.4157 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:40.814702 140367827080960 learning.py:507] global step 7140: loss = 1.4859 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:43.214839 140367827080960 learning.py:507] global step 7150: loss = 1.4101 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:45.584978 140367827080960 learning.py:507] global step 7160: loss = 1.4237 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:47.944670 140367827080960 learning.py:507] global step 7170: loss = 1.4458 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:50.363537 140367827080960 learning.py:507] global step 7180: loss = 1.4265 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:52.741949 140367827080960 learning.py:507] global step 7190: loss = 1.4443 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:55.106667 140367827080960 learning.py:507] global step 7200: loss = 1.4015 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:57.486742 140367827080960 learning.py:507] global step 7210: loss = 1.5172 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:34:59.876312 140367827080960 learning.py:507] global step 7220: loss = 1.4175 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:02.264215 140367827080960 learning.py:507] global step 7230: loss = 1.5397 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:04.634994 140367827080960 learning.py:507] global step 7240: loss = 1.3930 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:07.032463 140367827080960 learning.py:507] global step 7250: loss = 1.5039 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:09.430003 140367827080960 learning.py:507] global step 7260: loss = 1.4541 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:11.794099 140367827080960 learning.py:507] global step 7270: loss = 1.3746 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:14.175107 140367827080960 learning.py:507] global step 7280: loss = 1.4988 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:16.552405 140367827080960 learning.py:507] global step 7290: loss = 1.2619 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:18.955595 140367827080960 learning.py:507] global step 7300: loss = 1.4973 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:21.345828 140367827080960 learning.py:507] global step 7310: loss = 1.4953 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:23.721988 140367827080960 learning.py:507] global step 7320: loss = 1.4681 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:26.136526 140367827080960 learning.py:507] global step 7330: loss = 1.3530 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:28.531301 140367827080960 learning.py:507] global step 7340: loss = 1.4335 (0.232 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:35:30.918241 140367827080960 learning.py:507] global step 7350: loss = 1.5228 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:33.309840 140367827080960 learning.py:507] global step 7360: loss = 1.4133 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:35.684829 140367827080960 learning.py:507] global step 7370: loss = 1.4668 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:38.064368 140367827080960 learning.py:507] global step 7380: loss = 1.4208 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:40.432279 140367827080960 learning.py:507] global step 7390: loss = 1.5168 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:42.815766 140367827080960 learning.py:507] global step 7400: loss = 1.4433 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:45.221494 140367827080960 learning.py:507] global step 7410: loss = 1.4232 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:47.598648 140367827080960 learning.py:507] global step 7420: loss = 1.3765 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:50.001419 140367827080960 learning.py:507] global step 7430: loss = 1.4831 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:52.398797 140367827080960 learning.py:507] global step 7440: loss = 1.4630 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:54.792267 140367827080960 learning.py:507] global step 7450: loss = 1.5824 (0.249 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:57.174910 140367827080960 learning.py:507] global step 7460: loss = 1.4347 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:35:59.563004 140367827080960 learning.py:507] global step 7470: loss = 1.5851 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:01.956944 140367827080960 learning.py:507] global step 7480: loss = 1.4536 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:04.323498 140367827080960 learning.py:507] global step 7490: loss = 1.3748 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:06.708120 140367827080960 learning.py:507] global step 7500: loss = 1.4844 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:08.552082 140361938757376 supervisor.py:1117] Saving checkpoint to path /opt/ml/model/model.ckpt\u001b[0m\n",
      "\u001b[32mI0701 01:36:08.593712 140361930364672 supervisor.py:1099] global_step/sec: 4.17971\u001b[0m\n",
      "\u001b[32mI0701 01:36:10.148211 140361921971968 supervisor.py:1050] Recording summary at step 7509.\u001b[0m\n",
      "\u001b[32mI0701 01:36:10.173907 140367827080960 learning.py:507] global step 7510: loss = 1.4145 (0.299 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:12.571238 140367827080960 learning.py:507] global step 7520: loss = 1.3931 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:14.962162 140367827080960 learning.py:507] global step 7530: loss = 1.4445 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:17.357945 140367827080960 learning.py:507] global step 7540: loss = 1.5119 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:19.717967 140367827080960 learning.py:507] global step 7550: loss = 1.3380 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:22.105854 140367827080960 learning.py:507] global step 7560: loss = 1.3563 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:24.490238 140367827080960 learning.py:507] global step 7570: loss = 1.4782 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:26.864444 140367827080960 learning.py:507] global step 7580: loss = 1.3897 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:29.246980 140367827080960 learning.py:507] global step 7590: loss = 1.4007 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:31.628664 140367827080960 learning.py:507] global step 7600: loss = 1.3834 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:34.020117 140367827080960 learning.py:507] global step 7610: loss = 1.4872 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:36.389317 140367827080960 learning.py:507] global step 7620: loss = 1.4176 (0.249 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:38.771600 140367827080960 learning.py:507] global step 7630: loss = 1.4299 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:41.114732 140367827080960 learning.py:507] global step 7640: loss = 1.3715 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:43.484090 140367827080960 learning.py:507] global step 7650: loss = 1.5504 (0.223 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:45.864588 140367827080960 learning.py:507] global step 7660: loss = 1.3720 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:48.292419 140367827080960 learning.py:507] global step 7670: loss = 1.4034 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:50.653139 140367827080960 learning.py:507] global step 7680: loss = 1.5081 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:53.019783 140367827080960 learning.py:507] global step 7690: loss = 1.4135 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:55.389843 140367827080960 learning.py:507] global step 7700: loss = 1.3477 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:36:57.758737 140367827080960 learning.py:507] global step 7710: loss = 1.4894 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:00.133919 140367827080960 learning.py:507] global step 7720: loss = 1.4866 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:02.516330 140367827080960 learning.py:507] global step 7730: loss = 1.3588 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:04.897394 140367827080960 learning.py:507] global step 7740: loss = 1.3574 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:07.289104 140367827080960 learning.py:507] global step 7750: loss = 1.4599 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:09.670166 140367827080960 learning.py:507] global step 7760: loss = 1.3167 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:12.058669 140367827080960 learning.py:507] global step 7770: loss = 1.3931 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:14.400717 140367827080960 learning.py:507] global step 7780: loss = 1.4450 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:16.805412 140367827080960 learning.py:507] global step 7790: loss = 1.3700 (0.249 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:19.196351 140367827080960 learning.py:507] global step 7800: loss = 1.4632 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:21.593574 140367827080960 learning.py:507] global step 7810: loss = 1.3571 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:23.962492 140367827080960 learning.py:507] global step 7820: loss = 1.5282 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:26.356260 140367827080960 learning.py:507] global step 7830: loss = 1.2547 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:28.736649 140367827080960 learning.py:507] global step 7840: loss = 1.4413 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:31.128185 140367827080960 learning.py:507] global step 7850: loss = 1.4485 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:33.513488 140367827080960 learning.py:507] global step 7860: loss = 1.3405 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:35.861829 140367827080960 learning.py:507] global step 7870: loss = 1.5111 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:38.266596 140367827080960 learning.py:507] global step 7880: loss = 1.3615 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:40.637238 140367827080960 learning.py:507] global step 7890: loss = 1.5789 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:43.051762 140367827080960 learning.py:507] global step 7900: loss = 1.3866 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:45.434499 140367827080960 learning.py:507] global step 7910: loss = 1.5161 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:47.837774 140367827080960 learning.py:507] global step 7920: loss = 1.3271 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:50.247911 140367827080960 learning.py:507] global step 7930: loss = 1.4891 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:52.636683 140367827080960 learning.py:507] global step 7940: loss = 1.4727 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:55.017528 140367827080960 learning.py:507] global step 7950: loss = 1.5365 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:57.398894 140367827080960 learning.py:507] global step 7960: loss = 1.4206 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:37:59.790673 140367827080960 learning.py:507] global step 7970: loss = 1.3717 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:02.181798 140367827080960 learning.py:507] global step 7980: loss = 1.4375 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:04.529342 140367827080960 learning.py:507] global step 7990: loss = 1.3412 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:06.920171 140367827080960 learning.py:507] global step 8000: loss = 1.5132 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:09.294673 140367827080960 learning.py:507] global step 8010: loss = 1.3755 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:11.657147 140367827080960 learning.py:507] global step 8020: loss = 1.3692 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:14.024152 140367827080960 learning.py:507] global step 8030: loss = 1.4599 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:16.397428 140367827080960 learning.py:507] global step 8040: loss = 1.3821 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:18.789691 140367827080960 learning.py:507] global step 8050: loss = 1.3234 (0.244 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:38:21.188458 140367827080960 learning.py:507] global step 8060: loss = 1.4552 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:23.596170 140367827080960 learning.py:507] global step 8070: loss = 1.3572 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:25.980482 140367827080960 learning.py:507] global step 8080: loss = 1.4349 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:28.392743 140367827080960 learning.py:507] global step 8090: loss = 1.4341 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:30.783756 140367827080960 learning.py:507] global step 8100: loss = 1.4775 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:33.170727 140367827080960 learning.py:507] global step 8110: loss = 1.4118 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:35.533927 140367827080960 learning.py:507] global step 8120: loss = 1.4745 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:37.903333 140367827080960 learning.py:507] global step 8130: loss = 1.5523 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:40.326488 140367827080960 learning.py:507] global step 8140: loss = 1.4317 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:42.697102 140367827080960 learning.py:507] global step 8150: loss = 1.4565 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:45.107784 140367827080960 learning.py:507] global step 8160: loss = 1.4426 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:47.477751 140367827080960 learning.py:507] global step 8170: loss = 1.4169 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:49.828752 140367827080960 learning.py:507] global step 8180: loss = 1.4230 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:52.191438 140367827080960 learning.py:507] global step 8190: loss = 1.4755 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:54.544045 140367827080960 learning.py:507] global step 8200: loss = 1.3539 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:56.936971 140367827080960 learning.py:507] global step 8210: loss = 1.3029 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:38:59.304182 140367827080960 learning.py:507] global step 8220: loss = 1.3976 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:01.674309 140367827080960 learning.py:507] global step 8230: loss = 1.5803 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:04.048491 140367827080960 learning.py:507] global step 8240: loss = 1.3582 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:06.420669 140367827080960 learning.py:507] global step 8250: loss = 1.5159 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:08.814809 140367827080960 learning.py:507] global step 8260: loss = 1.5555 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:11.182193 140367827080960 learning.py:507] global step 8270: loss = 1.2108 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:13.556754 140367827080960 learning.py:507] global step 8280: loss = 1.4334 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:15.918670 140367827080960 learning.py:507] global step 8290: loss = 1.3447 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:18.283335 140367827080960 learning.py:507] global step 8300: loss = 1.5235 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:20.664118 140367827080960 learning.py:507] global step 8310: loss = 1.5062 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:23.053442 140367827080960 learning.py:507] global step 8320: loss = 1.4166 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:25.451318 140367827080960 learning.py:507] global step 8330: loss = 1.3411 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:27.819336 140367827080960 learning.py:507] global step 8340: loss = 1.3785 (0.250 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:30.206012 140367827080960 learning.py:507] global step 8350: loss = 1.3733 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:32.600205 140367827080960 learning.py:507] global step 8360: loss = 1.3857 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:34.980139 140367827080960 learning.py:507] global step 8370: loss = 1.4245 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:37.347872 140367827080960 learning.py:507] global step 8380: loss = 1.2863 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:39.747656 140367827080960 learning.py:507] global step 8390: loss = 1.4289 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:42.144237 140367827080960 learning.py:507] global step 8400: loss = 1.4174 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:44.557007 140367827080960 learning.py:507] global step 8410: loss = 1.4595 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:46.938602 140367827080960 learning.py:507] global step 8420: loss = 1.3389 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:49.326472 140367827080960 learning.py:507] global step 8430: loss = 1.3737 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:51.736444 140367827080960 learning.py:507] global step 8440: loss = 1.4567 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:54.141669 140367827080960 learning.py:507] global step 8450: loss = 1.3350 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:56.536492 140367827080960 learning.py:507] global step 8460: loss = 1.4645 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:39:58.915014 140367827080960 learning.py:507] global step 8470: loss = 1.5171 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:01.292692 140367827080960 learning.py:507] global step 8480: loss = 1.4597 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:03.661242 140367827080960 learning.py:507] global step 8490: loss = 1.4925 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:06.027301 140367827080960 learning.py:507] global step 8500: loss = 1.3231 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:08.408565 140367827080960 learning.py:507] global step 8510: loss = 1.5171 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:10.769634 140367827080960 learning.py:507] global step 8520: loss = 1.2650 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:13.149207 140367827080960 learning.py:507] global step 8530: loss = 1.3742 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:15.539236 140367827080960 learning.py:507] global step 8540: loss = 1.3573 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:17.914218 140367827080960 learning.py:507] global step 8550: loss = 1.3825 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:20.284043 140367827080960 learning.py:507] global step 8560: loss = 1.4320 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:22.679302 140367827080960 learning.py:507] global step 8570: loss = 1.2965 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:25.067397 140367827080960 learning.py:507] global step 8580: loss = 1.3112 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:27.465440 140367827080960 learning.py:507] global step 8590: loss = 1.3922 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:29.854882 140367827080960 learning.py:507] global step 8600: loss = 1.3703 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:32.214914 140367827080960 learning.py:507] global step 8610: loss = 1.3729 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:34.592932 140367827080960 learning.py:507] global step 8620: loss = 1.3030 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:36.962073 140367827080960 learning.py:507] global step 8630: loss = 1.3054 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:39.345257 140367827080960 learning.py:507] global step 8640: loss = 1.3652 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:41.762382 140367827080960 learning.py:507] global step 8650: loss = 1.3831 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:44.132437 140367827080960 learning.py:507] global step 8660: loss = 1.4092 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:46.503149 140367827080960 learning.py:507] global step 8670: loss = 1.3858 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:48.877512 140367827080960 learning.py:507] global step 8680: loss = 1.3991 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:51.256051 140367827080960 learning.py:507] global step 8690: loss = 1.4725 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:53.632924 140367827080960 learning.py:507] global step 8700: loss = 1.3582 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:56.014522 140367827080960 learning.py:507] global step 8710: loss = 1.3881 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:40:58.378479 140367827080960 learning.py:507] global step 8720: loss = 1.3482 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:00.750216 140367827080960 learning.py:507] global step 8730: loss = 1.3373 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:03.143544 140367827080960 learning.py:507] global step 8740: loss = 1.2830 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:05.512727 140367827080960 learning.py:507] global step 8750: loss = 1.3510 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:07.893907 140367827080960 learning.py:507] global step 8760: loss = 1.4295 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:08.594163 140361930364672 supervisor.py:1099] global_step/sec: 4.18333\u001b[0m\n",
      "\u001b[32mI0701 01:41:10.352765 140361921971968 supervisor.py:1050] Recording summary at step 8767.\u001b[0m\n",
      "\u001b[32mI0701 01:41:11.032298 140367827080960 learning.py:507] global step 8770: loss = 1.6033 (0.241 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:41:13.420392 140367827080960 learning.py:507] global step 8780: loss = 1.4348 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:15.809756 140367827080960 learning.py:507] global step 8790: loss = 1.4649 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:18.197788 140367827080960 learning.py:507] global step 8800: loss = 1.4361 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:20.558670 140367827080960 learning.py:507] global step 8810: loss = 1.4707 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:22.926728 140367827080960 learning.py:507] global step 8820: loss = 1.5226 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:25.283198 140367827080960 learning.py:507] global step 8830: loss = 1.4723 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:27.653179 140367827080960 learning.py:507] global step 8840: loss = 1.4578 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:29.998842 140367827080960 learning.py:507] global step 8850: loss = 1.4112 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:32.403798 140367827080960 learning.py:507] global step 8860: loss = 1.4948 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:34.776176 140367827080960 learning.py:507] global step 8870: loss = 1.4108 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:37.158314 140367827080960 learning.py:507] global step 8880: loss = 1.5341 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:39.553966 140367827080960 learning.py:507] global step 8890: loss = 1.3656 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:41.926498 140367827080960 learning.py:507] global step 8900: loss = 1.3716 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:44.310271 140367827080960 learning.py:507] global step 8910: loss = 1.4512 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:46.701994 140367827080960 learning.py:507] global step 8920: loss = 1.5052 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:49.080559 140367827080960 learning.py:507] global step 8930: loss = 1.3976 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:51.476337 140367827080960 learning.py:507] global step 8940: loss = 1.3895 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:53.875866 140367827080960 learning.py:507] global step 8950: loss = 1.4218 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:56.249355 140367827080960 learning.py:507] global step 8960: loss = 1.4507 (0.249 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:41:58.604163 140367827080960 learning.py:507] global step 8970: loss = 1.4900 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:00.998089 140367827080960 learning.py:507] global step 8980: loss = 1.3864 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:03.385834 140367827080960 learning.py:507] global step 8990: loss = 1.4136 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:05.804251 140367827080960 learning.py:507] global step 9000: loss = 1.3037 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:08.176827 140367827080960 learning.py:507] global step 9010: loss = 1.4608 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:10.561539 140367827080960 learning.py:507] global step 9020: loss = 1.5262 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:12.944087 140367827080960 learning.py:507] global step 9030: loss = 1.4264 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:15.326818 140367827080960 learning.py:507] global step 9040: loss = 1.4895 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:17.707082 140367827080960 learning.py:507] global step 9050: loss = 1.4339 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:20.090944 140367827080960 learning.py:507] global step 9060: loss = 1.3772 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:22.474564 140367827080960 learning.py:507] global step 9070: loss = 1.3906 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:24.886771 140367827080960 learning.py:507] global step 9080: loss = 1.4208 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:27.322902 140367827080960 learning.py:507] global step 9090: loss = 1.4490 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:29.713217 140367827080960 learning.py:507] global step 9100: loss = 1.3549 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:32.079101 140367827080960 learning.py:507] global step 9110: loss = 1.3619 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:34.476176 140367827080960 learning.py:507] global step 9120: loss = 1.3796 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:36.853580 140367827080960 learning.py:507] global step 9130: loss = 1.3945 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:39.208930 140367827080960 learning.py:507] global step 9140: loss = 1.3722 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:41.589586 140367827080960 learning.py:507] global step 9150: loss = 1.3564 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:43.964865 140367827080960 learning.py:507] global step 9160: loss = 1.3837 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:46.358017 140367827080960 learning.py:507] global step 9170: loss = 1.4430 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:48.756076 140367827080960 learning.py:507] global step 9180: loss = 1.4091 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:51.125227 140367827080960 learning.py:507] global step 9190: loss = 1.5434 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:53.508052 140367827080960 learning.py:507] global step 9200: loss = 1.4773 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:55.906136 140367827080960 learning.py:507] global step 9210: loss = 1.3409 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:42:58.259815 140367827080960 learning.py:507] global step 9220: loss = 1.5490 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:00.637110 140367827080960 learning.py:507] global step 9230: loss = 1.4284 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:03.008302 140367827080960 learning.py:507] global step 9240: loss = 1.3686 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:05.356168 140367827080960 learning.py:507] global step 9250: loss = 1.5829 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:07.729978 140367827080960 learning.py:507] global step 9260: loss = 1.3196 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:10.213865 140367827080960 learning.py:507] global step 9270: loss = 1.3395 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:12.585803 140367827080960 learning.py:507] global step 9280: loss = 1.4328 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:14.947562 140367827080960 learning.py:507] global step 9290: loss = 1.5268 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:17.296845 140367827080960 learning.py:507] global step 9300: loss = 1.5034 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:19.653497 140367827080960 learning.py:507] global step 9310: loss = 1.4004 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:22.028006 140367827080960 learning.py:507] global step 9320: loss = 1.4683 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:24.406208 140367827080960 learning.py:507] global step 9330: loss = 1.4355 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:26.791960 140367827080960 learning.py:507] global step 9340: loss = 1.3138 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:29.191395 140367827080960 learning.py:507] global step 9350: loss = 1.4716 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:31.561631 140367827080960 learning.py:507] global step 9360: loss = 1.5300 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:33.946476 140367827080960 learning.py:507] global step 9370: loss = 1.3446 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:36.331101 140367827080960 learning.py:507] global step 9380: loss = 1.5220 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:38.706327 140367827080960 learning.py:507] global step 9390: loss = 1.3547 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:41.094933 140367827080960 learning.py:507] global step 9400: loss = 1.5336 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:43.497488 140367827080960 learning.py:507] global step 9410: loss = 1.5536 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:45.876017 140367827080960 learning.py:507] global step 9420: loss = 1.3985 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:48.258752 140367827080960 learning.py:507] global step 9430: loss = 1.4789 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:50.637531 140367827080960 learning.py:507] global step 9440: loss = 1.3732 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:53.020431 140367827080960 learning.py:507] global step 9450: loss = 1.4160 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:55.369831 140367827080960 learning.py:507] global step 9460: loss = 1.3753 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:43:57.751451 140367827080960 learning.py:507] global step 9470: loss = 1.5131 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:00.148111 140367827080960 learning.py:507] global step 9480: loss = 1.4496 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:02.532459 140367827080960 learning.py:507] global step 9490: loss = 1.3392 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:04.936877 140367827080960 learning.py:507] global step 9500: loss = 1.3078 (0.234 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:44:07.300396 140367827080960 learning.py:507] global step 9510: loss = 1.3289 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:09.669750 140367827080960 learning.py:507] global step 9520: loss = 1.4162 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:12.050621 140367827080960 learning.py:507] global step 9530: loss = 1.3964 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:14.423583 140367827080960 learning.py:507] global step 9540: loss = 1.3820 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:16.796386 140367827080960 learning.py:507] global step 9550: loss = 1.3928 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:19.203363 140367827080960 learning.py:507] global step 9560: loss = 1.3207 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:21.575881 140367827080960 learning.py:507] global step 9570: loss = 1.3874 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:23.949872 140367827080960 learning.py:507] global step 9580: loss = 1.4068 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:26.352528 140367827080960 learning.py:507] global step 9590: loss = 1.3714 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:28.760288 140367827080960 learning.py:507] global step 9600: loss = 1.4208 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:31.147789 140367827080960 learning.py:507] global step 9610: loss = 1.3869 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:33.507770 140367827080960 learning.py:507] global step 9620: loss = 1.3266 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:35.910356 140367827080960 learning.py:507] global step 9630: loss = 1.3349 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:38.279081 140367827080960 learning.py:507] global step 9640: loss = 1.4868 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:40.667057 140367827080960 learning.py:507] global step 9650: loss = 1.4371 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:43.047436 140367827080960 learning.py:507] global step 9660: loss = 1.3933 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:45.443996 140367827080960 learning.py:507] global step 9670: loss = 1.2893 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:47.812438 140367827080960 learning.py:507] global step 9680: loss = 1.3900 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:50.205238 140367827080960 learning.py:507] global step 9690: loss = 1.4365 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:52.621975 140367827080960 learning.py:507] global step 9700: loss = 1.4599 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:54.998768 140367827080960 learning.py:507] global step 9710: loss = 1.4617 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:57.364238 140367827080960 learning.py:507] global step 9720: loss = 1.4240 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:44:59.736407 140367827080960 learning.py:507] global step 9730: loss = 1.3150 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:02.131906 140367827080960 learning.py:507] global step 9740: loss = 1.4079 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:04.508349 140367827080960 learning.py:507] global step 9750: loss = 1.3411 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:06.841869 140367827080960 learning.py:507] global step 9760: loss = 1.3164 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:09.250982 140367827080960 learning.py:507] global step 9770: loss = 1.4916 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:11.630692 140367827080960 learning.py:507] global step 9780: loss = 1.3830 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:14.032690 140367827080960 learning.py:507] global step 9790: loss = 1.4572 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:16.403341 140367827080960 learning.py:507] global step 9800: loss = 1.3686 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:18.780488 140367827080960 learning.py:507] global step 9810: loss = 1.4122 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:21.137587 140367827080960 learning.py:507] global step 9820: loss = 1.4159 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:23.529112 140367827080960 learning.py:507] global step 9830: loss = 1.3281 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:25.926285 140367827080960 learning.py:507] global step 9840: loss = 1.3887 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:28.312748 140367827080960 learning.py:507] global step 9850: loss = 1.5356 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:30.693963 140367827080960 learning.py:507] global step 9860: loss = 1.3684 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:33.111885 140367827080960 learning.py:507] global step 9870: loss = 1.4847 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:35.493371 140367827080960 learning.py:507] global step 9880: loss = 1.3672 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:37.858756 140367827080960 learning.py:507] global step 9890: loss = 1.4962 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:40.230624 140367827080960 learning.py:507] global step 9900: loss = 1.3826 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:42.589847 140367827080960 learning.py:507] global step 9910: loss = 1.3316 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:44.990525 140367827080960 learning.py:507] global step 9920: loss = 1.4928 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:47.362222 140367827080960 learning.py:507] global step 9930: loss = 1.3686 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:49.703064 140367827080960 learning.py:507] global step 9940: loss = 1.3538 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:52.059606 140367827080960 learning.py:507] global step 9950: loss = 1.3587 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:54.423202 140367827080960 learning.py:507] global step 9960: loss = 1.4169 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:56.800636 140367827080960 learning.py:507] global step 9970: loss = 1.4116 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:45:59.174818 140367827080960 learning.py:507] global step 9980: loss = 1.4775 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:01.548086 140367827080960 learning.py:507] global step 9990: loss = 1.3444 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:03.959814 140367827080960 learning.py:507] global step 10000: loss = 1.4253 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:06.338457 140367827080960 learning.py:507] global step 10010: loss = 1.4021 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:08.552160 140361938757376 supervisor.py:1117] Saving checkpoint to path /opt/ml/model/model.ckpt\u001b[0m\n",
      "\u001b[32mI0701 01:46:08.560708 140361930364672 supervisor.py:1099] global_step/sec: 4.19047\u001b[0m\n",
      "\u001b[32mI0701 01:46:09.396226 140367827080960 learning.py:507] global step 10020: loss = 1.4660 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:10.406738 140361921971968 supervisor.py:1050] Recording summary at step 10022.\u001b[0m\n",
      "\u001b[32mI0701 01:46:12.179609 140367827080960 learning.py:507] global step 10030: loss = 1.4063 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:14.535573 140367827080960 learning.py:507] global step 10040: loss = 1.4198 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:16.899866 140367827080960 learning.py:507] global step 10050: loss = 1.5359 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:19.299081 140367827080960 learning.py:507] global step 10060: loss = 1.2493 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:21.663589 140367827080960 learning.py:507] global step 10070: loss = 1.4064 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:24.017668 140367827080960 learning.py:507] global step 10080: loss = 1.4264 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:26.422780 140367827080960 learning.py:507] global step 10090: loss = 1.3853 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:28.805830 140367827080960 learning.py:507] global step 10100: loss = 1.4318 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:31.148899 140367827080960 learning.py:507] global step 10110: loss = 1.3722 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:33.519745 140367827080960 learning.py:507] global step 10120: loss = 1.3874 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:35.885816 140367827080960 learning.py:507] global step 10130: loss = 1.3971 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:38.252819 140367827080960 learning.py:507] global step 10140: loss = 1.4715 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:40.622118 140367827080960 learning.py:507] global step 10150: loss = 1.3731 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:42.991482 140367827080960 learning.py:507] global step 10160: loss = 1.4922 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:45.373261 140367827080960 learning.py:507] global step 10170: loss = 1.4302 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:47.747656 140367827080960 learning.py:507] global step 10180: loss = 1.5570 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:50.133495 140367827080960 learning.py:507] global step 10190: loss = 1.2848 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:52.501151 140367827080960 learning.py:507] global step 10200: loss = 1.3340 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:54.881886 140367827080960 learning.py:507] global step 10210: loss = 1.3876 (0.235 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:46:57.278892 140367827080960 learning.py:507] global step 10220: loss = 1.4955 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:46:59.672465 140367827080960 learning.py:507] global step 10230: loss = 1.2319 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:02.024688 140367827080960 learning.py:507] global step 10240: loss = 1.4003 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:04.387957 140367827080960 learning.py:507] global step 10250: loss = 1.3510 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:06.768074 140367827080960 learning.py:507] global step 10260: loss = 1.3790 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:09.133878 140367827080960 learning.py:507] global step 10270: loss = 1.5718 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:11.464607 140367827080960 learning.py:507] global step 10280: loss = 1.3938 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:13.831808 140367827080960 learning.py:507] global step 10290: loss = 1.3619 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:16.201750 140367827080960 learning.py:507] global step 10300: loss = 1.3862 (0.255 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:18.551636 140367827080960 learning.py:507] global step 10310: loss = 1.3586 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:20.934412 140367827080960 learning.py:507] global step 10320: loss = 1.3355 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:23.345036 140367827080960 learning.py:507] global step 10330: loss = 1.4212 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:25.701875 140367827080960 learning.py:507] global step 10340: loss = 1.3208 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:28.071908 140367827080960 learning.py:507] global step 10350: loss = 1.3754 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:30.456094 140367827080960 learning.py:507] global step 10360: loss = 1.3963 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:32.832117 140367827080960 learning.py:507] global step 10370: loss = 1.4477 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:35.236746 140367827080960 learning.py:507] global step 10380: loss = 1.5038 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:37.611948 140367827080960 learning.py:507] global step 10390: loss = 1.2747 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:40.006227 140367827080960 learning.py:507] global step 10400: loss = 1.3726 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:42.355046 140367827080960 learning.py:507] global step 10410: loss = 1.3182 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:44.712383 140367827080960 learning.py:507] global step 10420: loss = 1.2665 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:47.047009 140367827080960 learning.py:507] global step 10430: loss = 1.3983 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:49.398718 140367827080960 learning.py:507] global step 10440: loss = 1.3449 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:51.763309 140367827080960 learning.py:507] global step 10450: loss = 1.4511 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:54.118355 140367827080960 learning.py:507] global step 10460: loss = 1.4427 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:56.491539 140367827080960 learning.py:507] global step 10470: loss = 1.5115 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:47:58.856507 140367827080960 learning.py:507] global step 10480: loss = 1.4880 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:01.208086 140367827080960 learning.py:507] global step 10490: loss = 1.2931 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:03.568202 140367827080960 learning.py:507] global step 10500: loss = 1.4834 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:05.935225 140367827080960 learning.py:507] global step 10510: loss = 1.5485 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:08.314881 140367827080960 learning.py:507] global step 10520: loss = 1.4383 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:10.693649 140367827080960 learning.py:507] global step 10530: loss = 1.4543 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:13.059614 140367827080960 learning.py:507] global step 10540: loss = 1.3815 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:15.458668 140367827080960 learning.py:507] global step 10550: loss = 1.4991 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:17.866373 140367827080960 learning.py:507] global step 10560: loss = 1.3662 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:20.191900 140367827080960 learning.py:507] global step 10570: loss = 1.4319 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:22.576174 140367827080960 learning.py:507] global step 10580: loss = 1.4213 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:24.944782 140367827080960 learning.py:507] global step 10590: loss = 1.3535 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:27.310420 140367827080960 learning.py:507] global step 10600: loss = 1.3807 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:29.697989 140367827080960 learning.py:507] global step 10610: loss = 1.4420 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:32.045980 140367827080960 learning.py:507] global step 10620: loss = 1.4552 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:34.408658 140367827080960 learning.py:507] global step 10630: loss = 1.4053 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:36.812441 140367827080960 learning.py:507] global step 10640: loss = 1.4139 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:39.170402 140367827080960 learning.py:507] global step 10650: loss = 1.4316 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:41.530963 140367827080960 learning.py:507] global step 10660: loss = 1.3988 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:43.920399 140367827080960 learning.py:507] global step 10670: loss = 1.4272 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:46.282313 140367827080960 learning.py:507] global step 10680: loss = 1.5053 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:48.667524 140367827080960 learning.py:507] global step 10690: loss = 1.3406 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:51.051712 140367827080960 learning.py:507] global step 10700: loss = 1.5147 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:53.400311 140367827080960 learning.py:507] global step 10710: loss = 1.5051 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:55.766433 140367827080960 learning.py:507] global step 10720: loss = 1.4959 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:48:58.133315 140367827080960 learning.py:507] global step 10730: loss = 1.3882 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:00.497395 140367827080960 learning.py:507] global step 10740: loss = 1.5203 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:02.881597 140367827080960 learning.py:507] global step 10750: loss = 1.4221 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:05.261221 140367827080960 learning.py:507] global step 10760: loss = 1.4806 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:07.614887 140367827080960 learning.py:507] global step 10770: loss = 1.2893 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:09.992882 140367827080960 learning.py:507] global step 10780: loss = 1.5022 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:12.365236 140367827080960 learning.py:507] global step 10790: loss = 1.3472 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:14.732498 140367827080960 learning.py:507] global step 10800: loss = 1.5414 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:17.071595 140367827080960 learning.py:507] global step 10810: loss = 1.4188 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:19.479801 140367827080960 learning.py:507] global step 10820: loss = 1.4100 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:21.876017 140367827080960 learning.py:507] global step 10830: loss = 1.3431 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:24.275480 140367827080960 learning.py:507] global step 10840: loss = 1.3139 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:26.666035 140367827080960 learning.py:507] global step 10850: loss = 1.3012 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:29.045146 140367827080960 learning.py:507] global step 10860: loss = 1.3097 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:31.397667 140367827080960 learning.py:507] global step 10870: loss = 1.3797 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:33.773328 140367827080960 learning.py:507] global step 10880: loss = 1.4456 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:36.117899 140367827080960 learning.py:507] global step 10890: loss = 1.2714 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:38.508117 140367827080960 learning.py:507] global step 10900: loss = 1.3838 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:40.906215 140367827080960 learning.py:507] global step 10910: loss = 1.3781 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:43.272658 140367827080960 learning.py:507] global step 10920: loss = 1.4410 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:45.637878 140367827080960 learning.py:507] global step 10930: loss = 1.4234 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:48.028076 140367827080960 learning.py:507] global step 10940: loss = 1.3755 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:50.379271 140367827080960 learning.py:507] global step 10950: loss = 1.5031 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:52.748599 140367827080960 learning.py:507] global step 10960: loss = 1.3803 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:55.109483 140367827080960 learning.py:507] global step 10970: loss = 1.4291 (0.241 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:49:57.493206 140367827080960 learning.py:507] global step 10980: loss = 1.4666 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:49:59.850282 140367827080960 learning.py:507] global step 10990: loss = 1.3612 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:02.217854 140367827080960 learning.py:507] global step 11000: loss = 1.4584 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:04.569880 140367827080960 learning.py:507] global step 11010: loss = 1.4029 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:06.930921 140367827080960 learning.py:507] global step 11020: loss = 1.4275 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:09.290746 140367827080960 learning.py:507] global step 11030: loss = 1.4364 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:11.678077 140367827080960 learning.py:507] global step 11040: loss = 1.2569 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:14.053822 140367827080960 learning.py:507] global step 11050: loss = 1.5354 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:16.431025 140367827080960 learning.py:507] global step 11060: loss = 1.4776 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:18.799076 140367827080960 learning.py:507] global step 11070: loss = 1.4307 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:21.191986 140367827080960 learning.py:507] global step 11080: loss = 1.4685 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:23.569107 140367827080960 learning.py:507] global step 11090: loss = 1.3309 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:25.955058 140367827080960 learning.py:507] global step 11100: loss = 1.4388 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:28.321902 140367827080960 learning.py:507] global step 11110: loss = 1.4215 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:30.687284 140367827080960 learning.py:507] global step 11120: loss = 1.4916 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:33.071654 140367827080960 learning.py:507] global step 11130: loss = 1.5068 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:35.478038 140367827080960 learning.py:507] global step 11140: loss = 1.4763 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:37.839345 140367827080960 learning.py:507] global step 11150: loss = 1.3613 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:40.232575 140367827080960 learning.py:507] global step 11160: loss = 1.3720 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:42.642132 140367827080960 learning.py:507] global step 11170: loss = 1.4694 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:45.011766 140367827080960 learning.py:507] global step 11180: loss = 1.5537 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:47.390681 140367827080960 learning.py:507] global step 11190: loss = 1.4176 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:49.767257 140367827080960 learning.py:507] global step 11200: loss = 1.3789 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:52.141956 140367827080960 learning.py:507] global step 11210: loss = 1.4340 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:54.544718 140367827080960 learning.py:507] global step 11220: loss = 1.4999 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:56.894913 140367827080960 learning.py:507] global step 11230: loss = 1.3078 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:50:59.270855 140367827080960 learning.py:507] global step 11240: loss = 1.3459 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:01.646944 140367827080960 learning.py:507] global step 11250: loss = 1.4674 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:04.031725 140367827080960 learning.py:507] global step 11260: loss = 1.6450 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:06.379374 140367827080960 learning.py:507] global step 11270: loss = 1.3005 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:08.553334 140361930364672 supervisor.py:1099] global_step/sec: 4.2001\u001b[0m\n",
      "\u001b[32mI0701 01:51:08.749949 140367827080960 learning.py:507] global step 11280: loss = 1.4306 (0.259 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:10.441236 140361921971968 supervisor.py:1050] Recording summary at step 11282.\u001b[0m\n",
      "\u001b[32mI0701 01:51:12.162257 140367827080960 learning.py:507] global step 11290: loss = 1.3889 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:14.541260 140367827080960 learning.py:507] global step 11300: loss = 1.4095 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:16.936386 140367827080960 learning.py:507] global step 11310: loss = 1.3571 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:19.308773 140367827080960 learning.py:507] global step 11320: loss = 1.3624 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:21.687218 140367827080960 learning.py:507] global step 11330: loss = 1.4163 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:24.060684 140367827080960 learning.py:507] global step 11340: loss = 1.3454 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:26.407371 140367827080960 learning.py:507] global step 11350: loss = 1.3174 (0.222 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:28.761346 140367827080960 learning.py:507] global step 11360: loss = 1.4365 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:31.157685 140367827080960 learning.py:507] global step 11370: loss = 1.5825 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:33.505353 140367827080960 learning.py:507] global step 11380: loss = 1.4139 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:35.902267 140367827080960 learning.py:507] global step 11390: loss = 1.3712 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:38.257328 140367827080960 learning.py:507] global step 11400: loss = 1.3923 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:40.603935 140367827080960 learning.py:507] global step 11410: loss = 1.2986 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:43.004138 140367827080960 learning.py:507] global step 11420: loss = 1.3094 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:45.400162 140367827080960 learning.py:507] global step 11430: loss = 1.3052 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:47.809979 140367827080960 learning.py:507] global step 11440: loss = 1.3179 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:50.279395 140367827080960 learning.py:507] global step 11450: loss = 1.3556 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:52.666902 140367827080960 learning.py:507] global step 11460: loss = 1.3331 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:55.039892 140367827080960 learning.py:507] global step 11470: loss = 1.4326 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:57.414992 140367827080960 learning.py:507] global step 11480: loss = 1.3639 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:51:59.795162 140367827080960 learning.py:507] global step 11490: loss = 1.5235 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:02.173840 140367827080960 learning.py:507] global step 11500: loss = 1.5407 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:04.561686 140367827080960 learning.py:507] global step 11510: loss = 1.4182 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:06.953722 140367827080960 learning.py:507] global step 11520: loss = 1.4411 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:09.315570 140367827080960 learning.py:507] global step 11530: loss = 1.3279 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:11.715665 140367827080960 learning.py:507] global step 11540: loss = 1.3929 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:14.099308 140367827080960 learning.py:507] global step 11550: loss = 1.3876 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:16.491049 140367827080960 learning.py:507] global step 11560: loss = 1.4437 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:18.867515 140367827080960 learning.py:507] global step 11570: loss = 1.5504 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:21.260535 140367827080960 learning.py:507] global step 11580: loss = 1.4018 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:23.645605 140367827080960 learning.py:507] global step 11590: loss = 1.3084 (0.250 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:26.038140 140367827080960 learning.py:507] global step 11600: loss = 1.3625 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:28.441030 140367827080960 learning.py:507] global step 11610: loss = 1.3866 (0.249 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:30.815329 140367827080960 learning.py:507] global step 11620: loss = 1.3052 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:33.195647 140367827080960 learning.py:507] global step 11630: loss = 1.4166 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:35.600840 140367827080960 learning.py:507] global step 11640: loss = 1.3593 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:37.951894 140367827080960 learning.py:507] global step 11650: loss = 1.3759 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:40.322952 140367827080960 learning.py:507] global step 11660: loss = 1.3411 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:42.686038 140367827080960 learning.py:507] global step 11670: loss = 1.3853 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:45.045017 140367827080960 learning.py:507] global step 11680: loss = 1.4141 (0.232 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:52:47.429649 140367827080960 learning.py:507] global step 11690: loss = 1.4491 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:49.815994 140367827080960 learning.py:507] global step 11700: loss = 1.3915 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:52.213608 140367827080960 learning.py:507] global step 11710: loss = 1.3612 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:54.610546 140367827080960 learning.py:507] global step 11720: loss = 1.3677 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:57.001697 140367827080960 learning.py:507] global step 11730: loss = 1.3964 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:52:59.362385 140367827080960 learning.py:507] global step 11740: loss = 1.4181 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:01.769756 140367827080960 learning.py:507] global step 11750: loss = 1.3584 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:04.152853 140367827080960 learning.py:507] global step 11760: loss = 1.4304 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:06.510026 140367827080960 learning.py:507] global step 11770: loss = 1.4150 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:08.910067 140367827080960 learning.py:507] global step 11780: loss = 1.4642 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:11.282968 140367827080960 learning.py:507] global step 11790: loss = 1.4142 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:13.647042 140367827080960 learning.py:507] global step 11800: loss = 1.3317 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:16.019946 140367827080960 learning.py:507] global step 11810: loss = 1.3965 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:18.384615 140367827080960 learning.py:507] global step 11820: loss = 1.3624 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:20.763668 140367827080960 learning.py:507] global step 11830: loss = 1.3908 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:23.148394 140367827080960 learning.py:507] global step 11840: loss = 1.4124 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:25.538177 140367827080960 learning.py:507] global step 11850: loss = 1.3374 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:27.911412 140367827080960 learning.py:507] global step 11860: loss = 1.3231 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:30.292603 140367827080960 learning.py:507] global step 11870: loss = 1.5262 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:32.656790 140367827080960 learning.py:507] global step 11880: loss = 1.3950 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:35.062632 140367827080960 learning.py:507] global step 11890: loss = 1.4885 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:37.417260 140367827080960 learning.py:507] global step 11900: loss = 1.3399 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:39.809582 140367827080960 learning.py:507] global step 11910: loss = 1.4210 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:42.204010 140367827080960 learning.py:507] global step 11920: loss = 1.3381 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:44.581598 140367827080960 learning.py:507] global step 11930: loss = 1.3656 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:46.955184 140367827080960 learning.py:507] global step 11940: loss = 1.3966 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:49.322474 140367827080960 learning.py:507] global step 11950: loss = 1.4460 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:51.689712 140367827080960 learning.py:507] global step 11960: loss = 1.4860 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:54.073369 140367827080960 learning.py:507] global step 11970: loss = 1.3684 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:56.452883 140367827080960 learning.py:507] global step 11980: loss = 1.2770 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:53:58.865032 140367827080960 learning.py:507] global step 11990: loss = 1.4114 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:01.248994 140367827080960 learning.py:507] global step 12000: loss = 1.4489 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:03.646651 140367827080960 learning.py:507] global step 12010: loss = 1.4791 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:06.021785 140367827080960 learning.py:507] global step 12020: loss = 1.4068 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:08.408385 140367827080960 learning.py:507] global step 12030: loss = 1.4090 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:10.807220 140367827080960 learning.py:507] global step 12040: loss = 1.5116 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:13.188594 140367827080960 learning.py:507] global step 12050: loss = 1.4774 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:15.565993 140367827080960 learning.py:507] global step 12060: loss = 1.4264 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:17.912076 140367827080960 learning.py:507] global step 12070: loss = 1.4922 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:20.276618 140367827080960 learning.py:507] global step 12080: loss = 1.4510 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:22.639908 140367827080960 learning.py:507] global step 12090: loss = 1.4569 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:25.005548 140367827080960 learning.py:507] global step 12100: loss = 1.3382 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:27.353674 140367827080960 learning.py:507] global step 12110: loss = 1.3466 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:29.722615 140367827080960 learning.py:507] global step 12120: loss = 1.3449 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:32.085678 140367827080960 learning.py:507] global step 12130: loss = 1.4668 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:34.470670 140367827080960 learning.py:507] global step 12140: loss = 1.3906 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:36.850963 140367827080960 learning.py:507] global step 12150: loss = 1.3444 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:39.255886 140367827080960 learning.py:507] global step 12160: loss = 1.4667 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:41.637492 140367827080960 learning.py:507] global step 12170: loss = 1.5238 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:44.001583 140367827080960 learning.py:507] global step 12180: loss = 1.3402 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:46.366039 140367827080960 learning.py:507] global step 12190: loss = 1.4040 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:48.747215 140367827080960 learning.py:507] global step 12200: loss = 1.5407 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:51.132440 140367827080960 learning.py:507] global step 12210: loss = 1.3460 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:53.495174 140367827080960 learning.py:507] global step 12220: loss = 1.4154 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:55.881197 140367827080960 learning.py:507] global step 12230: loss = 1.3165 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:54:58.234687 140367827080960 learning.py:507] global step 12240: loss = 1.3850 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:00.627583 140367827080960 learning.py:507] global step 12250: loss = 1.4666 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:03.032293 140367827080960 learning.py:507] global step 12260: loss = 1.4009 (0.248 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:05.430663 140367827080960 learning.py:507] global step 12270: loss = 1.3249 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:07.796343 140367827080960 learning.py:507] global step 12280: loss = 1.4939 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:10.174088 140367827080960 learning.py:507] global step 12290: loss = 1.2854 (0.249 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:12.531888 140367827080960 learning.py:507] global step 12300: loss = 1.3903 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:14.915540 140367827080960 learning.py:507] global step 12310: loss = 1.4010 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:17.297866 140367827080960 learning.py:507] global step 12320: loss = 1.4524 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:19.683228 140367827080960 learning.py:507] global step 12330: loss = 1.4109 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:22.052963 140367827080960 learning.py:507] global step 12340: loss = 1.4056 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:24.422540 140367827080960 learning.py:507] global step 12350: loss = 1.4906 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:26.779196 140367827080960 learning.py:507] global step 12360: loss = 1.4176 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:29.133122 140367827080960 learning.py:507] global step 12370: loss = 1.3583 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:31.503106 140367827080960 learning.py:507] global step 12380: loss = 1.3536 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:33.965686 140367827080960 learning.py:507] global step 12390: loss = 1.3992 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:36.344002 140367827080960 learning.py:507] global step 12400: loss = 1.4479 (0.230 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:55:38.725064 140367827080960 learning.py:507] global step 12410: loss = 1.4033 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:41.115477 140367827080960 learning.py:507] global step 12420: loss = 1.4181 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:43.530858 140367827080960 learning.py:507] global step 12430: loss = 1.3615 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:45.874139 140367827080960 learning.py:507] global step 12440: loss = 1.3392 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:48.264301 140367827080960 learning.py:507] global step 12450: loss = 1.4342 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:50.635454 140367827080960 learning.py:507] global step 12460: loss = 1.4771 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:53.003410 140367827080960 learning.py:507] global step 12470: loss = 1.3988 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:55.355042 140367827080960 learning.py:507] global step 12480: loss = 1.3156 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:55:57.726031 140367827080960 learning.py:507] global step 12490: loss = 1.4567 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:00.118940 140367827080960 learning.py:507] global step 12500: loss = 1.4392 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:02.523700 140367827080960 learning.py:507] global step 12510: loss = 1.4832 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:04.933339 140367827080960 learning.py:507] global step 12520: loss = 1.3578 (0.249 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:07.313110 140367827080960 learning.py:507] global step 12530: loss = 1.3615 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:08.552144 140361938757376 supervisor.py:1117] Saving checkpoint to path /opt/ml/model/model.ckpt\u001b[0m\n",
      "\u001b[32mW0701 01:56:08.580401 140361938757376 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mUse standard file APIs to delete files with this prefix.\u001b[0m\n",
      "\u001b[32mI0701 01:56:10.273968 140361921971968 supervisor.py:1050] Recording summary at step 12538.\u001b[0m\n",
      "\u001b[32mI0701 01:56:10.691206 140367827080960 learning.py:507] global step 12540: loss = 1.3443 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:13.051619 140367827080960 learning.py:507] global step 12550: loss = 1.4611 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:15.438272 140367827080960 learning.py:507] global step 12560: loss = 1.3618 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:17.813208 140367827080960 learning.py:507] global step 12570: loss = 1.3632 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:20.208033 140367827080960 learning.py:507] global step 12580: loss = 1.3573 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:22.580936 140367827080960 learning.py:507] global step 12590: loss = 1.4078 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:24.966317 140367827080960 learning.py:507] global step 12600: loss = 1.3266 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:27.333753 140367827080960 learning.py:507] global step 12610: loss = 1.4193 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:29.693190 140367827080960 learning.py:507] global step 12620: loss = 1.2711 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:32.078121 140367827080960 learning.py:507] global step 12630: loss = 1.2599 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:34.458612 140367827080960 learning.py:507] global step 12640: loss = 1.3415 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:36.845494 140367827080960 learning.py:507] global step 12650: loss = 1.4288 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:39.227843 140367827080960 learning.py:507] global step 12660: loss = 1.4186 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:41.612354 140367827080960 learning.py:507] global step 12670: loss = 1.4379 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:44.013109 140367827080960 learning.py:507] global step 12680: loss = 1.3717 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:46.378555 140367827080960 learning.py:507] global step 12690: loss = 1.3005 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:48.762911 140367827080960 learning.py:507] global step 12700: loss = 1.3217 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:51.161765 140367827080960 learning.py:507] global step 12710: loss = 1.4730 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:53.511219 140367827080960 learning.py:507] global step 12720: loss = 1.3663 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:55.900765 140367827080960 learning.py:507] global step 12730: loss = 1.4117 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:56:58.253705 140367827080960 learning.py:507] global step 12740: loss = 1.3774 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:00.605600 140367827080960 learning.py:507] global step 12750: loss = 1.5005 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:02.985951 140367827080960 learning.py:507] global step 12760: loss = 1.3384 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:05.386948 140367827080960 learning.py:507] global step 12770: loss = 1.4902 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:07.738724 140367827080960 learning.py:507] global step 12780: loss = 1.3943 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:10.096776 140367827080960 learning.py:507] global step 12790: loss = 1.3638 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:12.438766 140367827080960 learning.py:507] global step 12800: loss = 1.4385 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:14.809789 140367827080960 learning.py:507] global step 12810: loss = 1.3925 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:17.186985 140367827080960 learning.py:507] global step 12820: loss = 1.4061 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:19.574707 140367827080960 learning.py:507] global step 12830: loss = 1.3818 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:21.920453 140367827080960 learning.py:507] global step 12840: loss = 1.3707 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:24.297724 140367827080960 learning.py:507] global step 12850: loss = 1.3011 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:26.686336 140367827080960 learning.py:507] global step 12860: loss = 1.4910 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:29.032356 140367827080960 learning.py:507] global step 12870: loss = 1.4879 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:31.416048 140367827080960 learning.py:507] global step 12880: loss = 1.2904 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:33.793477 140367827080960 learning.py:507] global step 12890: loss = 1.3142 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:36.178637 140367827080960 learning.py:507] global step 12900: loss = 1.3650 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:38.562566 140367827080960 learning.py:507] global step 12910: loss = 1.5313 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:40.958300 140367827080960 learning.py:507] global step 12920: loss = 1.5041 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:43.347806 140367827080960 learning.py:507] global step 12930: loss = 1.3839 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:45.737337 140367827080960 learning.py:507] global step 12940: loss = 1.3869 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:48.129529 140367827080960 learning.py:507] global step 12950: loss = 1.3924 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:50.512712 140367827080960 learning.py:507] global step 12960: loss = 1.2903 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:52.860349 140367827080960 learning.py:507] global step 12970: loss = 1.5446 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:55.213557 140367827080960 learning.py:507] global step 12980: loss = 1.2587 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:57.568082 140367827080960 learning.py:507] global step 12990: loss = 1.3886 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:57:59.921823 140367827080960 learning.py:507] global step 13000: loss = 1.5836 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:02.313245 140367827080960 learning.py:507] global step 13010: loss = 1.4072 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:04.702192 140367827080960 learning.py:507] global step 13020: loss = 1.3689 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:07.047378 140367827080960 learning.py:507] global step 13030: loss = 1.3966 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:09.428618 140367827080960 learning.py:507] global step 13040: loss = 1.5549 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:11.814677 140367827080960 learning.py:507] global step 13050: loss = 1.2839 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:14.179167 140367827080960 learning.py:507] global step 13060: loss = 1.3241 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:16.561881 140367827080960 learning.py:507] global step 13070: loss = 1.5023 (0.237 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 01:58:18.941777 140367827080960 learning.py:507] global step 13080: loss = 1.5478 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:21.343441 140367827080960 learning.py:507] global step 13090: loss = 1.3784 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:23.697429 140367827080960 learning.py:507] global step 13100: loss = 1.3500 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:26.064692 140367827080960 learning.py:507] global step 13110: loss = 1.5065 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:28.415188 140367827080960 learning.py:507] global step 13120: loss = 1.4657 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:30.783432 140367827080960 learning.py:507] global step 13130: loss = 1.2990 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:33.135495 140367827080960 learning.py:507] global step 13140: loss = 1.3152 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:35.495562 140367827080960 learning.py:507] global step 13150: loss = 1.3612 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:37.854080 140367827080960 learning.py:507] global step 13160: loss = 1.3489 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:40.237556 140367827080960 learning.py:507] global step 13170: loss = 1.4272 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:42.605022 140367827080960 learning.py:507] global step 13180: loss = 1.2349 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:44.998950 140367827080960 learning.py:507] global step 13190: loss = 1.3415 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:47.387305 140367827080960 learning.py:507] global step 13200: loss = 1.5089 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:49.765727 140367827080960 learning.py:507] global step 13210: loss = 1.3717 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:52.118210 140367827080960 learning.py:507] global step 13220: loss = 1.3752 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:54.481257 140367827080960 learning.py:507] global step 13230: loss = 1.4054 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:56.870094 140367827080960 learning.py:507] global step 13240: loss = 1.3010 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:58:59.273209 140367827080960 learning.py:507] global step 13250: loss = 1.3179 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:01.656445 140367827080960 learning.py:507] global step 13260: loss = 1.4504 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:04.037790 140367827080960 learning.py:507] global step 13270: loss = 1.4127 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:06.381599 140367827080960 learning.py:507] global step 13280: loss = 1.5720 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:08.740104 140367827080960 learning.py:507] global step 13290: loss = 1.3205 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:11.117388 140367827080960 learning.py:507] global step 13300: loss = 1.3747 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:13.500499 140367827080960 learning.py:507] global step 13310: loss = 1.3735 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:15.905647 140367827080960 learning.py:507] global step 13320: loss = 1.3528 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:18.288779 140367827080960 learning.py:507] global step 13330: loss = 1.4532 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:20.660563 140367827080960 learning.py:507] global step 13340: loss = 1.3578 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:23.018913 140367827080960 learning.py:507] global step 13350: loss = 1.4616 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:25.394246 140367827080960 learning.py:507] global step 13360: loss = 1.3739 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:27.770741 140367827080960 learning.py:507] global step 13370: loss = 1.2894 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:30.129995 140367827080960 learning.py:507] global step 13380: loss = 1.3791 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:32.487396 140367827080960 learning.py:507] global step 13390: loss = 1.3225 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:34.863202 140367827080960 learning.py:507] global step 13400: loss = 1.4318 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:37.258102 140367827080960 learning.py:507] global step 13410: loss = 1.3947 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:39.626126 140367827080960 learning.py:507] global step 13420: loss = 1.3028 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:42.016590 140367827080960 learning.py:507] global step 13430: loss = 1.4115 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:44.397224 140367827080960 learning.py:507] global step 13440: loss = 1.3983 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:46.789247 140367827080960 learning.py:507] global step 13450: loss = 1.4307 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:49.176713 140367827080960 learning.py:507] global step 13460: loss = 1.4373 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:51.552922 140367827080960 learning.py:507] global step 13470: loss = 1.3926 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:53.917562 140367827080960 learning.py:507] global step 13480: loss = 1.3601 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:56.282705 140367827080960 learning.py:507] global step 13490: loss = 1.4300 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 01:59:58.662017 140367827080960 learning.py:507] global step 13500: loss = 1.3571 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:01.021697 140367827080960 learning.py:507] global step 13510: loss = 1.5484 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:03.423084 140367827080960 learning.py:507] global step 13520: loss = 1.3429 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:05.773396 140367827080960 learning.py:507] global step 13530: loss = 1.3403 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:08.136528 140367827080960 learning.py:507] global step 13540: loss = 1.4315 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:10.518711 140367827080960 learning.py:507] global step 13550: loss = 1.4948 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:12.928750 140367827080960 learning.py:507] global step 13560: loss = 1.5019 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:15.291999 140367827080960 learning.py:507] global step 13570: loss = 1.4570 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:17.674242 140367827080960 learning.py:507] global step 13580: loss = 1.3224 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:20.076020 140367827080960 learning.py:507] global step 13590: loss = 1.3823 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:22.415949 140367827080960 learning.py:507] global step 13600: loss = 1.4749 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:24.813421 140367827080960 learning.py:507] global step 13610: loss = 1.3849 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:27.208964 140367827080960 learning.py:507] global step 13620: loss = 1.2527 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:29.555789 140367827080960 learning.py:507] global step 13630: loss = 1.2724 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:31.925045 140367827080960 learning.py:507] global step 13640: loss = 1.3412 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:34.259886 140367827080960 learning.py:507] global step 13650: loss = 1.3941 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:36.618329 140367827080960 learning.py:507] global step 13660: loss = 1.3783 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:38.988393 140367827080960 learning.py:507] global step 13670: loss = 1.4321 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:41.376806 140367827080960 learning.py:507] global step 13680: loss = 1.3980 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:43.768130 140367827080960 learning.py:507] global step 13690: loss = 1.2863 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:46.133414 140367827080960 learning.py:507] global step 13700: loss = 1.3872 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:48.502274 140367827080960 learning.py:507] global step 13710: loss = 1.2763 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:50.840476 140367827080960 learning.py:507] global step 13720: loss = 1.4254 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:53.217921 140367827080960 learning.py:507] global step 13730: loss = 1.2829 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:55.589678 140367827080960 learning.py:507] global step 13740: loss = 1.5062 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:00:57.992211 140367827080960 learning.py:507] global step 13750: loss = 1.3965 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:00.391535 140367827080960 learning.py:507] global step 13760: loss = 1.4993 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:02.767513 140367827080960 learning.py:507] global step 13770: loss = 1.4022 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:05.119797 140367827080960 learning.py:507] global step 13780: loss = 1.3148 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:07.542124 140367827080960 learning.py:507] global step 13790: loss = 1.4455 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:10.122514 140361921971968 supervisor.py:1050] Recording summary at step 13796.\u001b[0m\n",
      "\u001b[32mI0701 02:01:10.857108 140367827080960 learning.py:507] global step 13800: loss = 1.4910 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:13.232501 140367827080960 learning.py:507] global step 13810: loss = 1.3796 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:15.585170 140367827080960 learning.py:507] global step 13820: loss = 1.3458 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:17.958928 140367827080960 learning.py:507] global step 13830: loss = 1.4606 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:20.320322 140367827080960 learning.py:507] global step 13840: loss = 1.4525 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:22.686544 140367827080960 learning.py:507] global step 13850: loss = 1.3590 (0.239 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 02:01:25.061774 140367827080960 learning.py:507] global step 13860: loss = 1.3685 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:27.416923 140367827080960 learning.py:507] global step 13870: loss = 1.4380 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:29.814148 140367827080960 learning.py:507] global step 13880: loss = 1.4255 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:32.189137 140367827080960 learning.py:507] global step 13890: loss = 1.4866 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:34.561567 140367827080960 learning.py:507] global step 13900: loss = 1.3683 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:36.925004 140367827080960 learning.py:507] global step 13910: loss = 1.3832 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:39.315049 140367827080960 learning.py:507] global step 13920: loss = 1.3572 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:41.704598 140367827080960 learning.py:507] global step 13930: loss = 1.3273 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:44.050980 140367827080960 learning.py:507] global step 13940: loss = 1.4111 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:46.426661 140367827080960 learning.py:507] global step 13950: loss = 1.4898 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:48.808919 140367827080960 learning.py:507] global step 13960: loss = 1.4445 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:51.191812 140367827080960 learning.py:507] global step 13970: loss = 1.5023 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:53.588998 140367827080960 learning.py:507] global step 13980: loss = 1.2574 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:55.952127 140367827080960 learning.py:507] global step 13990: loss = 1.3461 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:01:58.317544 140367827080960 learning.py:507] global step 14000: loss = 1.4028 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:00.712670 140367827080960 learning.py:507] global step 14010: loss = 1.3815 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:03.055167 140367827080960 learning.py:507] global step 14020: loss = 1.2787 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:05.430445 140367827080960 learning.py:507] global step 14030: loss = 1.4107 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:07.789876 140367827080960 learning.py:507] global step 14040: loss = 1.4752 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:10.158029 140367827080960 learning.py:507] global step 14050: loss = 1.2684 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:12.546884 140367827080960 learning.py:507] global step 14060: loss = 1.4333 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:14.895258 140367827080960 learning.py:507] global step 14070: loss = 1.3114 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:17.252907 140367827080960 learning.py:507] global step 14080: loss = 1.4493 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:19.623173 140367827080960 learning.py:507] global step 14090: loss = 1.3544 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:21.999929 140367827080960 learning.py:507] global step 14100: loss = 1.4386 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:24.360986 140367827080960 learning.py:507] global step 14110: loss = 1.4728 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:26.717278 140367827080960 learning.py:507] global step 14120: loss = 1.3608 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:29.078548 140367827080960 learning.py:507] global step 14130: loss = 1.3884 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:31.443142 140367827080960 learning.py:507] global step 14140: loss = 1.3497 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:33.838932 140367827080960 learning.py:507] global step 14150: loss = 1.5079 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:36.175725 140367827080960 learning.py:507] global step 14160: loss = 1.4429 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:38.534661 140367827080960 learning.py:507] global step 14170: loss = 1.3891 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:40.915266 140367827080960 learning.py:507] global step 14180: loss = 1.3387 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:43.305049 140367827080960 learning.py:507] global step 14190: loss = 1.3490 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:45.682296 140367827080960 learning.py:507] global step 14200: loss = 1.4367 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:48.071974 140367827080960 learning.py:507] global step 14210: loss = 1.3384 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:50.441314 140367827080960 learning.py:507] global step 14220: loss = 1.4156 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:52.810184 140367827080960 learning.py:507] global step 14230: loss = 1.3988 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:55.155489 140367827080960 learning.py:507] global step 14240: loss = 1.4395 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:57.518784 140367827080960 learning.py:507] global step 14250: loss = 1.4017 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:02:59.888119 140367827080960 learning.py:507] global step 14260: loss = 1.4090 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:02.274743 140367827080960 learning.py:507] global step 14270: loss = 1.3444 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:04.621135 140367827080960 learning.py:507] global step 14280: loss = 1.4809 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:07.010087 140367827080960 learning.py:507] global step 14290: loss = 1.2876 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:09.399070 140367827080960 learning.py:507] global step 14300: loss = 1.4563 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:11.764813 140367827080960 learning.py:507] global step 14310: loss = 1.4002 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:14.117580 140367827080960 learning.py:507] global step 14320: loss = 1.3339 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:16.493021 140367827080960 learning.py:507] global step 14330: loss = 1.3760 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:18.854888 140367827080960 learning.py:507] global step 14340: loss = 1.3505 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:21.291714 140367827080960 learning.py:507] global step 14350: loss = 1.3853 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:23.667895 140367827080960 learning.py:507] global step 14360: loss = 1.4507 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:26.061414 140367827080960 learning.py:507] global step 14370: loss = 1.4758 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:28.425199 140367827080960 learning.py:507] global step 14380: loss = 1.4516 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:30.804469 140367827080960 learning.py:507] global step 14390: loss = 1.2779 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:33.203644 140367827080960 learning.py:507] global step 14400: loss = 1.4354 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:35.610515 140367827080960 learning.py:507] global step 14410: loss = 1.3534 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:37.995770 140367827080960 learning.py:507] global step 14420: loss = 1.3556 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:40.371593 140367827080960 learning.py:507] global step 14430: loss = 1.4800 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:42.749456 140367827080960 learning.py:507] global step 14440: loss = 1.4151 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:45.117882 140367827080960 learning.py:507] global step 14450: loss = 1.2782 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:47.490278 140367827080960 learning.py:507] global step 14460: loss = 1.4835 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:49.877135 140367827080960 learning.py:507] global step 14470: loss = 1.4124 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:52.230294 140367827080960 learning.py:507] global step 14480: loss = 1.3587 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:54.593857 140367827080960 learning.py:507] global step 14490: loss = 1.4257 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:56.974670 140367827080960 learning.py:507] global step 14500: loss = 1.5713 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:03:59.348042 140367827080960 learning.py:507] global step 14510: loss = 1.2496 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:01.714346 140367827080960 learning.py:507] global step 14520: loss = 1.4800 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:04.099937 140367827080960 learning.py:507] global step 14530: loss = 1.3510 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:06.479677 140367827080960 learning.py:507] global step 14540: loss = 1.4960 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:08.850507 140367827080960 learning.py:507] global step 14550: loss = 1.3186 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:11.217912 140367827080960 learning.py:507] global step 14560: loss = 1.4693 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:13.641918 140367827080960 learning.py:507] global step 14570: loss = 1.4725 (0.234 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 02:04:16.050041 140367827080960 learning.py:507] global step 14580: loss = 1.3273 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:18.404109 140367827080960 learning.py:507] global step 14590: loss = 1.3871 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:20.798896 140367827080960 learning.py:507] global step 14600: loss = 1.4473 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:23.186650 140367827080960 learning.py:507] global step 14610: loss = 1.3827 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:25.553075 140367827080960 learning.py:507] global step 14620: loss = 1.4290 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:27.925122 140367827080960 learning.py:507] global step 14630: loss = 1.4502 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:30.306108 140367827080960 learning.py:507] global step 14640: loss = 1.3899 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:32.690135 140367827080960 learning.py:507] global step 14650: loss = 1.3471 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:35.059233 140367827080960 learning.py:507] global step 14660: loss = 1.5013 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:37.416045 140367827080960 learning.py:507] global step 14670: loss = 1.4045 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:39.788825 140367827080960 learning.py:507] global step 14680: loss = 1.3726 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:42.152237 140367827080960 learning.py:507] global step 14690: loss = 1.3662 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:44.527342 140367827080960 learning.py:507] global step 14700: loss = 1.4574 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:46.895167 140367827080960 learning.py:507] global step 14710: loss = 1.3782 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:49.255462 140367827080960 learning.py:507] global step 14720: loss = 1.3276 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:51.638735 140367827080960 learning.py:507] global step 14730: loss = 1.4466 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:54.023190 140367827080960 learning.py:507] global step 14740: loss = 1.4621 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:56.377109 140367827080960 learning.py:507] global step 14750: loss = 1.3555 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:04:58.732115 140367827080960 learning.py:507] global step 14760: loss = 1.3325 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:01.115694 140367827080960 learning.py:507] global step 14770: loss = 1.4534 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:03.515306 140367827080960 learning.py:507] global step 14780: loss = 1.3743 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:05.897134 140367827080960 learning.py:507] global step 14790: loss = 1.4506 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:08.279172 140367827080960 learning.py:507] global step 14800: loss = 1.3723 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:10.651238 140367827080960 learning.py:507] global step 14810: loss = 1.2689 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:13.015784 140367827080960 learning.py:507] global step 14820: loss = 1.2713 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:15.389396 140367827080960 learning.py:507] global step 14830: loss = 1.3288 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:17.766510 140367827080960 learning.py:507] global step 14840: loss = 1.3599 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:20.122165 140367827080960 learning.py:507] global step 14850: loss = 1.3915 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:22.488941 140367827080960 learning.py:507] global step 14860: loss = 1.3678 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:24.852629 140367827080960 learning.py:507] global step 14870: loss = 1.4539 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:27.249536 140367827080960 learning.py:507] global step 14880: loss = 1.4005 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:29.632098 140367827080960 learning.py:507] global step 14890: loss = 1.4070 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:31.989979 140367827080960 learning.py:507] global step 14900: loss = 1.4345 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:34.354163 140367827080960 learning.py:507] global step 14910: loss = 1.4904 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:36.727559 140367827080960 learning.py:507] global step 14920: loss = 1.4085 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:39.108942 140367827080960 learning.py:507] global step 14930: loss = 1.4154 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:41.470591 140367827080960 learning.py:507] global step 14940: loss = 1.3471 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:43.814821 140367827080960 learning.py:507] global step 14950: loss = 1.3410 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:46.181274 140367827080960 learning.py:507] global step 14960: loss = 1.4865 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:48.560322 140367827080960 learning.py:507] global step 14970: loss = 1.3824 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:50.888143 140367827080960 learning.py:507] global step 14980: loss = 1.2794 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:53.262872 140367827080960 learning.py:507] global step 14990: loss = 1.4136 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:55.621045 140367827080960 learning.py:507] global step 15000: loss = 1.3996 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:05:57.991760 140367827080960 learning.py:507] global step 15010: loss = 1.3627 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:00.369507 140367827080960 learning.py:507] global step 15020: loss = 1.4059 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:02.740489 140367827080960 learning.py:507] global step 15030: loss = 1.3824 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:05.092915 140367827080960 learning.py:507] global step 15040: loss = 1.4315 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:07.478527 140367827080960 learning.py:507] global step 15050: loss = 1.4809 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:08.552211 140361938757376 supervisor.py:1117] Saving checkpoint to path /opt/ml/model/model.ckpt\u001b[0m\n",
      "\u001b[32mI0701 02:06:10.264837 140361921971968 supervisor.py:1050] Recording summary at step 15057.\u001b[0m\n",
      "\u001b[32mI0701 02:06:10.895996 140367827080960 learning.py:507] global step 15060: loss = 1.3343 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:13.298929 140367827080960 learning.py:507] global step 15070: loss = 1.4457 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:15.702873 140367827080960 learning.py:507] global step 15080: loss = 1.4952 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:18.068399 140367827080960 learning.py:507] global step 15090: loss = 1.3439 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:20.426325 140367827080960 learning.py:507] global step 15100: loss = 1.4115 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:22.815505 140367827080960 learning.py:507] global step 15110: loss = 1.5122 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:25.201900 140367827080960 learning.py:507] global step 15120: loss = 1.4743 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:27.566184 140367827080960 learning.py:507] global step 15130: loss = 1.3092 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:29.909086 140367827080960 learning.py:507] global step 15140: loss = 1.4017 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:32.278017 140367827080960 learning.py:507] global step 15150: loss = 1.3839 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:34.661808 140367827080960 learning.py:507] global step 15160: loss = 1.3589 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:37.050795 140367827080960 learning.py:507] global step 15170: loss = 1.4180 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:39.463144 140367827080960 learning.py:507] global step 15180: loss = 1.3397 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:41.854306 140367827080960 learning.py:507] global step 15190: loss = 1.3882 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:44.220943 140367827080960 learning.py:507] global step 15200: loss = 1.3831 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:46.597794 140367827080960 learning.py:507] global step 15210: loss = 1.3167 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:48.973537 140367827080960 learning.py:507] global step 15220: loss = 1.3711 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:51.335308 140367827080960 learning.py:507] global step 15230: loss = 1.3467 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:53.703520 140367827080960 learning.py:507] global step 15240: loss = 1.3964 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:56.080188 140367827080960 learning.py:507] global step 15250: loss = 1.4000 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:06:58.482268 140367827080960 learning.py:507] global step 15260: loss = 1.3127 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:00.868628 140367827080960 learning.py:507] global step 15270: loss = 1.3132 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:03.231349 140367827080960 learning.py:507] global step 15280: loss = 1.3408 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:05.591080 140367827080960 learning.py:507] global step 15290: loss = 1.4330 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:07.939737 140367827080960 learning.py:507] global step 15300: loss = 1.3880 (0.241 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 02:07:10.311676 140367827080960 learning.py:507] global step 15310: loss = 1.4037 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:12.651503 140367827080960 learning.py:507] global step 15320: loss = 1.4680 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:15.021303 140367827080960 learning.py:507] global step 15330: loss = 1.3516 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:17.398276 140367827080960 learning.py:507] global step 15340: loss = 1.3300 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:19.767375 140367827080960 learning.py:507] global step 15350: loss = 1.3756 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:22.165488 140367827080960 learning.py:507] global step 15360: loss = 1.3305 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:24.559042 140367827080960 learning.py:507] global step 15370: loss = 1.4520 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:26.925276 140367827080960 learning.py:507] global step 15380: loss = 1.3682 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:29.296529 140367827080960 learning.py:507] global step 15390: loss = 1.4188 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:31.640921 140367827080960 learning.py:507] global step 15400: loss = 1.3341 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:34.002224 140367827080960 learning.py:507] global step 15410: loss = 1.4203 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:36.379405 140367827080960 learning.py:507] global step 15420: loss = 1.4107 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:38.722091 140367827080960 learning.py:507] global step 15430: loss = 1.3351 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:41.115394 140367827080960 learning.py:507] global step 15440: loss = 1.5404 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:43.492017 140367827080960 learning.py:507] global step 15450: loss = 1.3980 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:45.882731 140367827080960 learning.py:507] global step 15460: loss = 1.3948 (0.249 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:48.253622 140367827080960 learning.py:507] global step 15470: loss = 1.3183 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:50.618309 140367827080960 learning.py:507] global step 15480: loss = 1.3353 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:52.993195 140367827080960 learning.py:507] global step 15490: loss = 1.3631 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:55.394970 140367827080960 learning.py:507] global step 15500: loss = 1.3356 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:07:57.778836 140367827080960 learning.py:507] global step 15510: loss = 1.4528 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:00.139734 140367827080960 learning.py:507] global step 15520: loss = 1.3509 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:02.544348 140367827080960 learning.py:507] global step 15530: loss = 1.3653 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:04.906969 140367827080960 learning.py:507] global step 15540: loss = 1.4510 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:07.263705 140367827080960 learning.py:507] global step 15550: loss = 1.3593 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:09.665554 140367827080960 learning.py:507] global step 15560: loss = 1.4706 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:12.021436 140367827080960 learning.py:507] global step 15570: loss = 1.2724 (0.223 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:14.403873 140367827080960 learning.py:507] global step 15580: loss = 1.3978 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:16.793853 140367827080960 learning.py:507] global step 15590: loss = 1.3984 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:19.145165 140367827080960 learning.py:507] global step 15600: loss = 1.2814 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:21.533399 140367827080960 learning.py:507] global step 15610: loss = 1.3879 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:23.878431 140367827080960 learning.py:507] global step 15620: loss = 1.3566 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:26.257770 140367827080960 learning.py:507] global step 15630: loss = 1.3981 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:28.629196 140367827080960 learning.py:507] global step 15640: loss = 1.4001 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:30.984496 140367827080960 learning.py:507] global step 15650: loss = 1.3715 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:33.378487 140367827080960 learning.py:507] global step 15660: loss = 1.3071 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:35.753413 140367827080960 learning.py:507] global step 15670: loss = 1.3063 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:38.154711 140367827080960 learning.py:507] global step 15680: loss = 1.3307 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:40.508922 140367827080960 learning.py:507] global step 15690: loss = 1.3842 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:42.872946 140367827080960 learning.py:507] global step 15700: loss = 1.2954 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:45.227929 140367827080960 learning.py:507] global step 15710: loss = 1.3250 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:47.626974 140367827080960 learning.py:507] global step 15720: loss = 1.4368 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:49.993757 140367827080960 learning.py:507] global step 15730: loss = 1.4066 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:52.361368 140367827080960 learning.py:507] global step 15740: loss = 1.3901 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:54.722559 140367827080960 learning.py:507] global step 15750: loss = 1.4463 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:57.074884 140367827080960 learning.py:507] global step 15760: loss = 1.3502 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:08:59.430594 140367827080960 learning.py:507] global step 15770: loss = 1.3938 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:01.812691 140367827080960 learning.py:507] global step 15780: loss = 1.4358 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:04.197799 140367827080960 learning.py:507] global step 15790: loss = 1.4583 (0.248 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:06.594384 140367827080960 learning.py:507] global step 15800: loss = 1.3880 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:08.958879 140367827080960 learning.py:507] global step 15810: loss = 1.3851 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:11.328846 140367827080960 learning.py:507] global step 15820: loss = 1.3993 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:13.680851 140367827080960 learning.py:507] global step 15830: loss = 1.4337 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:16.039613 140367827080960 learning.py:507] global step 15840: loss = 1.4539 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:18.410195 140367827080960 learning.py:507] global step 15850: loss = 1.3636 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:20.783987 140367827080960 learning.py:507] global step 15860: loss = 1.3674 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:23.158901 140367827080960 learning.py:507] global step 15870: loss = 1.3258 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:25.535240 140367827080960 learning.py:507] global step 15880: loss = 1.4395 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:27.921432 140367827080960 learning.py:507] global step 15890: loss = 1.4703 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:30.297589 140367827080960 learning.py:507] global step 15900: loss = 1.2978 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:32.636701 140367827080960 learning.py:507] global step 15910: loss = 1.4429 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:35.022397 140367827080960 learning.py:507] global step 15920: loss = 1.3153 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:37.408618 140367827080960 learning.py:507] global step 15930: loss = 1.4998 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:39.769254 140367827080960 learning.py:507] global step 15940: loss = 1.3248 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:42.136842 140367827080960 learning.py:507] global step 15950: loss = 1.3482 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:44.521528 140367827080960 learning.py:507] global step 15960: loss = 1.3226 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:46.894462 140367827080960 learning.py:507] global step 15970: loss = 1.3771 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:49.281023 140367827080960 learning.py:507] global step 15980: loss = 1.3686 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:51.684067 140367827080960 learning.py:507] global step 15990: loss = 1.3902 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:54.035789 140367827080960 learning.py:507] global step 16000: loss = 1.3763 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:56.414663 140367827080960 learning.py:507] global step 16010: loss = 1.3277 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:09:58.788167 140367827080960 learning.py:507] global step 16020: loss = 1.4381 (0.232 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 02:10:01.142545 140367827080960 learning.py:507] global step 16030: loss = 1.3655 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:03.491894 140367827080960 learning.py:507] global step 16040: loss = 1.3753 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:05.834808 140367827080960 learning.py:507] global step 16050: loss = 1.4096 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:08.192497 140367827080960 learning.py:507] global step 16060: loss = 1.4480 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:10.552006 140367827080960 learning.py:507] global step 16070: loss = 1.3529 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:12.969381 140367827080960 learning.py:507] global step 16080: loss = 1.4108 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:15.355283 140367827080960 learning.py:507] global step 16090: loss = 1.4776 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:17.725588 140367827080960 learning.py:507] global step 16100: loss = 1.3533 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:20.115427 140367827080960 learning.py:507] global step 16110: loss = 1.4263 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:22.485135 140367827080960 learning.py:507] global step 16120: loss = 1.2963 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:24.847664 140367827080960 learning.py:507] global step 16130: loss = 1.4932 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:27.194282 140367827080960 learning.py:507] global step 16140: loss = 1.4505 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:29.577493 140367827080960 learning.py:507] global step 16150: loss = 1.4078 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:31.966805 140367827080960 learning.py:507] global step 16160: loss = 1.3398 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:34.325556 140367827080960 learning.py:507] global step 16170: loss = 1.4100 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:36.687915 140367827080960 learning.py:507] global step 16180: loss = 1.5194 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:39.034257 140367827080960 learning.py:507] global step 16190: loss = 1.4445 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:41.399018 140367827080960 learning.py:507] global step 16200: loss = 1.2814 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:43.762543 140367827080960 learning.py:507] global step 16210: loss = 1.3838 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:46.135936 140367827080960 learning.py:507] global step 16220: loss = 1.2791 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:48.475363 140367827080960 learning.py:507] global step 16230: loss = 1.4124 (0.221 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:50.835536 140367827080960 learning.py:507] global step 16240: loss = 1.4946 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:53.203962 140367827080960 learning.py:507] global step 16250: loss = 1.2775 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:55.546484 140367827080960 learning.py:507] global step 16260: loss = 1.3661 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:10:57.913378 140367827080960 learning.py:507] global step 16270: loss = 1.4685 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:00.306422 140367827080960 learning.py:507] global step 16280: loss = 1.4257 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:02.698673 140367827080960 learning.py:507] global step 16290: loss = 1.3128 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:05.069901 140367827080960 learning.py:507] global step 16300: loss = 1.4850 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:07.443703 140367827080960 learning.py:507] global step 16310: loss = 1.3978 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:10.120148 140361921971968 supervisor.py:1050] Recording summary at step 16317.\u001b[0m\n",
      "\u001b[32mI0701 02:11:10.769182 140367827080960 learning.py:507] global step 16320: loss = 1.4721 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:13.133682 140367827080960 learning.py:507] global step 16330: loss = 1.4041 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:15.484508 140367827080960 learning.py:507] global step 16340: loss = 1.3632 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:17.860652 140367827080960 learning.py:507] global step 16350: loss = 1.3116 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:20.208997 140367827080960 learning.py:507] global step 16360: loss = 1.3719 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:22.595686 140367827080960 learning.py:507] global step 16370: loss = 1.4070 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:24.972213 140367827080960 learning.py:507] global step 16380: loss = 1.4022 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:27.340595 140367827080960 learning.py:507] global step 16390: loss = 1.4041 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:29.728418 140367827080960 learning.py:507] global step 16400: loss = 1.3816 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:32.116389 140367827080960 learning.py:507] global step 16410: loss = 1.4748 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:34.477616 140367827080960 learning.py:507] global step 16420: loss = 1.3475 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:36.855505 140367827080960 learning.py:507] global step 16430: loss = 1.4437 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:39.211019 140367827080960 learning.py:507] global step 16440: loss = 1.4412 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:41.584769 140367827080960 learning.py:507] global step 16450: loss = 1.4963 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:43.956943 140367827080960 learning.py:507] global step 16460: loss = 1.3377 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:46.322808 140367827080960 learning.py:507] global step 16470: loss = 1.5052 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:48.683533 140367827080960 learning.py:507] global step 16480: loss = 1.3648 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:51.050378 140367827080960 learning.py:507] global step 16490: loss = 1.3341 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:53.430531 140367827080960 learning.py:507] global step 16500: loss = 1.2777 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:55.760706 140367827080960 learning.py:507] global step 16510: loss = 1.3211 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:11:58.149790 140367827080960 learning.py:507] global step 16520: loss = 1.4170 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:00.543081 140367827080960 learning.py:507] global step 16530: loss = 1.3799 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:02.889885 140367827080960 learning.py:507] global step 16540: loss = 1.3551 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:05.263384 140367827080960 learning.py:507] global step 16550: loss = 1.3529 (0.250 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:07.629230 140367827080960 learning.py:507] global step 16560: loss = 1.4859 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:09.966886 140367827080960 learning.py:507] global step 16570: loss = 1.2869 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:12.334969 140367827080960 learning.py:507] global step 16580: loss = 1.3497 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:14.705756 140367827080960 learning.py:507] global step 16590: loss = 1.4194 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:17.095022 140367827080960 learning.py:507] global step 16600: loss = 1.4036 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:19.486979 140367827080960 learning.py:507] global step 16610: loss = 1.4837 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:21.871248 140367827080960 learning.py:507] global step 16620: loss = 1.2613 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:24.237377 140367827080960 learning.py:507] global step 16630: loss = 1.4027 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:26.599170 140367827080960 learning.py:507] global step 16640: loss = 1.4086 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:28.990209 140367827080960 learning.py:507] global step 16650: loss = 1.3503 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:31.386404 140367827080960 learning.py:507] global step 16660: loss = 1.3710 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:33.742031 140367827080960 learning.py:507] global step 16670: loss = 1.3251 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:36.094506 140367827080960 learning.py:507] global step 16680: loss = 1.3149 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:38.463687 140367827080960 learning.py:507] global step 16690: loss = 1.4427 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:40.832801 140367827080960 learning.py:507] global step 16700: loss = 1.3890 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:43.184901 140367827080960 learning.py:507] global step 16710: loss = 1.3698 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:45.550288 140367827080960 learning.py:507] global step 16720: loss = 1.2957 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:47.909693 140367827080960 learning.py:507] global step 16730: loss = 1.3908 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:50.296256 140367827080960 learning.py:507] global step 16740: loss = 1.4111 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:52.679723 140367827080960 learning.py:507] global step 16750: loss = 1.2886 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:55.059453 140367827080960 learning.py:507] global step 16760: loss = 1.3890 (0.234 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 02:12:57.439519 140367827080960 learning.py:507] global step 16770: loss = 1.3084 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:12:59.799427 140367827080960 learning.py:507] global step 16780: loss = 1.3860 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:02.168826 140367827080960 learning.py:507] global step 16790: loss = 1.3185 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:04.518376 140367827080960 learning.py:507] global step 16800: loss = 1.3074 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:06.877789 140367827080960 learning.py:507] global step 16810: loss = 1.3709 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:09.232033 140367827080960 learning.py:507] global step 16820: loss = 1.3981 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:11.610884 140367827080960 learning.py:507] global step 16830: loss = 1.3926 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:13.971019 140367827080960 learning.py:507] global step 16840: loss = 1.2730 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:16.357522 140367827080960 learning.py:507] global step 16850: loss = 1.3245 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:18.715797 140367827080960 learning.py:507] global step 16860: loss = 1.3883 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:21.096581 140367827080960 learning.py:507] global step 16870: loss = 1.3927 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:23.464034 140367827080960 learning.py:507] global step 16880: loss = 1.4160 (0.248 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:25.830710 140367827080960 learning.py:507] global step 16890: loss = 1.3923 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:28.210612 140367827080960 learning.py:507] global step 16900: loss = 1.3976 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:30.572002 140367827080960 learning.py:507] global step 16910: loss = 1.4698 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:32.957401 140367827080960 learning.py:507] global step 16920: loss = 1.4348 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:35.330544 140367827080960 learning.py:507] global step 16930: loss = 1.3249 (0.248 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:37.714375 140367827080960 learning.py:507] global step 16940: loss = 1.3823 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:40.106831 140367827080960 learning.py:507] global step 16950: loss = 1.4366 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:42.479624 140367827080960 learning.py:507] global step 16960: loss = 1.2894 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:44.842525 140367827080960 learning.py:507] global step 16970: loss = 1.4179 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:47.221889 140367827080960 learning.py:507] global step 16980: loss = 1.3797 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:49.617379 140367827080960 learning.py:507] global step 16990: loss = 1.3257 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:51.966830 140367827080960 learning.py:507] global step 17000: loss = 1.4790 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:54.335956 140367827080960 learning.py:507] global step 17010: loss = 1.3249 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:56.720786 140367827080960 learning.py:507] global step 17020: loss = 1.3304 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:13:59.115088 140367827080960 learning.py:507] global step 17030: loss = 1.3939 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:01.511349 140367827080960 learning.py:507] global step 17040: loss = 1.2694 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:03.885529 140367827080960 learning.py:507] global step 17050: loss = 1.4059 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:06.258613 140367827080960 learning.py:507] global step 17060: loss = 1.3770 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:08.632796 140367827080960 learning.py:507] global step 17070: loss = 1.3678 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:11.009699 140367827080960 learning.py:507] global step 17080: loss = 1.3527 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:13.392580 140367827080960 learning.py:507] global step 17090: loss = 1.4185 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:15.751674 140367827080960 learning.py:507] global step 17100: loss = 1.4026 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:18.111479 140367827080960 learning.py:507] global step 17110: loss = 1.4624 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:20.461230 140367827080960 learning.py:507] global step 17120: loss = 1.5376 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:22.833949 140367827080960 learning.py:507] global step 17130: loss = 1.4765 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:25.213222 140367827080960 learning.py:507] global step 17140: loss = 1.3611 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:27.594099 140367827080960 learning.py:507] global step 17150: loss = 1.3062 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:29.982098 140367827080960 learning.py:507] global step 17160: loss = 1.4566 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:32.342344 140367827080960 learning.py:507] global step 17170: loss = 1.3559 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:34.736258 140367827080960 learning.py:507] global step 17180: loss = 1.3762 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:37.109229 140367827080960 learning.py:507] global step 17190: loss = 1.4265 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:39.489495 140367827080960 learning.py:507] global step 17200: loss = 1.2776 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:41.873687 140367827080960 learning.py:507] global step 17210: loss = 1.3407 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:44.221227 140367827080960 learning.py:507] global step 17220: loss = 1.3624 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:46.590590 140367827080960 learning.py:507] global step 17230: loss = 1.4341 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:48.969644 140367827080960 learning.py:507] global step 17240: loss = 1.3780 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:51.371331 140367827080960 learning.py:507] global step 17250: loss = 1.4678 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:53.751460 140367827080960 learning.py:507] global step 17260: loss = 1.3134 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:56.126563 140367827080960 learning.py:507] global step 17270: loss = 1.4288 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:14:58.494307 140367827080960 learning.py:507] global step 17280: loss = 1.2768 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:00.869401 140367827080960 learning.py:507] global step 17290: loss = 1.3393 (0.249 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:03.243009 140367827080960 learning.py:507] global step 17300: loss = 1.3549 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:05.622803 140367827080960 learning.py:507] global step 17310: loss = 1.3715 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:08.016535 140367827080960 learning.py:507] global step 17320: loss = 1.4547 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:10.394409 140367827080960 learning.py:507] global step 17330: loss = 1.3611 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:12.771469 140367827080960 learning.py:507] global step 17340: loss = 1.4346 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:15.129239 140367827080960 learning.py:507] global step 17350: loss = 1.4437 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:17.533514 140367827080960 learning.py:507] global step 17360: loss = 1.3844 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:19.892242 140367827080960 learning.py:507] global step 17370: loss = 1.3960 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:22.264561 140367827080960 learning.py:507] global step 17380: loss = 1.4191 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:24.640392 140367827080960 learning.py:507] global step 17390: loss = 1.3565 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:27.009592 140367827080960 learning.py:507] global step 17400: loss = 1.3320 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:29.403059 140367827080960 learning.py:507] global step 17410: loss = 1.3444 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:31.780289 140367827080960 learning.py:507] global step 17420: loss = 1.3544 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:34.165442 140367827080960 learning.py:507] global step 17430: loss = 1.4975 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:36.531569 140367827080960 learning.py:507] global step 17440: loss = 1.3784 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:38.902765 140367827080960 learning.py:507] global step 17450: loss = 1.3958 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:41.283940 140367827080960 learning.py:507] global step 17460: loss = 1.4347 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:43.677845 140367827080960 learning.py:507] global step 17470: loss = 1.4245 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:46.043860 140367827080960 learning.py:507] global step 17480: loss = 1.3404 (0.237 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 02:15:48.444488 140367827080960 learning.py:507] global step 17490: loss = 1.3064 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:50.827689 140367827080960 learning.py:507] global step 17500: loss = 1.4191 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:53.202471 140367827080960 learning.py:507] global step 17510: loss = 1.3215 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:55.583663 140367827080960 learning.py:507] global step 17520: loss = 1.5055 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:15:57.964616 140367827080960 learning.py:507] global step 17530: loss = 1.3058 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:00.342035 140367827080960 learning.py:507] global step 17540: loss = 1.3471 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:02.704543 140367827080960 learning.py:507] global step 17550: loss = 1.4605 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:05.057917 140367827080960 learning.py:507] global step 17560: loss = 1.3936 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:07.440421 140367827080960 learning.py:507] global step 17570: loss = 1.4057 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:08.552130 140361938757376 supervisor.py:1117] Saving checkpoint to path /opt/ml/model/model.ckpt\u001b[0m\n",
      "\u001b[32mI0701 02:16:09.984257 140361921971968 supervisor.py:1050] Recording summary at step 17576.\u001b[0m\n",
      "\u001b[32mI0701 02:16:10.874759 140367827080960 learning.py:507] global step 17580: loss = 1.4079 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:13.242161 140367827080960 learning.py:507] global step 17590: loss = 1.3170 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:15.606647 140367827080960 learning.py:507] global step 17600: loss = 1.4834 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:17.995333 140367827080960 learning.py:507] global step 17610: loss = 1.3725 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:20.345751 140367827080960 learning.py:507] global step 17620: loss = 1.2369 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:22.717302 140367827080960 learning.py:507] global step 17630: loss = 1.4441 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:25.070972 140367827080960 learning.py:507] global step 17640: loss = 1.4022 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:27.445188 140367827080960 learning.py:507] global step 17650: loss = 1.4414 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:29.818639 140367827080960 learning.py:507] global step 17660: loss = 1.4688 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:32.203728 140367827080960 learning.py:507] global step 17670: loss = 1.3586 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:34.569853 140367827080960 learning.py:507] global step 17680: loss = 1.3968 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:36.951266 140367827080960 learning.py:507] global step 17690: loss = 1.3851 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:39.328669 140367827080960 learning.py:507] global step 17700: loss = 1.3216 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:41.674566 140367827080960 learning.py:507] global step 17710: loss = 1.3103 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:44.057276 140367827080960 learning.py:507] global step 17720: loss = 1.3492 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:46.471319 140367827080960 learning.py:507] global step 17730: loss = 1.3842 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:48.841639 140367827080960 learning.py:507] global step 17740: loss = 1.3965 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:51.216727 140367827080960 learning.py:507] global step 17750: loss = 1.2647 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:53.582705 140367827080960 learning.py:507] global step 17760: loss = 1.3219 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:55.952709 140367827080960 learning.py:507] global step 17770: loss = 1.2833 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:16:58.286859 140367827080960 learning.py:507] global step 17780: loss = 1.3253 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:00.643057 140367827080960 learning.py:507] global step 17790: loss = 1.3941 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:03.017201 140367827080960 learning.py:507] global step 17800: loss = 1.3818 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:05.408212 140367827080960 learning.py:507] global step 17810: loss = 1.3114 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:07.799267 140367827080960 learning.py:507] global step 17820: loss = 1.4615 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:10.189259 140367827080960 learning.py:507] global step 17830: loss = 1.2812 (0.249 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:12.600843 140367827080960 learning.py:507] global step 17840: loss = 1.3503 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:14.960371 140367827080960 learning.py:507] global step 17850: loss = 1.4337 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:17.344693 140367827080960 learning.py:507] global step 17860: loss = 1.4453 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:19.726819 140367827080960 learning.py:507] global step 17870: loss = 1.3462 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:22.080919 140367827080960 learning.py:507] global step 17880: loss = 1.4323 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:24.436686 140367827080960 learning.py:507] global step 17890: loss = 1.3129 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:26.834593 140367827080960 learning.py:507] global step 17900: loss = 1.4074 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:29.218064 140367827080960 learning.py:507] global step 17910: loss = 1.3325 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:31.608572 140367827080960 learning.py:507] global step 17920: loss = 1.5360 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:33.979298 140367827080960 learning.py:507] global step 17930: loss = 1.3440 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:36.348192 140367827080960 learning.py:507] global step 17940: loss = 1.2999 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:38.713515 140367827080960 learning.py:507] global step 17950: loss = 1.3370 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:41.090904 140367827080960 learning.py:507] global step 17960: loss = 1.2573 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:43.474935 140367827080960 learning.py:507] global step 17970: loss = 1.4123 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:45.858298 140367827080960 learning.py:507] global step 17980: loss = 1.5829 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:48.236222 140367827080960 learning.py:507] global step 17990: loss = 1.4530 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:50.634562 140367827080960 learning.py:507] global step 18000: loss = 1.4271 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:53.004839 140367827080960 learning.py:507] global step 18010: loss = 1.3726 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:55.377902 140367827080960 learning.py:507] global step 18020: loss = 1.4544 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:17:57.736543 140367827080960 learning.py:507] global step 18030: loss = 1.4016 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:00.113923 140367827080960 learning.py:507] global step 18040: loss = 1.4376 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:02.521528 140367827080960 learning.py:507] global step 18050: loss = 1.3615 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:04.966018 140367827080960 learning.py:507] global step 18060: loss = 1.2688 (0.315 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:07.362555 140367827080960 learning.py:507] global step 18070: loss = 1.3273 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:09.742429 140367827080960 learning.py:507] global step 18080: loss = 1.3156 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:12.137769 140367827080960 learning.py:507] global step 18090: loss = 1.3377 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:14.508872 140367827080960 learning.py:507] global step 18100: loss = 1.4638 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:16.875080 140367827080960 learning.py:507] global step 18110: loss = 1.5681 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:19.246613 140367827080960 learning.py:507] global step 18120: loss = 1.3811 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:21.636044 140367827080960 learning.py:507] global step 18130: loss = 1.2750 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:24.000823 140367827080960 learning.py:507] global step 18140: loss = 1.3678 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:26.380103 140367827080960 learning.py:507] global step 18150: loss = 1.3934 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:28.768438 140367827080960 learning.py:507] global step 18160: loss = 1.5087 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:31.144052 140367827080960 learning.py:507] global step 18170: loss = 1.4217 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:33.516623 140367827080960 learning.py:507] global step 18180: loss = 1.5425 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:35.898593 140367827080960 learning.py:507] global step 18190: loss = 1.5717 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:38.252357 140367827080960 learning.py:507] global step 18200: loss = 1.5551 (0.235 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 02:18:40.605462 140367827080960 learning.py:507] global step 18210: loss = 1.4014 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:42.973313 140367827080960 learning.py:507] global step 18220: loss = 1.5102 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:45.358773 140367827080960 learning.py:507] global step 18230: loss = 1.3917 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:47.720493 140367827080960 learning.py:507] global step 18240: loss = 1.5399 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:50.089432 140367827080960 learning.py:507] global step 18250: loss = 1.3464 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:52.478522 140367827080960 learning.py:507] global step 18260: loss = 1.4350 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:54.842022 140367827080960 learning.py:507] global step 18270: loss = 1.4627 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:57.209100 140367827080960 learning.py:507] global step 18280: loss = 1.4399 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:18:59.546264 140367827080960 learning.py:507] global step 18290: loss = 1.3666 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:01.934062 140367827080960 learning.py:507] global step 18300: loss = 1.3709 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:04.297892 140367827080960 learning.py:507] global step 18310: loss = 1.4659 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:06.669374 140367827080960 learning.py:507] global step 18320: loss = 1.3228 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:09.051354 140367827080960 learning.py:507] global step 18330: loss = 1.5153 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:11.452837 140367827080960 learning.py:507] global step 18340: loss = 1.4324 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:13.808238 140367827080960 learning.py:507] global step 18350: loss = 1.2853 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:16.182952 140367827080960 learning.py:507] global step 18360: loss = 1.4060 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:18.566791 140367827080960 learning.py:507] global step 18370: loss = 1.3553 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:20.954298 140367827080960 learning.py:507] global step 18380: loss = 1.4076 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:23.315671 140367827080960 learning.py:507] global step 18390: loss = 1.4343 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:25.684864 140367827080960 learning.py:507] global step 18400: loss = 1.4016 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:28.050693 140367827080960 learning.py:507] global step 18410: loss = 1.4213 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:30.399198 140367827080960 learning.py:507] global step 18420: loss = 1.2851 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:32.768846 140367827080960 learning.py:507] global step 18430: loss = 1.3740 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:35.141860 140367827080960 learning.py:507] global step 18440: loss = 1.3704 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:37.539948 140367827080960 learning.py:507] global step 18450: loss = 1.3659 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:39.938889 140367827080960 learning.py:507] global step 18460: loss = 1.4137 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:42.318969 140367827080960 learning.py:507] global step 18470: loss = 1.4227 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:44.690573 140367827080960 learning.py:507] global step 18480: loss = 1.3606 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:47.052201 140367827080960 learning.py:507] global step 18490: loss = 1.3871 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:49.434300 140367827080960 learning.py:507] global step 18500: loss = 1.4018 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:51.799634 140367827080960 learning.py:507] global step 18510: loss = 1.3236 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:54.187098 140367827080960 learning.py:507] global step 18520: loss = 1.3409 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:56.662132 140367827080960 learning.py:507] global step 18530: loss = 1.4029 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:19:59.033507 140367827080960 learning.py:507] global step 18540: loss = 1.2785 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:01.428040 140367827080960 learning.py:507] global step 18550: loss = 1.3124 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:03.797200 140367827080960 learning.py:507] global step 18560: loss = 1.4891 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:06.158917 140367827080960 learning.py:507] global step 18570: loss = 1.2714 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:08.548616 140367827080960 learning.py:507] global step 18580: loss = 1.3997 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:10.903808 140367827080960 learning.py:507] global step 18590: loss = 1.3211 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:13.283497 140367827080960 learning.py:507] global step 18600: loss = 1.4104 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:15.648227 140367827080960 learning.py:507] global step 18610: loss = 1.3123 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:18.022629 140367827080960 learning.py:507] global step 18620: loss = 1.3611 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:20.352413 140367827080960 learning.py:507] global step 18630: loss = 1.3646 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:22.727871 140367827080960 learning.py:507] global step 18640: loss = 1.4417 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:25.101694 140367827080960 learning.py:507] global step 18650: loss = 1.4137 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:27.482484 140367827080960 learning.py:507] global step 18660: loss = 1.3658 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:29.842483 140367827080960 learning.py:507] global step 18670: loss = 1.4477 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:32.184062 140367827080960 learning.py:507] global step 18680: loss = 1.3160 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:34.551249 140367827080960 learning.py:507] global step 18690: loss = 1.5072 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:36.930052 140367827080960 learning.py:507] global step 18700: loss = 1.3569 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:39.283842 140367827080960 learning.py:507] global step 18710: loss = 1.3806 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:41.632649 140367827080960 learning.py:507] global step 18720: loss = 1.3747 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:44.000695 140367827080960 learning.py:507] global step 18730: loss = 1.3508 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:46.366226 140367827080960 learning.py:507] global step 18740: loss = 1.4421 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:48.732924 140367827080960 learning.py:507] global step 18750: loss = 1.3603 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:51.126388 140367827080960 learning.py:507] global step 18760: loss = 1.2468 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:53.506099 140367827080960 learning.py:507] global step 18770: loss = 1.3554 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:55.844475 140367827080960 learning.py:507] global step 18780: loss = 1.4704 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:20:58.240927 140367827080960 learning.py:507] global step 18790: loss = 1.4063 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:00.632657 140367827080960 learning.py:507] global step 18800: loss = 1.4319 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:03.002250 140367827080960 learning.py:507] global step 18810: loss = 1.2901 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:05.344427 140367827080960 learning.py:507] global step 18820: loss = 1.3645 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:07.726979 140367827080960 learning.py:507] global step 18830: loss = 1.4426 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:09.959826 140361921971968 supervisor.py:1050] Recording summary at step 18835.\u001b[0m\n",
      "\u001b[32mI0701 02:21:11.015362 140367827080960 learning.py:507] global step 18840: loss = 1.5050 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:13.351058 140367827080960 learning.py:507] global step 18850: loss = 1.3016 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:15.724251 140367827080960 learning.py:507] global step 18860: loss = 1.3082 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:18.087095 140367827080960 learning.py:507] global step 18870: loss = 1.3317 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:20.466902 140367827080960 learning.py:507] global step 18880: loss = 1.3586 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:22.854255 140367827080960 learning.py:507] global step 18890: loss = 1.3190 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:25.252692 140367827080960 learning.py:507] global step 18900: loss = 1.3347 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:27.642947 140367827080960 learning.py:507] global step 18910: loss = 1.2386 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:30.022586 140367827080960 learning.py:507] global step 18920: loss = 1.4086 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:32.376046 140367827080960 learning.py:507] global step 18930: loss = 1.2933 (0.237 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 02:21:34.740288 140367827080960 learning.py:507] global step 18940: loss = 1.3953 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:37.100079 140367827080960 learning.py:507] global step 18950: loss = 1.3616 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:39.471885 140367827080960 learning.py:507] global step 18960: loss = 1.3162 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:41.857556 140367827080960 learning.py:507] global step 18970: loss = 1.3730 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:44.228373 140367827080960 learning.py:507] global step 18980: loss = 1.4940 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:46.584085 140367827080960 learning.py:507] global step 18990: loss = 1.4324 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:48.991602 140367827080960 learning.py:507] global step 19000: loss = 1.3253 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:51.375726 140367827080960 learning.py:507] global step 19010: loss = 1.3836 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:53.751705 140367827080960 learning.py:507] global step 19020: loss = 1.2649 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:56.086127 140367827080960 learning.py:507] global step 19030: loss = 1.3717 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:21:58.432097 140367827080960 learning.py:507] global step 19040: loss = 1.4171 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:00.813582 140367827080960 learning.py:507] global step 19050: loss = 1.3712 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:03.175829 140367827080960 learning.py:507] global step 19060: loss = 1.3544 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:05.536319 140367827080960 learning.py:507] global step 19070: loss = 1.3423 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:07.912673 140367827080960 learning.py:507] global step 19080: loss = 1.3327 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:10.274142 140367827080960 learning.py:507] global step 19090: loss = 1.4553 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:12.650914 140367827080960 learning.py:507] global step 19100: loss = 1.4137 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:15.010539 140367827080960 learning.py:507] global step 19110: loss = 1.4485 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:17.379770 140367827080960 learning.py:507] global step 19120: loss = 1.2766 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:19.758337 140367827080960 learning.py:507] global step 19130: loss = 1.5465 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:22.167712 140367827080960 learning.py:507] global step 19140: loss = 1.3294 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:24.523051 140367827080960 learning.py:507] global step 19150: loss = 1.4098 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:26.907001 140367827080960 learning.py:507] global step 19160: loss = 1.4502 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:29.286520 140367827080960 learning.py:507] global step 19170: loss = 1.3653 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:31.650963 140367827080960 learning.py:507] global step 19180: loss = 1.4521 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:34.042651 140367827080960 learning.py:507] global step 19190: loss = 1.3973 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:36.392420 140367827080960 learning.py:507] global step 19200: loss = 1.4225 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:38.766125 140367827080960 learning.py:507] global step 19210: loss = 1.4792 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:41.127501 140367827080960 learning.py:507] global step 19220: loss = 1.4247 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:43.491184 140367827080960 learning.py:507] global step 19230: loss = 1.4979 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:45.859543 140367827080960 learning.py:507] global step 19240: loss = 1.5101 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:48.228401 140367827080960 learning.py:507] global step 19250: loss = 1.4557 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:50.576634 140367827080960 learning.py:507] global step 19260: loss = 1.3902 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:52.926246 140367827080960 learning.py:507] global step 19270: loss = 1.3876 (0.223 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:55.316020 140367827080960 learning.py:507] global step 19280: loss = 1.3613 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:22:57.679481 140367827080960 learning.py:507] global step 19290: loss = 1.3277 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:00.035200 140367827080960 learning.py:507] global step 19300: loss = 1.3651 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:02.428272 140367827080960 learning.py:507] global step 19310: loss = 1.3981 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:04.787568 140367827080960 learning.py:507] global step 19320: loss = 1.3725 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:07.155349 140367827080960 learning.py:507] global step 19330: loss = 1.3876 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:09.526361 140367827080960 learning.py:507] global step 19340: loss = 1.5143 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:11.909701 140367827080960 learning.py:507] global step 19350: loss = 1.3728 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:14.290956 140367827080960 learning.py:507] global step 19360: loss = 1.4435 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:16.657278 140367827080960 learning.py:507] global step 19370: loss = 1.3328 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:19.042844 140367827080960 learning.py:507] global step 19380: loss = 1.3385 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:21.413086 140367827080960 learning.py:507] global step 19390: loss = 1.3129 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:23.789340 140367827080960 learning.py:507] global step 19400: loss = 1.4348 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:26.137264 140367827080960 learning.py:507] global step 19410: loss = 1.4251 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:28.480776 140367827080960 learning.py:507] global step 19420: loss = 1.2996 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:30.849131 140367827080960 learning.py:507] global step 19430: loss = 1.3298 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:33.202455 140367827080960 learning.py:507] global step 19440: loss = 1.3554 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:35.572185 140367827080960 learning.py:507] global step 19450: loss = 1.3566 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:37.917526 140367827080960 learning.py:507] global step 19460: loss = 1.2529 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:40.291265 140367827080960 learning.py:507] global step 19470: loss = 1.2369 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:42.665852 140367827080960 learning.py:507] global step 19480: loss = 1.4502 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:45.019738 140367827080960 learning.py:507] global step 19490: loss = 1.3051 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:47.374974 140367827080960 learning.py:507] global step 19500: loss = 1.4550 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:49.741365 140367827080960 learning.py:507] global step 19510: loss = 1.3839 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:52.106839 140367827080960 learning.py:507] global step 19520: loss = 1.4506 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:54.471384 140367827080960 learning.py:507] global step 19530: loss = 1.3211 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:56.840245 140367827080960 learning.py:507] global step 19540: loss = 1.3806 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:23:59.234324 140367827080960 learning.py:507] global step 19550: loss = 1.3803 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:01.579138 140367827080960 learning.py:507] global step 19560: loss = 1.4727 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:03.949687 140367827080960 learning.py:507] global step 19570: loss = 1.4313 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:06.308165 140367827080960 learning.py:507] global step 19580: loss = 1.4673 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:08.681006 140367827080960 learning.py:507] global step 19590: loss = 1.3955 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:11.043579 140367827080960 learning.py:507] global step 19600: loss = 1.3840 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:13.419409 140367827080960 learning.py:507] global step 19610: loss = 1.4776 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:15.786567 140367827080960 learning.py:507] global step 19620: loss = 1.4114 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:18.137933 140367827080960 learning.py:507] global step 19630: loss = 1.3692 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:20.503968 140367827080960 learning.py:507] global step 19640: loss = 1.3567 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:22.912221 140367827080960 learning.py:507] global step 19650: loss = 1.3628 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:25.277585 140367827080960 learning.py:507] global step 19660: loss = 1.3125 (0.227 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 02:24:27.645294 140367827080960 learning.py:507] global step 19670: loss = 1.3623 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:30.011737 140367827080960 learning.py:507] global step 19680: loss = 1.3835 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:32.386437 140367827080960 learning.py:507] global step 19690: loss = 1.4117 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:34.791805 140367827080960 learning.py:507] global step 19700: loss = 1.3482 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:37.130825 140367827080960 learning.py:507] global step 19710: loss = 1.4700 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:39.507546 140367827080960 learning.py:507] global step 19720: loss = 1.2743 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:41.871457 140367827080960 learning.py:507] global step 19730: loss = 1.5025 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:44.235464 140367827080960 learning.py:507] global step 19740: loss = 1.2886 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:46.605443 140367827080960 learning.py:507] global step 19750: loss = 1.3969 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:48.960578 140367827080960 learning.py:507] global step 19760: loss = 1.3709 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:51.334909 140367827080960 learning.py:507] global step 19770: loss = 1.3920 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:53.755475 140367827080960 learning.py:507] global step 19780: loss = 1.3530 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:56.146284 140367827080960 learning.py:507] global step 19790: loss = 1.3344 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:24:58.517527 140367827080960 learning.py:507] global step 19800: loss = 1.2959 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:00.875000 140367827080960 learning.py:507] global step 19810: loss = 1.4094 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:03.218930 140367827080960 learning.py:507] global step 19820: loss = 1.3797 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:05.546654 140367827080960 learning.py:507] global step 19830: loss = 1.3261 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:07.935604 140367827080960 learning.py:507] global step 19840: loss = 1.3979 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:10.325922 140367827080960 learning.py:507] global step 19850: loss = 1.3969 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:12.703826 140367827080960 learning.py:507] global step 19860: loss = 1.3463 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:15.078382 140367827080960 learning.py:507] global step 19870: loss = 1.3733 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:17.482184 140367827080960 learning.py:507] global step 19880: loss = 1.3672 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:19.872277 140367827080960 learning.py:507] global step 19890: loss = 1.3843 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:22.223798 140367827080960 learning.py:507] global step 19900: loss = 1.3167 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:24.567163 140367827080960 learning.py:507] global step 19910: loss = 1.3824 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:26.908180 140367827080960 learning.py:507] global step 19920: loss = 1.3808 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:29.301659 140367827080960 learning.py:507] global step 19930: loss = 1.3496 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:31.682960 140367827080960 learning.py:507] global step 19940: loss = 1.3245 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:34.092752 140367827080960 learning.py:507] global step 19950: loss = 1.4132 (0.248 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:36.486727 140367827080960 learning.py:507] global step 19960: loss = 1.3607 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:38.881645 140367827080960 learning.py:507] global step 19970: loss = 1.3722 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:41.245376 140367827080960 learning.py:507] global step 19980: loss = 1.3798 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:43.638063 140367827080960 learning.py:507] global step 19990: loss = 1.3615 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:45.985888 140367827080960 learning.py:507] global step 20000: loss = 1.2969 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:48.386641 140367827080960 learning.py:507] global step 20010: loss = 1.4316 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:50.778435 140367827080960 learning.py:507] global step 20020: loss = 1.3499 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:53.123871 140367827080960 learning.py:507] global step 20030: loss = 1.3791 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:55.498644 140367827080960 learning.py:507] global step 20040: loss = 1.2933 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:25:57.862588 140367827080960 learning.py:507] global step 20050: loss = 1.4303 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:00.182204 140367827080960 learning.py:507] global step 20060: loss = 1.4806 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:02.543335 140367827080960 learning.py:507] global step 20070: loss = 1.3024 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:04.921144 140367827080960 learning.py:507] global step 20080: loss = 1.4558 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:07.305303 140367827080960 learning.py:507] global step 20090: loss = 1.3805 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:08.551992 140361938757376 supervisor.py:1117] Saving checkpoint to path /opt/ml/model/model.ckpt\u001b[0m\n",
      "\u001b[32mI0701 02:26:10.099716 140361921971968 supervisor.py:1050] Recording summary at step 20096.\u001b[0m\n",
      "\u001b[32mI0701 02:26:10.988254 140367827080960 learning.py:507] global step 20100: loss = 1.4411 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:13.345293 140367827080960 learning.py:507] global step 20110: loss = 1.4963 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:15.704996 140367827080960 learning.py:507] global step 20120: loss = 1.2980 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:18.077033 140367827080960 learning.py:507] global step 20130: loss = 1.3323 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:20.466108 140367827080960 learning.py:507] global step 20140: loss = 1.5043 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:22.844671 140367827080960 learning.py:507] global step 20150: loss = 1.2375 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:25.215581 140367827080960 learning.py:507] global step 20160: loss = 1.4053 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:27.581207 140367827080960 learning.py:507] global step 20170: loss = 1.3430 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:29.981611 140367827080960 learning.py:507] global step 20180: loss = 1.3067 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:32.364350 140367827080960 learning.py:507] global step 20190: loss = 1.4662 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:34.710201 140367827080960 learning.py:507] global step 20200: loss = 1.4485 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:37.077951 140367827080960 learning.py:507] global step 20210: loss = 1.3579 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:39.442651 140367827080960 learning.py:507] global step 20220: loss = 1.4060 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:41.805483 140367827080960 learning.py:507] global step 20230: loss = 1.4452 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:44.159804 140367827080960 learning.py:507] global step 20240: loss = 1.3910 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:46.539477 140367827080960 learning.py:507] global step 20250: loss = 1.2865 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:48.918616 140367827080960 learning.py:507] global step 20260: loss = 1.4335 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:51.299215 140367827080960 learning.py:507] global step 20270: loss = 1.4215 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:53.654272 140367827080960 learning.py:507] global step 20280: loss = 1.4338 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:56.026091 140367827080960 learning.py:507] global step 20290: loss = 1.3928 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:26:58.398936 140367827080960 learning.py:507] global step 20300: loss = 1.3467 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:00.811364 140367827080960 learning.py:507] global step 20310: loss = 1.4114 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:03.177360 140367827080960 learning.py:507] global step 20320: loss = 1.4248 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:05.547372 140367827080960 learning.py:507] global step 20330: loss = 1.4652 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:07.898766 140367827080960 learning.py:507] global step 20340: loss = 1.4467 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:10.256901 140367827080960 learning.py:507] global step 20350: loss = 1.4344 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:12.617585 140367827080960 learning.py:507] global step 20360: loss = 1.4181 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:15.002382 140367827080960 learning.py:507] global step 20370: loss = 1.3818 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:17.372359 140367827080960 learning.py:507] global step 20380: loss = 1.4685 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:19.755377 140367827080960 learning.py:507] global step 20390: loss = 1.3090 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:22.104562 140367827080960 learning.py:507] global step 20400: loss = 1.3997 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:24.483386 140367827080960 learning.py:507] global step 20410: loss = 1.4219 (0.235 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 02:27:26.873224 140367827080960 learning.py:507] global step 20420: loss = 1.3700 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:29.234690 140367827080960 learning.py:507] global step 20430: loss = 1.3448 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:31.580889 140367827080960 learning.py:507] global step 20440: loss = 1.2905 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:33.946213 140367827080960 learning.py:507] global step 20450: loss = 1.4120 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:36.332820 140367827080960 learning.py:507] global step 20460: loss = 1.3789 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:38.697267 140367827080960 learning.py:507] global step 20470: loss = 1.4220 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:41.070329 140367827080960 learning.py:507] global step 20480: loss = 1.3286 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:43.452883 140367827080960 learning.py:507] global step 20490: loss = 1.3944 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:45.790287 140367827080960 learning.py:507] global step 20500: loss = 1.3152 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:48.165354 140367827080960 learning.py:507] global step 20510: loss = 1.2684 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:50.522711 140367827080960 learning.py:507] global step 20520: loss = 1.5006 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:52.911729 140367827080960 learning.py:507] global step 20530: loss = 1.6280 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:55.288902 140367827080960 learning.py:507] global step 20540: loss = 1.4633 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:27:57.671369 140367827080960 learning.py:507] global step 20550: loss = 1.2471 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:00.074325 140367827080960 learning.py:507] global step 20560: loss = 1.4806 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:02.432208 140367827080960 learning.py:507] global step 20570: loss = 1.3162 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:04.818850 140367827080960 learning.py:507] global step 20580: loss = 1.4812 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:07.194358 140367827080960 learning.py:507] global step 20590: loss = 1.2493 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:09.571321 140367827080960 learning.py:507] global step 20600: loss = 1.4072 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:11.933953 140367827080960 learning.py:507] global step 20610: loss = 1.3951 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:14.283562 140367827080960 learning.py:507] global step 20620: loss = 1.3208 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:16.712620 140367827080960 learning.py:507] global step 20630: loss = 1.2551 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:19.090290 140367827080960 learning.py:507] global step 20640: loss = 1.4051 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:21.475161 140367827080960 learning.py:507] global step 20650: loss = 1.3646 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:23.854937 140367827080960 learning.py:507] global step 20660: loss = 1.3506 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:26.241697 140367827080960 learning.py:507] global step 20670: loss = 1.4556 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:28.586404 140367827080960 learning.py:507] global step 20680: loss = 1.4926 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:30.949831 140367827080960 learning.py:507] global step 20690: loss = 1.5545 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:33.339423 140367827080960 learning.py:507] global step 20700: loss = 1.3258 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:35.681289 140367827080960 learning.py:507] global step 20710: loss = 1.3484 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:38.038041 140367827080960 learning.py:507] global step 20720: loss = 1.5065 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:40.444631 140367827080960 learning.py:507] global step 20730: loss = 1.3193 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:42.830984 140367827080960 learning.py:507] global step 20740: loss = 1.3364 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:45.182105 140367827080960 learning.py:507] global step 20750: loss = 1.3452 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:47.554954 140367827080960 learning.py:507] global step 20760: loss = 1.3702 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:49.937048 140367827080960 learning.py:507] global step 20770: loss = 1.2611 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:52.307307 140367827080960 learning.py:507] global step 20780: loss = 1.4291 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:54.676436 140367827080960 learning.py:507] global step 20790: loss = 1.4169 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:57.052005 140367827080960 learning.py:507] global step 20800: loss = 1.3464 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:28:59.482374 140367827080960 learning.py:507] global step 20810: loss = 1.3707 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:01.857476 140367827080960 learning.py:507] global step 20820: loss = 1.3331 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:04.247489 140367827080960 learning.py:507] global step 20830: loss = 1.2992 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:06.630842 140367827080960 learning.py:507] global step 20840: loss = 1.3988 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:08.990025 140367827080960 learning.py:507] global step 20850: loss = 1.2956 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:11.356817 140367827080960 learning.py:507] global step 20860: loss = 1.3274 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:13.750503 140367827080960 learning.py:507] global step 20870: loss = 1.3228 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:16.094784 140367827080960 learning.py:507] global step 20880: loss = 1.3991 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:18.472337 140367827080960 learning.py:507] global step 20890: loss = 1.3951 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:20.859395 140367827080960 learning.py:507] global step 20900: loss = 1.3475 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:23.244532 140367827080960 learning.py:507] global step 20910: loss = 1.4275 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:25.633414 140367827080960 learning.py:507] global step 20920: loss = 1.4131 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:28.015664 140367827080960 learning.py:507] global step 20930: loss = 1.2902 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:30.367959 140367827080960 learning.py:507] global step 20940: loss = 1.3155 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:32.745754 140367827080960 learning.py:507] global step 20950: loss = 1.3581 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:35.114547 140367827080960 learning.py:507] global step 20960: loss = 1.3966 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:37.468935 140367827080960 learning.py:507] global step 20970: loss = 1.5238 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:39.809883 140367827080960 learning.py:507] global step 20980: loss = 1.5059 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:42.164223 140367827080960 learning.py:507] global step 20990: loss = 1.4923 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:44.529850 140367827080960 learning.py:507] global step 21000: loss = 1.5328 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:46.886039 140367827080960 learning.py:507] global step 21010: loss = 1.5267 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:49.268256 140367827080960 learning.py:507] global step 21020: loss = 1.3981 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:51.654740 140367827080960 learning.py:507] global step 21030: loss = 1.3461 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:54.035799 140367827080960 learning.py:507] global step 21040: loss = 1.4758 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:56.408462 140367827080960 learning.py:507] global step 21050: loss = 1.3876 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:29:58.782253 140367827080960 learning.py:507] global step 21060: loss = 1.3744 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:01.130398 140367827080960 learning.py:507] global step 21070: loss = 1.3287 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:03.478692 140367827080960 learning.py:507] global step 21080: loss = 1.4178 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:05.850879 140367827080960 learning.py:507] global step 21090: loss = 1.3553 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:08.229501 140367827080960 learning.py:507] global step 21100: loss = 1.4550 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:10.605567 140367827080960 learning.py:507] global step 21110: loss = 1.4736 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:12.985436 140367827080960 learning.py:507] global step 21120: loss = 1.4093 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:15.392560 140367827080960 learning.py:507] global step 21130: loss = 1.4499 (0.236 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 02:30:17.773850 140367827080960 learning.py:507] global step 21140: loss = 1.3283 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:20.163125 140367827080960 learning.py:507] global step 21150: loss = 1.3202 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:22.525923 140367827080960 learning.py:507] global step 21160: loss = 1.4647 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:24.899550 140367827080960 learning.py:507] global step 21170: loss = 1.4367 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:27.293216 140367827080960 learning.py:507] global step 21180: loss = 1.2506 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:29.654544 140367827080960 learning.py:507] global step 21190: loss = 1.3215 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:32.031584 140367827080960 learning.py:507] global step 21200: loss = 1.4365 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:34.392072 140367827080960 learning.py:507] global step 21210: loss = 1.3341 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:36.769926 140367827080960 learning.py:507] global step 21220: loss = 1.4004 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:39.154510 140367827080960 learning.py:507] global step 21230: loss = 1.4362 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:41.520524 140367827080960 learning.py:507] global step 21240: loss = 1.3213 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:43.905424 140367827080960 learning.py:507] global step 21250: loss = 1.3440 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:46.267709 140367827080960 learning.py:507] global step 21260: loss = 1.5981 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:48.646560 140367827080960 learning.py:507] global step 21270: loss = 1.3997 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:51.011406 140367827080960 learning.py:507] global step 21280: loss = 1.2844 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:53.373030 140367827080960 learning.py:507] global step 21290: loss = 1.4105 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:55.735802 140367827080960 learning.py:507] global step 21300: loss = 1.4157 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:30:58.123290 140367827080960 learning.py:507] global step 21310: loss = 1.3830 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:00.486866 140367827080960 learning.py:507] global step 21320: loss = 1.3909 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:02.868865 140367827080960 learning.py:507] global step 21330: loss = 1.3940 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:05.216460 140367827080960 learning.py:507] global step 21340: loss = 1.4306 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:07.575059 140367827080960 learning.py:507] global step 21350: loss = 1.2947 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:10.232703 140361921971968 supervisor.py:1050] Recording summary at step 21357.\u001b[0m\n",
      "\u001b[32mI0701 02:31:10.906212 140367827080960 learning.py:507] global step 21360: loss = 1.3573 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:13.291757 140367827080960 learning.py:507] global step 21370: loss = 1.2886 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:15.674455 140367827080960 learning.py:507] global step 21380: loss = 1.3976 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:18.042520 140367827080960 learning.py:507] global step 21390: loss = 1.2833 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:20.428637 140367827080960 learning.py:507] global step 21400: loss = 1.3637 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:22.813105 140367827080960 learning.py:507] global step 21410: loss = 1.4503 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:25.183087 140367827080960 learning.py:507] global step 21420: loss = 1.3992 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:27.549316 140367827080960 learning.py:507] global step 21430: loss = 1.3318 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:29.944567 140367827080960 learning.py:507] global step 21440: loss = 1.4455 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:32.292927 140367827080960 learning.py:507] global step 21450: loss = 1.4073 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:34.685803 140367827080960 learning.py:507] global step 21460: loss = 1.4243 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:37.050343 140367827080960 learning.py:507] global step 21470: loss = 1.3269 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:39.399492 140367827080960 learning.py:507] global step 21480: loss = 1.4021 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:41.787206 140367827080960 learning.py:507] global step 21490: loss = 1.4784 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:44.169934 140367827080960 learning.py:507] global step 21500: loss = 1.4666 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:46.626262 140367827080960 learning.py:507] global step 21510: loss = 1.4057 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:48.990868 140367827080960 learning.py:507] global step 21520: loss = 1.5221 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:51.344327 140367827080960 learning.py:507] global step 21530: loss = 1.4003 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:53.734373 140367827080960 learning.py:507] global step 21540: loss = 1.3123 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:56.104160 140367827080960 learning.py:507] global step 21550: loss = 1.3368 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:31:58.489396 140367827080960 learning.py:507] global step 21560: loss = 1.3358 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:00.821274 140367827080960 learning.py:507] global step 21570: loss = 1.3891 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:03.193872 140367827080960 learning.py:507] global step 21580: loss = 1.4333 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:05.539323 140367827080960 learning.py:507] global step 21590: loss = 1.3496 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:07.900923 140367827080960 learning.py:507] global step 21600: loss = 1.4122 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:10.270884 140367827080960 learning.py:507] global step 21610: loss = 1.3847 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:12.644426 140367827080960 learning.py:507] global step 21620: loss = 1.3529 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:15.029546 140367827080960 learning.py:507] global step 21630: loss = 1.5089 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:17.388809 140367827080960 learning.py:507] global step 21640: loss = 1.4442 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:19.751497 140367827080960 learning.py:507] global step 21650: loss = 1.3184 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:22.111713 140367827080960 learning.py:507] global step 21660: loss = 1.4105 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:24.485577 140367827080960 learning.py:507] global step 21670: loss = 1.3773 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:26.875309 140367827080960 learning.py:507] global step 21680: loss = 1.2776 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:29.254667 140367827080960 learning.py:507] global step 21690: loss = 1.4263 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:31.649036 140367827080960 learning.py:507] global step 21700: loss = 1.3758 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:34.023159 140367827080960 learning.py:507] global step 21710: loss = 1.4720 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:36.377145 140367827080960 learning.py:507] global step 21720: loss = 1.4245 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:38.730606 140367827080960 learning.py:507] global step 21730: loss = 1.3148 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:41.104337 140367827080960 learning.py:507] global step 21740: loss = 1.3893 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:43.468291 140367827080960 learning.py:507] global step 21750: loss = 1.3658 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:45.848803 140367827080960 learning.py:507] global step 21760: loss = 1.4599 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:48.226826 140367827080960 learning.py:507] global step 21770: loss = 1.3575 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:50.591289 140367827080960 learning.py:507] global step 21780: loss = 1.2972 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:52.940388 140367827080960 learning.py:507] global step 21790: loss = 1.4490 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:55.307972 140367827080960 learning.py:507] global step 21800: loss = 1.3809 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:32:57.694308 140367827080960 learning.py:507] global step 21810: loss = 1.3888 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:00.042962 140367827080960 learning.py:507] global step 21820: loss = 1.3479 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:02.392586 140367827080960 learning.py:507] global step 21830: loss = 1.4206 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:04.757731 140367827080960 learning.py:507] global step 21840: loss = 1.4617 (0.243 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:07.110133 140367827080960 learning.py:507] global step 21850: loss = 1.5076 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:09.480704 140367827080960 learning.py:507] global step 21860: loss = 1.3942 (0.230 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 02:33:11.859224 140367827080960 learning.py:507] global step 21870: loss = 1.1890 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:14.225339 140367827080960 learning.py:507] global step 21880: loss = 1.3124 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:16.597301 140367827080960 learning.py:507] global step 21890: loss = 1.4429 (0.224 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:18.994446 140367827080960 learning.py:507] global step 21900: loss = 1.3799 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:21.354675 140367827080960 learning.py:507] global step 21910: loss = 1.3026 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:23.716437 140367827080960 learning.py:507] global step 21920: loss = 1.3373 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:26.091933 140367827080960 learning.py:507] global step 21930: loss = 1.4043 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:28.459032 140367827080960 learning.py:507] global step 21940: loss = 1.3192 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:30.864434 140367827080960 learning.py:507] global step 21950: loss = 1.2850 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:33.264488 140367827080960 learning.py:507] global step 21960: loss = 1.4240 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:35.639309 140367827080960 learning.py:507] global step 21970: loss = 1.4098 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:37.971682 140367827080960 learning.py:507] global step 21980: loss = 1.3688 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:40.331989 140367827080960 learning.py:507] global step 21990: loss = 1.3616 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:42.726908 140367827080960 learning.py:507] global step 22000: loss = 1.3437 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:45.089162 140367827080960 learning.py:507] global step 22010: loss = 1.3109 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:47.453488 140367827080960 learning.py:507] global step 22020: loss = 1.3814 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:49.832319 140367827080960 learning.py:507] global step 22030: loss = 1.4354 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:52.175950 140367827080960 learning.py:507] global step 22040: loss = 1.3780 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:54.575359 140367827080960 learning.py:507] global step 22050: loss = 1.5067 (0.248 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:56.930429 140367827080960 learning.py:507] global step 22060: loss = 1.3783 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:33:59.312381 140367827080960 learning.py:507] global step 22070: loss = 1.3568 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:01.688882 140367827080960 learning.py:507] global step 22080: loss = 1.2841 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:04.066124 140367827080960 learning.py:507] global step 22090: loss = 1.4430 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:06.438638 140367827080960 learning.py:507] global step 22100: loss = 1.4848 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:08.791733 140367827080960 learning.py:507] global step 22110: loss = 1.4933 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:11.148391 140367827080960 learning.py:507] global step 22120: loss = 1.4820 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:13.544451 140367827080960 learning.py:507] global step 22130: loss = 1.4582 (0.247 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:15.927710 140367827080960 learning.py:507] global step 22140: loss = 1.3384 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:18.306008 140367827080960 learning.py:507] global step 22150: loss = 1.3970 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:20.679371 140367827080960 learning.py:507] global step 22160: loss = 1.4019 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:23.045182 140367827080960 learning.py:507] global step 22170: loss = 1.2357 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:25.429732 140367827080960 learning.py:507] global step 22180: loss = 1.4127 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:27.784352 140367827080960 learning.py:507] global step 22190: loss = 1.5516 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:30.169815 140367827080960 learning.py:507] global step 22200: loss = 1.4402 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:32.549699 140367827080960 learning.py:507] global step 22210: loss = 1.4984 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:34.928894 140367827080960 learning.py:507] global step 22220: loss = 1.3958 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:37.273895 140367827080960 learning.py:507] global step 22230: loss = 1.4278 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:39.648389 140367827080960 learning.py:507] global step 22240: loss = 1.3691 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:42.013114 140367827080960 learning.py:507] global step 22250: loss = 1.2564 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:44.385576 140367827080960 learning.py:507] global step 22260: loss = 1.4256 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:46.738792 140367827080960 learning.py:507] global step 22270: loss = 1.4015 (0.242 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:49.120419 140367827080960 learning.py:507] global step 22280: loss = 1.3698 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:51.473970 140367827080960 learning.py:507] global step 22290: loss = 1.4350 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:53.836472 140367827080960 learning.py:507] global step 22300: loss = 1.4110 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:56.215202 140367827080960 learning.py:507] global step 22310: loss = 1.4612 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:34:58.613212 140367827080960 learning.py:507] global step 22320: loss = 1.4352 (0.234 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:00.989748 140367827080960 learning.py:507] global step 22330: loss = 1.3043 (0.228 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:03.364134 140367827080960 learning.py:507] global step 22340: loss = 1.4134 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:05.696407 140367827080960 learning.py:507] global step 22350: loss = 1.2513 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:08.045419 140367827080960 learning.py:507] global step 22360: loss = 1.3214 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:10.457840 140367827080960 learning.py:507] global step 22370: loss = 1.3384 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:12.836144 140367827080960 learning.py:507] global step 22380: loss = 1.4486 (0.244 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:15.173772 140367827080960 learning.py:507] global step 22390: loss = 1.2467 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:17.489201 140367827080960 learning.py:507] global step 22400: loss = 1.4539 (0.223 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:19.866718 140367827080960 learning.py:507] global step 22410: loss = 1.3957 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:22.279340 140367827080960 learning.py:507] global step 22420: loss = 1.3152 (0.245 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:24.638700 140367827080960 learning.py:507] global step 22430: loss = 1.2908 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:26.989008 140367827080960 learning.py:507] global step 22440: loss = 1.2460 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:29.369345 140367827080960 learning.py:507] global step 22450: loss = 1.3972 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:31.756015 140367827080960 learning.py:507] global step 22460: loss = 1.4375 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:34.091741 140367827080960 learning.py:507] global step 22470: loss = 1.2667 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:36.479793 140367827080960 learning.py:507] global step 22480: loss = 1.3045 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:38.839992 140367827080960 learning.py:507] global step 22490: loss = 1.4758 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:41.212922 140367827080960 learning.py:507] global step 22500: loss = 1.4203 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:43.607479 140367827080960 learning.py:507] global step 22510: loss = 1.3701 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:46.014537 140367827080960 learning.py:507] global step 22520: loss = 1.4636 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:48.398524 140367827080960 learning.py:507] global step 22530: loss = 1.3688 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:50.761416 140367827080960 learning.py:507] global step 22540: loss = 1.3705 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:53.121763 140367827080960 learning.py:507] global step 22550: loss = 1.5204 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:55.513329 140367827080960 learning.py:507] global step 22560: loss = 1.4743 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:35:57.900495 140367827080960 learning.py:507] global step 22570: loss = 1.4288 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:00.292610 140367827080960 learning.py:507] global step 22580: loss = 1.3879 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:02.656866 140367827080960 learning.py:507] global step 22590: loss = 1.4360 (0.234 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI0701 02:36:05.000130 140367827080960 learning.py:507] global step 22600: loss = 1.3120 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:07.372318 140367827080960 learning.py:507] global step 22610: loss = 1.3874 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:08.551965 140361938757376 supervisor.py:1117] Saving checkpoint to path /opt/ml/model/model.ckpt\u001b[0m\n",
      "\u001b[32mI0701 02:36:10.228773 140361921971968 supervisor.py:1050] Recording summary at step 22618.\u001b[0m\n",
      "\u001b[32mI0701 02:36:10.632455 140367827080960 learning.py:507] global step 22620: loss = 1.5265 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:13.026427 140367827080960 learning.py:507] global step 22630: loss = 1.4055 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:15.371654 140367827080960 learning.py:507] global step 22640: loss = 1.2377 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:17.743081 140367827080960 learning.py:507] global step 22650: loss = 1.4349 (0.232 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:20.116293 140367827080960 learning.py:507] global step 22660: loss = 1.5049 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:22.497136 140367827080960 learning.py:507] global step 22670: loss = 1.3917 (0.246 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:24.878855 140367827080960 learning.py:507] global step 22680: loss = 1.3614 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:27.231825 140367827080960 learning.py:507] global step 22690: loss = 1.5048 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:29.586587 140367827080960 learning.py:507] global step 22700: loss = 1.5079 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:31.930747 140367827080960 learning.py:507] global step 22710: loss = 1.4322 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:34.289741 140367827080960 learning.py:507] global step 22720: loss = 1.3618 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:36.677766 140367827080960 learning.py:507] global step 22730: loss = 1.4191 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:39.038674 140367827080960 learning.py:507] global step 22740: loss = 1.4040 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:41.417552 140367827080960 learning.py:507] global step 22750: loss = 1.4802 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:43.810564 140367827080960 learning.py:507] global step 22760: loss = 1.3987 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:46.177289 140367827080960 learning.py:507] global step 22770: loss = 1.3448 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:48.541445 140367827080960 learning.py:507] global step 22780: loss = 1.4098 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:50.887346 140367827080960 learning.py:507] global step 22790: loss = 1.3868 (0.229 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:53.259701 140367827080960 learning.py:507] global step 22800: loss = 1.5301 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:55.604054 140367827080960 learning.py:507] global step 22810: loss = 1.4379 (0.227 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:36:57.968291 140367827080960 learning.py:507] global step 22820: loss = 1.3800 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:00.304666 140367827080960 learning.py:507] global step 22830: loss = 1.3328 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:02.692920 140367827080960 learning.py:507] global step 22840: loss = 1.3595 (0.230 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:05.085897 140367827080960 learning.py:507] global step 22850: loss = 1.3278 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:07.444065 140367827080960 learning.py:507] global step 22860: loss = 1.4502 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:09.810600 140367827080960 learning.py:507] global step 22870: loss = 1.3399 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:12.183528 140367827080960 learning.py:507] global step 22880: loss = 1.4804 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:14.522392 140367827080960 learning.py:507] global step 22890: loss = 1.5072 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:16.881958 140367827080960 learning.py:507] global step 22900: loss = 1.4287 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:19.243860 140367827080960 learning.py:507] global step 22910: loss = 1.4112 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:21.616610 140367827080960 learning.py:507] global step 22920: loss = 1.4687 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:23.970572 140367827080960 learning.py:507] global step 22930: loss = 1.3386 (0.226 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:26.347351 140367827080960 learning.py:507] global step 22940: loss = 1.3032 (0.233 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:28.706470 140367827080960 learning.py:507] global step 22950: loss = 1.3742 (0.239 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:31.053874 140367827080960 learning.py:507] global step 22960: loss = 1.4090 (0.241 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:33.408200 140367827080960 learning.py:507] global step 22970: loss = 1.3515 (0.240 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:35.790646 140367827080960 learning.py:507] global step 22980: loss = 1.3518 (0.236 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:38.163188 140367827080960 learning.py:507] global step 22990: loss = 1.3819 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:40.559638 140367827080960 learning.py:507] global step 23000: loss = 1.2910 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:42.941224 140367827080960 learning.py:507] global step 23010: loss = 1.4588 (0.235 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:45.321752 140367827080960 learning.py:507] global step 23020: loss = 1.3460 (0.237 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:47.698085 140367827080960 learning.py:507] global step 23030: loss = 1.4372 (0.238 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:50.073945 140367827080960 learning.py:507] global step 23040: loss = 1.3471 (0.225 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:52.445178 140367827080960 learning.py:507] global step 23050: loss = 1.3660 (0.231 sec/step)\u001b[0m\n",
      "\u001b[32mI0701 02:37:54.840620 140367827080960 learning.py:507] global step 23060: loss = 1.3479 (0.239 sec/step)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sm_sess = sagemaker.Session()\n",
    "sm_sess.logs_for_job(estimator.latest_training_job.name, wait=True, log_type='All')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>학습이 모두 완료된 다음에 S3에서 모델 산출물을 SageMaker Notebook 환경으로 내려받습니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_dir = estimator.model_dir.replace('model','')\n",
    "print(artifacts_dir)\n",
    "!aws s3 ls --human-readable {artifacts_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir=artifacts_dir+'output/'\n",
    "print(model_dir)\n",
    "!aws s3 ls --human-readable {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./model_result/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json , os\n",
    "\n",
    "path = './model_result'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "!aws s3 cp {model_dir}model.tar.gz {path}/model.tar.gz\n",
    "!tar -xzf {path}/model.tar.gz -C {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>최종 결과물에는 tflite를 생성할 수 있도록 했습니다. 압축을 푼 다음 tflite 를 다시 활용하기 위해 S3에 파일을 upload 합니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = 's3://{}/{}'.format(bucket, 'workshop_final_result')\n",
    "\n",
    "!aws s3 cp ./img_datasets/labels.txt {final_result}/labels.txt\n",
    "!aws s3 cp {path}/mobilenetv1_model.tflite {final_result}/mobilenetv1_model.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p></p>\n",
    "<p>Amazon SageMaker에서 모든 학습을 완료하였습니다. 이제 tflite를 이용하여 AI Chip에서 활용할 수 있도록 Convertor를 수행합니다. 이 작업은 Cloud9에서 수행합니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
