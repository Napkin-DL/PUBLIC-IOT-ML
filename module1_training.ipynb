{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker 학습 스크립트\n",
    "\n",
    "<p>이 예제는 LG에서 개발한 AI chip에서 동작할 수 있도록, Tensorflow 1.X, python2.7 버전에서 학습하기 위한 코드입니다. </p>\n",
    "<p>이 코드는 <strong><a href=\"https://github.com/tensorflow/models/tree/master/research/slim\" target=\"_blank\" class ='btn-default'>TensorFlow-Slim image classification model library</a></strong>를 참고하여 SageMaker에서 학습할 수 있는 실행 스크립트로 수정하여 작성하였습니다. Amazon SageMaker로 실행 스크립트를 구성하는 이유는 노트북의 스크립트에서 일부 파라미터 수정으로 동일 모델 아키텍처에 대해 hyperparamter가 변경된 다양한 모델을 원하는 형태의 다수 인프라에서 동시에 학습 수행이 가능하며, 가장 높은 성능의 모델을 노트북 스크립트 내 명령어로 바로 hosting 서비스가 가능한 Endpoint 생성을 할 수 있습니다.</p>\n",
    "\n",
    "<p>이번 실습에서는 Amazon SageMaker가 어떤 방식으로 학습이 되는지 설명되는 구조와 함께 학습하는 방법을 간단하게 체험해 보는 시간을 갖도록 하겠습니다.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SageMaker notebook 설명\n",
    "<p>SageMaker notebook은 완전 관리형 서비스로 컨테이너 기반으로 구성되어 있습니다. 사용자가 직접 컨테이너를 볼 수 없지만, 내부적으로는 아래와 같은 원리로 동작합니다. </p>\n",
    "<p><img src=\"./fig/sm_notebook.png\" width=\"700\", height=\"70\"></p>\n",
    "\n",
    "- **S3 (Simple Storage Serivce)** : Object Storage로서 학습할 데이터 파일과 학습 결과인 model, checkpoint, tensorboard를 위한 event 파일, 로그 정보 등을 저장하는데 사용합니다.\n",
    "- **SageMaker Notebook** : 학습을 위한 스크립트 작성과 디버깅, 그리고 실제 학습을 수행하기 위한 Python을 개발하기 위한 환경을 제공합니다.\n",
    "- **Amazon Elastic Container Registry(ECR)** :  Docker 컨테이너 이미지를 손쉽게 저장, 관리 및 배포할 수 있게 해주는 완전관리형 Docker 컨테이너 레지스트리입니다. Sagemaker는 기본적인 컨테이너를 제공하기 때문에 별도 ECR에 컨테이너 이미지를 등록할 필요는 없습니다. 하지만, 별도의 학습 및 배포 환경이 필요한 경우 custom 컨테이너 이미지를 만들어서 ECR에 등록한 후 이 환경을 활용할 수 있습니다.\n",
    "\n",
    "<p>학습과 추론을 하는 hosting 서비스는 각각 다른 컨테이너 환경에서 수행할 수 있으며, 쉽게 다량으로 컨테이너 환경을 확장할 수 있으므로 다량의 학습과 hosting이 동시에 가능합니다.   \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 환경 설정\n",
    "\n",
    "<p>Sagemaker 학습에 필요한 기본적인 package를 import 합니다. </p>\n",
    "<p>boto3는 HTTP API 호출을 숨기는 편한 추상화 모델을 가지고 있고, Amazon EC2 인스턴스 및 S3 버켓과 같은 AWS 리소스와 동작하는 파이선 클래스를 제공합니다. </p>\n",
    "<p>sagemaker python sdk는 Amazon SageMaker에서 기계 학습 모델을 교육 및 배포하기 위한 오픈 소스 라이브러리입니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install --upgrade pip\n",
    "# !{sys.executable} -m pip install tensorflow_gpu==1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sagemaker\n",
    "import boto3\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>SageMaker에서 앞으로 사용할 SageMaker Session 설정, Role 정보를 설정합니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. S3의 저장 데이터 위치 가져오기\n",
    "<p> 데이터를 정하기 위한 S3의 bucket 위치는 아래 data_bucket 이름으로 생성하며, 기본적으로 SageMaker에서 학습한 모델과 로그 정보를 남기는 위치는 자동으로 생성되는 bucket 이름으로 저장됩니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a s3 bucket to hold data, note that your account might already created a bucket with the same name\n",
    "account_id = sess.client('sts').get_caller_identity()[\"Account\"]\n",
    "data_bucket = 'sagemaker-experiments-{}-{}'.format(sess.region_name, account_id)\n",
    "bucket = 'sagemaker-{}-{}'.format(sess.region_name, account_id)\n",
    "\n",
    "try:\n",
    "    if sess.region_name == \"us-east-1\":\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket)\n",
    "    else:\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket, \n",
    "                                        CreateBucketConfiguration={'LocationConstraint': sess.region_name})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 이미지를 TFRecord 변경하기\n",
    "<p>이미지 파일을 학습하기 위해 SageMaker Notebook 환경으로 upload를 합니다. 폴더 구조는 아래와 같은 형태로 구성되어야 합니다. </p>\n",
    "<pre>\n",
    "<div style='line-height:80%'>\n",
    "    image_path/class1/Aimage_1<br>\n",
    "                      Aimage_2<br>\n",
    "                       ...<br>\n",
    "                      Aimage_N<br>\n",
    "    image_path/class2/Bimage_1<br>\n",
    "                      Bimage_2<br>\n",
    "                       ...<br>\n",
    "                      Bimage_M<br>\n",
    "</div>\n",
    "</pre>\n",
    "<p>생성된 TFRecord 파일은 아래 정의하신 dataset_dir에 저장이 됩니다. train과 test의 데이터 수는 향후 학습에서 활용하기 위해 train_num_data, test_num_data 변수에 저장합니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/ec2-user/SageMaker/PUBLIC-IOT-ML/src_dir/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import image_to_tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/home/ec2-user/SageMaker/PUBLIC-IOT-ML/img_datasets'\n",
    "image_path = '/home/ec2-user/SageMaker/PUBLIC-IOT-ML/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/PUBLIC-IOT-ML/src_dir/datasets/image_to_tfrecord.py:158: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/PUBLIC-IOT-ML/src_dir/datasets/image_to_tfrecord.py:162: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/PUBLIC-IOT-ML/src_dir/datasets/dataset_utils.py:176: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/PUBLIC-IOT-ML/src_dir/datasets/image_to_tfrecord.py:195: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/PUBLIC-IOT-ML/src_dir/datasets/image_to_tfrecord.py:75: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/PUBLIC-IOT-ML/src_dir/datasets/image_to_tfrecord.py:78: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "\n",
      "Finished converting the image dataset!\n",
      "CPU times: user 22.9 s, sys: 1.05 s, total: 24 s\n",
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_num_data, test_num_data = image_to_tfrecord.run(image_path, dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TFRecord를 S3에 upload 하기\n",
    "\n",
    "<p>SageMaker 학습을 위해 TFRecord 파일을 S3에 upload합니다. TFRecord 은 이전에 지정한 data_bucket 내 prefix 하위 폴더에 저장됩니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: img_datasets/labels.txt to s3://sagemaker-experiments-us-east-1-143656149352/captured_data/tfrecord/labels.txt\n",
      "upload: img_datasets/captureddata_train.tfrecord to s3://sagemaker-experiments-us-east-1-143656149352/captured_data/tfrecord/captureddata_train.tfrecord\n",
      "upload: img_datasets/captureddata_val.tfrecord to s3://sagemaker-experiments-us-east-1-143656149352/captured_data/tfrecord/captureddata_val.tfrecord\n"
     ]
    }
   ],
   "source": [
    "prefix = 'captured_data/tfrecord'\n",
    "!aws s3 cp ./img_datasets s3://{data_bucket}/{prefix}/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 학습 스크립트 코딩하기\n",
    "\n",
    "<p>SageMaker에서 학습하는 것이 아니더라도 실제 모델 아키텍처와 학습을 위한 optimizer와 loss 함수 등을 정의하는 python 파일을 구성하게 됩니다. SageMaker에서 활용하는 python 파일도 동일한 python 파일을 사용하게 됩니다. 연계되는 다른 소스코드 파일이 있는 경우에도 별도 소스코드 수정 없이 학습이 가능하며, SageMaker에서 사용하기 위해서는 기존 python 파일에 SageMaker 학습에 사용할 수 있는 환경변수들만 추가하면 됩니다. 예를 들어, 환경변수 중 <code>SM_MODEL_DIR</code>은 컨테이너 환경에서는 <code>/opt/ml/model</code>를 의미합니다. 다양한 환경변수는 <strong><a href=\"https://github.com/aws/sagemaker-containers\" target=\"_blank\" class ='btn-default'>SageMaker Containers-IMPORTANT ENVIRONMENT VARIABLES</a></strong>를 참고하시기 바랍니다. </p><p>SageMaker 학습이 끝나면 자동은 컨테이너 환경은 삭제가 됩니다. 따라서, 학습이 완료된 모델 산출물과 다양한 output 파일은 S3로 저장해야 합니다. SageMaker는 자동으로 <code>SM_MODEL_DIR</code>에 저장된 최종 모델 파일을 학습이 끝난 다음 model.tar.gz로 압축하여 컨테이너 환경에서 S3의 특정 bucket에 저장하게 됩니다.</p><p> 별도 bucket을 설정하지 않으며, 기본적으로 생성되는 bucket에 저장됩니다. 이외 학습에 이용되는 python source code는 SageMaker 학습이 시작되면서 S3에 저장되며, 별도로 <code>SM_MODEL_DIR</code>에 checkpoint 또는 log 파일을 저장하게 되면 학습이 끝난 이후 자동으로 컨테이너 환경에서 S3로 저장된 파일들이 이동하게 됩니다. 이런 과정을 이해한다면 학습 시 저장되는 다양한 정보들을 저장한 다음 학습이 끝난 후 S3에서 download 받아 활용할 수 있습니다. </p>\n",
    "\n",
    "<p>아래는 시간 관계 상 미리 작성한 python 학습 스크립트 코드 입니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;49;00m\r\n",
      "\u001b[37m# you may not use this file except in compliance with the License.\u001b[39;49;00m\r\n",
      "\u001b[37m# You may obtain a copy of the License at\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# http://www.apache.org/licenses/LICENSE-2.0\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# Unless required by applicable law or agreed to in writing, software\u001b[39;49;00m\r\n",
      "\u001b[37m# distributed under the License is distributed on an \"AS IS\" BASIS,\u001b[39;49;00m\r\n",
      "\u001b[37m# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[39;49;00m\r\n",
      "\u001b[37m# See the License for the specific language governing permissions and\u001b[39;49;00m\r\n",
      "\u001b[37m# limitations under the License.\u001b[39;49;00m\r\n",
      "\u001b[37m# ==============================================================================\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"Generic training script that trains a model using a given dataset.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m absolute_import, division, print_function\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcodecs\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mPIL\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mfreeze_graph\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mfg\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m dataset_factory\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdeployment\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m model_deploy\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mnets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m nets_factory\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m preprocessing_factory\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.contrib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m quantize \u001b[34mas\u001b[39;49;00m contrib_quantize\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.contrib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m slim \u001b[34mas\u001b[39;49;00m contrib_slim\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.core.protobuf\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m saver_pb2\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.python.platform\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gfile\r\n",
      "\r\n",
      "slim = contrib_slim\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m###############################\u001b[39;49;00m\r\n",
      "    \u001b[37m# SageMaker Default Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m###############################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mDirectory where checkpoints and event logs are written to.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dataset_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m,\r\n",
      "                        default=json.loads(os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-config\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=json.loads,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_INPUT_DATA_CONFIG\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--fw-params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=json.loads,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_FRAMEWORK_PARAMS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "\r\n",
      "    \u001b[37m###############################\u001b[39;49;00m\r\n",
      "    \u001b[37m#  Default Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m###############################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--master\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe address of the TensorFlow master to use.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--warmup_epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mLinearly warmup learning rate from 0 to learning_rate over this many epochs.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_clones\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mNumber of model clones to deploy. Note For historical reasons\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mloss from all clones averaged out and learning rate decay happen per clone epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--clone_on_cpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mUse CPUs to deploy clones.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--worker_replicas\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m1\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mNumber of worker replicas.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_ps_tasks\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of parameter servers. If the value is 0,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mthen the parameters are handled locally by the worker.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--task\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mTask id of the replica running the training.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--save_interval_secs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m600\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe frequency with which the model is saved, in seconds.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--save_summaries_secs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m600\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe frequency with which summaries are saved, in seconds.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--log_every_n_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe frequency with which logs are print.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_preprocessing_threads\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of threads used to create the batches.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_readers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of parallel readers that read data from the dataset.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m##########################\u001b[39;49;00m\r\n",
      "    \u001b[37m# Optimization Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m##########################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--adam_beta1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe exponential decay rate for the 1st moment estimates.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--adam_beta2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.999\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe exponential decay rate for the 2nd moment estimates.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--adagrad_initial_accumulator_value\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.1\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mStarting value for the AdaGrad accumulators.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--adadelta_rho\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.95\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe decay rate for adadelta.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--optimizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mrmsprop\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe name of the optimizer, one of \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33madadelta\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33madagrad\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mftrl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mmomentum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33msgd\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m or \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mrmsprop\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--weight_decay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.00004\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe weight decay on the model weights.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--opt_epsilon\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m1.0\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mEpsilon term for the optimizer.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ftrl_learning_rate_power\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=-\u001b[34m0.5\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe learning rate power.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ftrl_initial_accumulator_value\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.1\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mStarting value for the FTRL accumulators.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ftrl_l1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe FTRL l1 regularization strength.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ftrl_l2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe FTRL l2 regularization strength.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe momentum for the MomentumOptimizer and RMSPropOptimizer.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--rmsprop_momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.9\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mMomentum.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--rmsprop_decay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.9\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mDecay term for RMSProp.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--quantize_delay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=-\u001b[34m1\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mNumber of steps to start quantized training.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mSet to -1 would disable quantized training.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m###########################\u001b[39;49;00m\r\n",
      "    \u001b[37m# Learning Rate Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m###########################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate_decay_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mexponential\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mSpecifies how the learning rate is decayed. One of \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mfixed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mexponential\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m or \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mpolynomial\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.01\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mInitial learning rate.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--end_learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.01\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mInitial learning rate.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--label_smoothing\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.0\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe amount of label smoothing.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate_decay_factor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.94\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mLearning rate decay factor.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_epochs_per_decay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m2.0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mNumber of epochs after which learning rate decays. \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mNote: this flag counts epochs per clone but aggregates per sync replicas.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mSo 1.0 means that each clone will go over full epoch individually, \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mbut replicas will go once across all replicas.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sync_replicas\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhether or not to synchronize the replicas during training.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--replicas_to_aggregate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m1\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe Number of gradients to collect before updating params.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--moving_average_decay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe decay to use for the moving average.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33m If left as None, then moving averages are not used.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m#####################\u001b[39;49;00m\r\n",
      "    \u001b[37m# Dataset Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m#####################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dataset_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[33m'\u001b[39;49;00m\u001b[33mimagenet\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe name of the dataset to load.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[37m# parser.add_argument('--dataset_split_name', type=str,\u001b[39;49;00m\r\n",
      "    \u001b[37m#                     default='train', help='The name of the train/test split.')\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--labels_offset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mAn offset for the labels in the dataset.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mThis flag is primarily used to evaluate the VGG and ResNet \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33marchitectures which do not use a background class for the ImageNet dataset.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[33m'\u001b[39;49;00m\u001b[33minception_v3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe name of the architecture to train.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--preprocessing_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe name of the preprocessing to use.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mIf left as `None`, then the model_name flag is used.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m32\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of samples in each batch.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--image_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mTrain image size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_number_of_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe maximum number of training steps.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_grayscale\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[36mFalse\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhether to convert input images to grayscale.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m#########################\u001b[39;49;00m\r\n",
      "    \u001b[37m# Fine-Tuning Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m#########################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--finetune_checkpoint_path\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe path to a checkpoint from which to fine-tune.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--checkpoint_exclude_scopes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of scopes of variables\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mto exclude when restoring from a checkpoint.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--trainable_scopes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of scopes to filter the set\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mof variables to train. By default, None would train all the variables.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ignore_missing_vars\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhen restoring a checkpoint would ignore missing variables.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m#########################\u001b[39;49;00m\r\n",
      "    \u001b[37m# evaluation Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m#########################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m100\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of samples in each batch in evaluation.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_num_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of train samples\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_num_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of test samples\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_eval_num_batches\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mMax number of batches to evaluate by default use all.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[37m# parser.add_argument('--num_preprocessing_threads', type=int,\u001b[39;49;00m\r\n",
      "    \u001b[37m#                     default=4, help='The number of threads used to create the batches.')\u001b[39;49;00m\r\n",
      "    \u001b[37m# parser.add_argument('--eval_image_size', type=int,\u001b[39;49;00m\r\n",
      "    \u001b[37m#                     default=None, help='Eval image size')\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--quantize\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mEval image size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--is_training\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhether to save out a training-focused version of the model.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--is_video_model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mwhether to use 5-D inputs for video model.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_frames\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of frames to use. Only used if is_video_model is True.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--write_text_graphdef\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhether to write a text version of graphdef.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    return_value = parser.parse_known_args()\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mparser.parse_known_args() : {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(return_value))\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m return_value\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_configure_learning_rate\u001b[39;49;00m(args, num_samples_per_epoch, global_step):\r\n",
      "    \u001b[33m\"\"\"Configures the learning rate.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m      num_samples_per_epoch: The number of samples in each epoch of training.\u001b[39;49;00m\r\n",
      "\u001b[33m      global_step: The global_step tensor.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m      A `Tensor` representing the learning rate.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Raises:\u001b[39;49;00m\r\n",
      "\u001b[33m      ValueError: if\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[37m# Note: when num_clones is > 1, this will actually have each clone to go\u001b[39;49;00m\r\n",
      "    \u001b[37m# over each epoch args.num_epochs_per_decay times. This is different\u001b[39;49;00m\r\n",
      "    \u001b[37m# behavior from sync replicas and is expected to produce different results.\u001b[39;49;00m\r\n",
      "    steps_per_epoch = num_samples_per_epoch / args.batch_size\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.sync_replicas:\r\n",
      "        steps_per_epoch /= args.replicas_to_aggregate\r\n",
      "\r\n",
      "    decay_steps = \u001b[36mint\u001b[39;49;00m(steps_per_epoch * args.num_epochs_per_decay)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.learning_rate_decay_type == \u001b[33m'\u001b[39;49;00m\u001b[33mexponential\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        learning_rate = tf.train.exponential_decay(\r\n",
      "            args.learning_rate,\r\n",
      "            global_step,\r\n",
      "            decay_steps,\r\n",
      "            args.learning_rate_decay_factor,\r\n",
      "            staircase=\u001b[36mTrue\u001b[39;49;00m,\r\n",
      "            name=\u001b[33m'\u001b[39;49;00m\u001b[33mexponential_decay_learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.learning_rate_decay_type == \u001b[33m'\u001b[39;49;00m\u001b[33mfixed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        learning_rate = tf.constant(\r\n",
      "            args.learning_rate, name=\u001b[33m'\u001b[39;49;00m\u001b[33mfixed_learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.learning_rate_decay_type == \u001b[33m'\u001b[39;49;00m\u001b[33mpolynomial\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        learning_rate = tf.train.polynomial_decay(\r\n",
      "            args.learning_rate,\r\n",
      "            global_step,\r\n",
      "            decay_steps,\r\n",
      "            args.end_learning_rate,\r\n",
      "            power=\u001b[34m1.0\u001b[39;49;00m,\r\n",
      "            cycle=\u001b[36mFalse\u001b[39;49;00m,\r\n",
      "            name=\u001b[33m'\u001b[39;49;00m\u001b[33mpolynomial_decay_learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate_decay_type [\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m] was not recognized\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m %\r\n",
      "                         args.learning_rate_decay_type)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.warmup_epochs:\r\n",
      "        warmup_lr = (\r\n",
      "            args.learning_rate * tf.cast(global_step, tf.float32) /\r\n",
      "            (steps_per_epoch * args.warmup_epochs))\r\n",
      "        learning_rate = tf.minimum(warmup_lr, learning_rate)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m learning_rate\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_configure_optimizer\u001b[39;49;00m(args, learning_rate):\r\n",
      "    \u001b[33m\"\"\"Configures the optimizer used for training.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m      learning_rate: A scalar or `Tensor` learning rate.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m      An instance of an optimizer.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Raises:\u001b[39;49;00m\r\n",
      "\u001b[33m      ValueError: if args.optimizer is not recognized.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33madadelta\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.AdadeltaOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            rho=args.adadelta_rho,\r\n",
      "            epsilon=args.opt_epsilon)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33madagrad\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.AdagradOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            initial_accumulator_value=args.adagrad_initial_accumulator_value)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.AdamOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            beta1=args.adam_beta1,\r\n",
      "            beta2=args.adam_beta2,\r\n",
      "            epsilon=args.opt_epsilon)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33mftrl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.FtrlOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            learning_rate_power=args.ftrl_learning_rate_power,\r\n",
      "            initial_accumulator_value=args.ftrl_initial_accumulator_value,\r\n",
      "            l1_regularization_strength=args.ftrl_l1,\r\n",
      "            l2_regularization_strength=args.ftrl_l2)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33mmomentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.MomentumOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            momentum=args.momentum,\r\n",
      "            name=\u001b[33m'\u001b[39;49;00m\u001b[33mMomentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33mrmsprop\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.RMSPropOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            decay=args.rmsprop_decay,\r\n",
      "            momentum=args.rmsprop_momentum,\r\n",
      "            epsilon=args.opt_epsilon)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33msgd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mOptimizer [\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m] was not recognized\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % args.optimizer)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m optimizer\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_init_fn\u001b[39;49;00m(args):\r\n",
      "    \u001b[33m\"\"\"Returns a function run by the chief worker to warm-start the training.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Note that the init_fn is only run when initializing the model during the very\u001b[39;49;00m\r\n",
      "\u001b[33m    first global step.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m      An init function run by the supervisor.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.finetune_checkpoint_path \u001b[35mis\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Warn the user if a checkpoint exists in the train_dir. Then we'll be\u001b[39;49;00m\r\n",
      "    \u001b[37m# ignoring the checkpoint anyway.\u001b[39;49;00m\r\n",
      "    \u001b[37m# if tf.train.latest_checkpoint(args.finetune_checkpoint_path):\u001b[39;49;00m\r\n",
      "    \u001b[37m#     tf.logging.info(\u001b[39;49;00m\r\n",
      "    \u001b[37m#         'Ignoring --checkpoint_path because a checkpoint already exists in %s'\u001b[39;49;00m\r\n",
      "    \u001b[37m#         % args.finetune_checkpoint_path)\u001b[39;49;00m\r\n",
      "    \u001b[37m#     return None\u001b[39;49;00m\r\n",
      "\r\n",
      "    exclusions = []\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.checkpoint_exclude_scopes:\r\n",
      "        exclusions = [scope.strip()\r\n",
      "                      \u001b[34mfor\u001b[39;49;00m scope \u001b[35min\u001b[39;49;00m args.checkpoint_exclude_scopes.split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)]\r\n",
      "\r\n",
      "    \u001b[37m# TODO(sguada) variables.filter_variables()\u001b[39;49;00m\r\n",
      "    variables_to_restore = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m var \u001b[35min\u001b[39;49;00m slim.get_model_variables():\r\n",
      "        \u001b[34mfor\u001b[39;49;00m exclusion \u001b[35min\u001b[39;49;00m exclusions:\r\n",
      "            \u001b[34mif\u001b[39;49;00m var.op.name.startswith(exclusion):\r\n",
      "                \u001b[34mbreak\u001b[39;49;00m\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            variables_to_restore.append(var)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m tf.gfile.IsDirectory(args.finetune_checkpoint_path):\r\n",
      "        checkpoint_path = tf.train.latest_checkpoint(\r\n",
      "            args.finetune_checkpoint_path)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        checkpoint_path = args.finetune_checkpoint_path\r\n",
      "\r\n",
      "    tf.logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mFine-tuning from \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % checkpoint_path)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m slim.assign_from_checkpoint_fn(\r\n",
      "        checkpoint_path,\r\n",
      "        variables_to_restore,\r\n",
      "        ignore_missing_vars=args.ignore_missing_vars)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_variables_to_train\u001b[39;49;00m(args):\r\n",
      "    \u001b[33m\"\"\"Returns a list of variables to train.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m      A list of variables to train by the optimizer.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.trainable_scopes \u001b[35mis\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m tf.trainable_variables()\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        scopes = [scope.strip() \u001b[34mfor\u001b[39;49;00m scope \u001b[35min\u001b[39;49;00m args.trainable_scopes.split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)]\r\n",
      "\r\n",
      "    variables_to_train = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m scope \u001b[35min\u001b[39;49;00m scopes:\r\n",
      "        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\r\n",
      "        variables_to_train.extend(variables)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m variables_to_train\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mevaluation\u001b[39;49;00m(args):\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.dataset_dir:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mYou must supply the dataset directory with --dataset_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    tf.logging.set_verbosity(tf.logging.INFO)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.Graph().as_default():\r\n",
      "        tf_global_step = slim.get_or_create_global_step()\r\n",
      "\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the dataset #\u001b[39;49;00m\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        dataset = dataset_factory.get_dataset(\r\n",
      "            args.dataset_name, \u001b[33m'\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.dataset_dir, args.test_num_data)\r\n",
      "\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the model #\u001b[39;49;00m\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        network_fn = nets_factory.get_network_fn(\r\n",
      "            args.model_name,\r\n",
      "            num_classes=(dataset.num_classes - args.labels_offset),\r\n",
      "            is_training=\u001b[36mFalse\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m##############################################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Create a dataset provider that loads data from the dataset #\u001b[39;49;00m\r\n",
      "        \u001b[37m##############################################################\u001b[39;49;00m\r\n",
      "        provider = slim.dataset_data_provider.DatasetDataProvider(\r\n",
      "            dataset,\r\n",
      "            shuffle=\u001b[36mFalse\u001b[39;49;00m,\r\n",
      "            common_queue_capacity=\u001b[34m2\u001b[39;49;00m * args.eval_batch_size,\r\n",
      "            common_queue_min=args.eval_batch_size)\r\n",
      "        [image, label] = provider.get([\u001b[33m'\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "        label -= args.labels_offset\r\n",
      "\r\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the preprocessing function #\u001b[39;49;00m\r\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\r\n",
      "        preprocessing_name = args.preprocessing_name \u001b[35mor\u001b[39;49;00m args.model_name\r\n",
      "        \r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33meval_args.use_grayscale : {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.use_grayscale))\r\n",
      "        \r\n",
      "        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\r\n",
      "            preprocessing_name,\r\n",
      "            is_training=\u001b[36mFalse\u001b[39;49;00m,\r\n",
      "            use_grayscale=args.use_grayscale)\r\n",
      "\r\n",
      "        eval_image_size = args.image_size \u001b[35mor\u001b[39;49;00m network_fn.default_image_size\r\n",
      "\r\n",
      "        image = image_preprocessing_fn(image, eval_image_size, eval_image_size)\r\n",
      "\r\n",
      "        images, labels = tf.train.batch(\r\n",
      "            [image, label],\r\n",
      "            batch_size=args.eval_batch_size,\r\n",
      "            num_threads=args.num_preprocessing_threads,\r\n",
      "            capacity=\u001b[34m5\u001b[39;49;00m * args.eval_batch_size)\r\n",
      "\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Define the model #\u001b[39;49;00m\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        logits, _ = network_fn(images)\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.quantize:\r\n",
      "            contrib_quantize.create_eval_graph()\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.moving_average_decay:\r\n",
      "            variable_averages = tf.train.ExponentialMovingAverage(\r\n",
      "                args.moving_average_decay, tf_global_step)\r\n",
      "            variables_to_restore = variable_averages.variables_to_restore(\r\n",
      "                slim.get_model_variables())\r\n",
      "            variables_to_restore[tf_global_step.op.name] = tf_global_step\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            variables_to_restore = slim.get_variables_to_restore()\r\n",
      "\r\n",
      "        predictions = tf.argmax(logits, \u001b[34m1\u001b[39;49;00m)\r\n",
      "        labels = tf.squeeze(labels)\r\n",
      "\r\n",
      "        \u001b[37m# Define the metrics:\u001b[39;49;00m\r\n",
      "        names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mAccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: slim.metrics.streaming_accuracy(predictions, labels),\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mRecall_5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: slim.metrics.streaming_recall_at_k(\r\n",
      "                logits, labels, \u001b[34m5\u001b[39;49;00m),\r\n",
      "        })\r\n",
      "\r\n",
      "        \u001b[37m# Print the summaries to screen.\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m name, value \u001b[35min\u001b[39;49;00m names_to_values.items():\r\n",
      "            summary_name = \u001b[33m'\u001b[39;49;00m\u001b[33meval/\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % name\r\n",
      "            op = tf.summary.scalar(summary_name, value, collections=[])\r\n",
      "            op = tf.Print(op, [value], summary_name)\r\n",
      "            tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\r\n",
      "\r\n",
      "        \u001b[37m# TODO(sguada) use num_epochs=1\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.max_eval_num_batches:\r\n",
      "            num_batches = args.max_eval_num_batches\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[37m# This ensures that we make a single pass over all of the data.\u001b[39;49;00m\r\n",
      "            num_batches = math.ceil(\r\n",
      "                dataset.num_samples / \u001b[36mfloat\u001b[39;49;00m(args.eval_batch_size))\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m tf.gfile.IsDirectory(args.train_dir):\r\n",
      "            checkpoint_path = tf.train.latest_checkpoint(args.train_dir)\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            checkpoint_path = args.train_dir\r\n",
      "\r\n",
      "        tf.logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mEvaluating \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % checkpoint_path)\r\n",
      "\r\n",
      "        slim.evaluation.evaluate_once(\r\n",
      "            master=args.master,\r\n",
      "            checkpoint_path=checkpoint_path,\r\n",
      "            logdir=args.train_dir,\r\n",
      "            num_evals=num_batches,\r\n",
      "            eval_op=\u001b[36mlist\u001b[39;49;00m(names_to_updates.values()),\r\n",
      "            variables_to_restore=variables_to_restore)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mexport_inference_graph\u001b[39;49;00m(args):\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.train_dir:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mYou must supply the path to save to with --train_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.is_video_model \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.num_frames:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mNumber of frames must be specified for video models with --num_frames\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    tf.logging.set_verbosity(tf.logging.INFO)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.Graph().as_default() \u001b[34mas\u001b[39;49;00m graph:\r\n",
      "        dataset = dataset_factory.get_dataset(args.dataset_name, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                              args.dataset_dir, args.train_num_data)\r\n",
      "        network_fn = nets_factory.get_network_fn(\r\n",
      "            args.model_name,\r\n",
      "            num_classes=(dataset.num_classes - args.labels_offset),\r\n",
      "            is_training=args.is_training)\r\n",
      "        image_size = args.image_size \u001b[35mor\u001b[39;49;00m network_fn.default_image_size\r\n",
      "        num_channels = \u001b[34m1\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m args.use_grayscale \u001b[34melse\u001b[39;49;00m \u001b[34m3\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.is_video_model:\r\n",
      "            input_shape = [\r\n",
      "                \u001b[34m1\u001b[39;49;00m, args.num_frames, image_size, image_size,\r\n",
      "                num_channels\r\n",
      "            ]\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            input_shape = [\u001b[34m1\u001b[39;49;00m,\r\n",
      "                           image_size, image_size, num_channels]\r\n",
      "        placeholder = tf.placeholder(name=\u001b[33m'\u001b[39;49;00m\u001b[33minput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, dtype=tf.float32,\r\n",
      "                                     shape=input_shape)\r\n",
      "        network_fn(placeholder)\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.quantize:\r\n",
      "            contrib_quantize.create_eval_graph()\r\n",
      "\r\n",
      "        graph_def = graph.as_graph_def()\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.write_text_graphdef:\r\n",
      "            tf.io.write_graph(\r\n",
      "                graph_def,\r\n",
      "                os.path.dirname(args.train_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/inference_graph.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "                os.path.basename(args.train_dir),\r\n",
      "                as_text=\u001b[36mTrue\u001b[39;49;00m)\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[34mwith\u001b[39;49;00m gfile.GFile(args.train_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/inference_graph.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "                f.write(graph_def.SerializeToString())\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mfreeze_graph\u001b[39;49;00m(args):\r\n",
      "    checkpoint_version = saver_pb2.SaverDef.V2\r\n",
      "    input_graph = args.train_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/inference_graph.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    input_checkpoint = tf.train.latest_checkpoint(args.train_dir)\r\n",
      "    input_binary = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "    output_graph = args.train_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/inference_graph_frozen.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    output_node_names = \u001b[33m'\u001b[39;49;00m\u001b[33mMobilenetV1/Predictions/Reshape_1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    input_saved_model_dir = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    saved_model_tags = \u001b[33m\"\u001b[39;49;00m\u001b[33mserve\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    input_meta_graph = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    variable_names_blacklist = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    variable_names_whitelist = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    initializer_nodes = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    clear_devices = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "    filename_tensor_name = \u001b[33m\"\u001b[39;49;00m\u001b[33msave/Const:0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    restore_op_name = \u001b[33m\"\u001b[39;49;00m\u001b[33msave/restore_all\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    input_saver = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mfreeze_graph input_checkpoint : {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(input_checkpoint))\r\n",
      "\r\n",
      "    fg.freeze_graph(input_graph, input_saver, input_binary,\r\n",
      "                    input_checkpoint, output_node_names,\r\n",
      "                    restore_op_name, filename_tensor_name,\r\n",
      "                    output_graph, clear_devices, initializer_nodes,\r\n",
      "                    variable_names_whitelist, variable_names_blacklist,\r\n",
      "                    input_meta_graph, input_saved_model_dir,\r\n",
      "                    saved_model_tags, checkpoint_version)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\r\n",
      "    args, unknown = parse_args()\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.dataset_dir:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mYou must supply the dataset directory with --dataset_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    tf.logging.set_verbosity(tf.logging.INFO)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.Graph().as_default():\r\n",
      "        \u001b[37m#######################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Config model_deploy #\u001b[39;49;00m\r\n",
      "        \u001b[37m#######################\u001b[39;49;00m\r\n",
      "        deploy_config = model_deploy.DeploymentConfig(\r\n",
      "            num_clones=args.num_clones,\r\n",
      "            clone_on_cpu=args.clone_on_cpu,\r\n",
      "            replica_id=args.task,\r\n",
      "            num_replicas=args.worker_replicas,\r\n",
      "            num_ps_tasks=args.num_ps_tasks)\r\n",
      "\r\n",
      "        \u001b[37m# Create global_step\u001b[39;49;00m\r\n",
      "        \u001b[34mwith\u001b[39;49;00m tf.device(deploy_config.variables_device()):\r\n",
      "            global_step = slim.create_global_step()\r\n",
      "\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the dataset #\u001b[39;49;00m\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        dataset = dataset_factory.get_dataset(\r\n",
      "            args.dataset_name, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.dataset_dir, args.train_num_data)\r\n",
      "\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the network #\u001b[39;49;00m\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        network_fn = nets_factory.get_network_fn(\r\n",
      "            args.model_name,\r\n",
      "            num_classes=(dataset.num_classes - args.labels_offset),\r\n",
      "            weight_decay=args.weight_decay,\r\n",
      "            is_training=\u001b[36mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the preprocessing function #\u001b[39;49;00m\r\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\r\n",
      "        preprocessing_name = args.preprocessing_name \u001b[35mor\u001b[39;49;00m args.model_name\r\n",
      "        \r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_args.use_grayscale : {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.use_grayscale))\r\n",
      "        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\r\n",
      "            preprocessing_name,\r\n",
      "            is_training=\u001b[36mTrue\u001b[39;49;00m,\r\n",
      "            use_grayscale=args.use_grayscale)\r\n",
      "\r\n",
      "        \u001b[37m##############################################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Create a dataset provider that loads data from the dataset #\u001b[39;49;00m\r\n",
      "        \u001b[37m##############################################################\u001b[39;49;00m\r\n",
      "        \u001b[34mwith\u001b[39;49;00m tf.device(deploy_config.inputs_device()):\r\n",
      "            provider = slim.dataset_data_provider.DatasetDataProvider(\r\n",
      "                dataset,\r\n",
      "                num_readers=args.num_readers,\r\n",
      "                common_queue_capacity=\u001b[34m20\u001b[39;49;00m * args.batch_size,\r\n",
      "                common_queue_min=\u001b[34m10\u001b[39;49;00m * args.batch_size)\r\n",
      "            [image, label] = provider.get([\u001b[33m'\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "            label -= args.labels_offset\r\n",
      "\r\n",
      "            train_image_size = args.image_size \u001b[35mor\u001b[39;49;00m network_fn.default_image_size\r\n",
      "\r\n",
      "            image = image_preprocessing_fn(\r\n",
      "                image, train_image_size, train_image_size)\r\n",
      "\r\n",
      "            images, labels = tf.train.batch(\r\n",
      "                [image, label],\r\n",
      "                batch_size=args.batch_size,\r\n",
      "                num_threads=args.num_preprocessing_threads,\r\n",
      "                capacity=\u001b[34m5\u001b[39;49;00m * args.batch_size)\r\n",
      "            labels = slim.one_hot_encoding(\r\n",
      "                labels, dataset.num_classes - args.labels_offset)\r\n",
      "            batch_queue = slim.prefetch_queue.prefetch_queue(\r\n",
      "                [images, labels], capacity=\u001b[34m2\u001b[39;49;00m * deploy_config.num_clones)\r\n",
      "\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Define the model #\u001b[39;49;00m\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32mclone_fn\u001b[39;49;00m(args, batch_queue):\r\n",
      "            \u001b[33m\"\"\"Allows data parallelism by creating multiple clones of network_fn.\"\"\"\u001b[39;49;00m\r\n",
      "            images, labels = batch_queue.dequeue()\r\n",
      "            logits, end_points = network_fn(images)\r\n",
      "\r\n",
      "            \u001b[37m#############################\u001b[39;49;00m\r\n",
      "            \u001b[37m# Specify the loss function #\u001b[39;49;00m\r\n",
      "            \u001b[37m#############################\u001b[39;49;00m\r\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mAuxLogits\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m end_points:\r\n",
      "                slim.losses.softmax_cross_entropy(\r\n",
      "                    end_points[\u001b[33m'\u001b[39;49;00m\u001b[33mAuxLogits\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], labels,\r\n",
      "                    label_smoothing=args.label_smoothing, weights=\u001b[34m0.4\u001b[39;49;00m,\r\n",
      "                    scope=\u001b[33m'\u001b[39;49;00m\u001b[33maux_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            slim.losses.softmax_cross_entropy(\r\n",
      "                logits, labels, label_smoothing=args.label_smoothing, weights=\u001b[34m1.0\u001b[39;49;00m)\r\n",
      "            \u001b[34mreturn\u001b[39;49;00m end_points\r\n",
      "\r\n",
      "        \u001b[37m# Gather initial summaries.\u001b[39;49;00m\r\n",
      "        summaries = \u001b[36mset\u001b[39;49;00m(tf.get_collection(tf.GraphKeys.SUMMARIES))\r\n",
      "\r\n",
      "        clones = model_deploy.create_clones(\r\n",
      "            deploy_config, clone_fn, [args, batch_queue])\r\n",
      "        first_clone_scope = deploy_config.clone_scope(\u001b[34m0\u001b[39;49;00m)\r\n",
      "        \u001b[37m# Gather update_ops from the first clone. These contain, for example,\u001b[39;49;00m\r\n",
      "        \u001b[37m# the updates for the batch_norm variables created by network_fn.\u001b[39;49;00m\r\n",
      "        update_ops = tf.get_collection(\r\n",
      "            tf.GraphKeys.UPDATE_OPS, first_clone_scope)\r\n",
      "\r\n",
      "        \u001b[37m# Add summaries for end_points.\u001b[39;49;00m\r\n",
      "        end_points = clones[\u001b[34m0\u001b[39;49;00m].outputs\r\n",
      "        \u001b[34mfor\u001b[39;49;00m end_point \u001b[35min\u001b[39;49;00m end_points:\r\n",
      "            x = end_points[end_point]\r\n",
      "            summaries.add(tf.summary.histogram(\u001b[33m'\u001b[39;49;00m\u001b[33mactivations/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + end_point, x))\r\n",
      "            summaries.add(tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33msparsity/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + end_point,\r\n",
      "                                            tf.nn.zero_fraction(x)))\r\n",
      "\r\n",
      "        \u001b[37m# Add summaries for losses.\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m loss \u001b[35min\u001b[39;49;00m tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\r\n",
      "            summaries.add(tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mlosses/\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % loss.op.name, loss))\r\n",
      "\r\n",
      "        \u001b[37m# Add summaries for variables.\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m variable \u001b[35min\u001b[39;49;00m slim.get_model_variables():\r\n",
      "            summaries.add(tf.summary.histogram(variable.op.name, variable))\r\n",
      "\r\n",
      "        \u001b[37m#################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Configure the moving averages #\u001b[39;49;00m\r\n",
      "        \u001b[37m#################################\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.moving_average_decay:\r\n",
      "            moving_average_variables = slim.get_model_variables()\r\n",
      "            variable_averages = tf.train.ExponentialMovingAverage(\r\n",
      "                args.moving_average_decay, global_step)\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            moving_average_variables, variable_averages = \u001b[36mNone\u001b[39;49;00m, \u001b[36mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.quantize_delay >= \u001b[34m0\u001b[39;49;00m:\r\n",
      "            contrib_quantize.create_training_graph(\r\n",
      "                quant_delay=args.quantize_delay)\r\n",
      "\r\n",
      "        \u001b[37m#########################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Configure the optimization procedure. #\u001b[39;49;00m\r\n",
      "        \u001b[37m#########################################\u001b[39;49;00m\r\n",
      "        \u001b[34mwith\u001b[39;49;00m tf.device(deploy_config.optimizer_device()):\r\n",
      "            learning_rate = _configure_learning_rate(args,\r\n",
      "                                                     dataset.num_samples, global_step)\r\n",
      "            optimizer = _configure_optimizer(args, learning_rate)\r\n",
      "            summaries.add(tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, learning_rate))\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.sync_replicas:\r\n",
      "            \u001b[37m# If sync_replicas is enabled, the averaging will be done in the chief\u001b[39;49;00m\r\n",
      "            \u001b[37m# queue runner.\u001b[39;49;00m\r\n",
      "            optimizer = tf.train.SyncReplicasOptimizer(\r\n",
      "                opt=optimizer,\r\n",
      "                replicas_to_aggregate=args.replicas_to_aggregate,\r\n",
      "                total_num_replicas=args.worker_replicas,\r\n",
      "                variable_averages=variable_averages,\r\n",
      "                variables_to_average=moving_average_variables)\r\n",
      "        \u001b[34melif\u001b[39;49;00m args.moving_average_decay:\r\n",
      "            \u001b[37m# Update ops executed locally by trainer.\u001b[39;49;00m\r\n",
      "            update_ops.append(variable_averages.apply(\r\n",
      "                moving_average_variables))\r\n",
      "\r\n",
      "        \u001b[37m# Variables to train.\u001b[39;49;00m\r\n",
      "        variables_to_train = _get_variables_to_train(args)\r\n",
      "\r\n",
      "        \u001b[37m#  and returns a train_tensor and summary_op\u001b[39;49;00m\r\n",
      "        total_loss, clones_gradients = model_deploy.optimize_clones(\r\n",
      "            clones,\r\n",
      "            optimizer,\r\n",
      "            var_list=variables_to_train)\r\n",
      "        \u001b[37m# Add total_loss to summary.\u001b[39;49;00m\r\n",
      "        summaries.add(tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mtotal_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, total_loss))\r\n",
      "\r\n",
      "        \u001b[37m# Create gradient updates.\u001b[39;49;00m\r\n",
      "        grad_updates = optimizer.apply_gradients(clones_gradients,\r\n",
      "                                                 global_step=global_step)\r\n",
      "        update_ops.append(grad_updates)\r\n",
      "\r\n",
      "        update_op = tf.group(*update_ops)\r\n",
      "        \u001b[34mwith\u001b[39;49;00m tf.control_dependencies([update_op]):\r\n",
      "            train_tensor = tf.identity(total_loss, name=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_op\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m# Add the summaries from the first clone. These contain the summaries\u001b[39;49;00m\r\n",
      "        \u001b[37m# created by model_fn and either optimize_clones() or _gather_clone_loss().\u001b[39;49;00m\r\n",
      "        summaries |= \u001b[36mset\u001b[39;49;00m(tf.get_collection(tf.GraphKeys.SUMMARIES,\r\n",
      "                                           first_clone_scope))\r\n",
      "\r\n",
      "        \u001b[37m# Merge all summaries together.\u001b[39;49;00m\r\n",
      "        summary_op = tf.summary.merge(\u001b[36mlist\u001b[39;49;00m(summaries), name=\u001b[33m'\u001b[39;49;00m\u001b[33msummary_op\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m###########################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Kicks off the training. #\u001b[39;49;00m\r\n",
      "        \u001b[37m###########################\u001b[39;49;00m\r\n",
      "        slim.learning.train(\r\n",
      "            train_tensor,\r\n",
      "            logdir=args.train_dir,\r\n",
      "            master=args.master,\r\n",
      "            is_chief=(args.task == \u001b[34m0\u001b[39;49;00m),\r\n",
      "            init_fn=_get_init_fn(args),\r\n",
      "            summary_op=summary_op,\r\n",
      "            number_of_steps=args.max_number_of_steps,\r\n",
      "            log_every_n_steps=args.log_every_n_steps,\r\n",
      "            save_summaries_secs=args.save_summaries_secs,\r\n",
      "            save_interval_secs=args.save_interval_secs,\r\n",
      "            sync_optimizer=optimizer \u001b[34mif\u001b[39;49;00m args.sync_replicas \u001b[34melse\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m)\r\n",
      "\r\n",
      "    evaluation(args)\r\n",
      "    export_inference_graph(args)\r\n",
      "    freeze_graph(args)\r\n",
      "\r\n",
      "    converter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n",
      "        args.train_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/inference_graph_frozen.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33minput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], [\u001b[33m'\u001b[39;49;00m\u001b[33mMobilenetV1/Predictions/Reshape_1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    tflite_model = converter.convert()\r\n",
      "    \u001b[36mopen\u001b[39;49;00m(args.train_dir + \u001b[33m\"\u001b[39;49;00m\u001b[33m/mobilenetv1_model.tflite\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "         \u001b[33m\"\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).write(tflite_model)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize './src_dir/image_classifier.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. `TensorFlow` estimator를 이용한 training job 생성하기\n",
    "\n",
    "\n",
    "<p><strong><code>sagemaker.tensorflow.TensorFlow</code></strong> estimator는 처음 실행하는 스크립트 위치와 다양한 연계 코드들이 위치한 디렉토리 정보를 찾아서 스크립트를 S3에 upload하고 SageMaker의 training job을 수행하게 됩니다. training job은 학습을 수행한 단위입니다. 학습을 1번 돌리면 training job이 1개 생성됩니다. 몇 가지 중요 파라미터를 아래와 같이 설명드립니다. </p>\n",
    "\n",
    "- **entry_point** : 학습을 처음 실행하는 Python 소스 파일의 절대 또는 상대 경로이며, source_dir이 지정된 경우 entry_point는 source_dir 내 파일이 됩니다.\n",
    "- **source_dir** : 학습에 연계되는 다양한 소스코드 파일이 들어 있는 디렉토리 위치이며, 절대, 상대 경로 또는 S3 URI가 모두 가능하며,source_dir이 S3 URI 인 경우 tar.gz 파일이 됩니다.\n",
    "- **role** : Amazon SageMaker가 사용자를 대신해 작업(예: S3 버킷에서 모델 결과물이라고 하는 훈련 결과 읽기 및 Amazon S3에 훈련 결과 쓰기)을 수행하는 AWS Identity and Access Management(IAM) 역할입니다.\n",
    "- **train_instance_count** : 학습을 수행하는 instance 개수를 정의할 수 있습니다.\n",
    "- **train_instance_type** : 학습을 수행하는 instance 타입을 정의할 수 있습니다.\n",
    "- **train_volume_size** : 학습 인스턴스에 연결할 Amazon Elastic Block Store(Amazon EBS) 스토리지 볼륨의 크기(GB)입니다. File 모드를 사용할 경우 이 값이 훈련 데이터를 충분히 저장할 수 있는 크기여야 합니다(File 모드가 기본값)\n",
    "- **train_use_spot_instances** : 학습에서 SageMaker Managed Spot 인스턴스를 사용할지 여부를 지정합니다. 활성화되면 train_max_wait도 설정해야 합니다.\n",
    "- **train_max_run** : 최대 학습 시간을 설정할 수 있으며, 이 시간이 지나면 Amazon SageMaker는 현재 상태에 관계없이 작업을 종료합니다. (기본값 : 24 * 60 * 60)\n",
    "- **train_max_wait** : SageMaker Managed Spot 인스턴스를 기다리는 초 단위의 시간을 의미하는 것으로, 이 시간이 지나면 Amazon SageMaker는 스팟 인스턴스가 사용 가능해지기를 기다리는 것을 중지하며 결과는 fail이 됩니다.\n",
    "- **framework_version** : 학습에 사용될 특정 Tensorflow 버전을 정의할 수 있습니다.\n",
    "- **py_version** : 컨테이너 환경이 python3일 경우 py3, python2일 경우 py2로 설정하면 됩니다. python2는 지원이 중단되었지만 기존 python2로 구성된 파일들을 지원하기 위해 현재 계속 사용할 수 있습니다.\n",
    "- **hyperparameters** : 학습에 사용할 하이퍼 파라미터를 정의할 수 있으며, 정의된 하이퍼 파라미터 값들은 모두 학습 컨테이너로 전송이 됩니다.\n",
    "\n",
    "<p> 추가적으로 분산/ 멀티 GPU 학습도 가능합니다. SageMaker는 <strong><a href=\"https://github.com/horovod/horovod\" target=\"_blank\" class ='btn-default'>Horovod</a></strong>에 최적화된 환경을 제공하고 있으며, 자세한 내용은 <strong><a href=\"https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/tensorflow#distributed-training\" target=\"_blank\" class ='btn-default'>여기</a></strong>에서 확인이 가능합니다. 이번 학습에서는 분산/멀티 GPU 학습은 제외하였습니다.(단, 기존과 동일하게 python 소스코드에 분산/멀티 학습이 가능하도록 구성 필요) </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>S3에 저장된 TFRecord 파일의 위치를 다시 확인합니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-experiments-us-east-1-143656149352/captured_data/tfrecord'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Dataset 위치\n",
    "inputs= 's3://{}/{}'.format(data_bucket, prefix)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        'dataset_name' : 'captured_dataset',\n",
    "        'model_name' : 'mobilenet_v1_025',\n",
    "        'preprocessing_name' : 'mobilenet_v1',\n",
    "        'image_size' : 128,\n",
    "        'save_summaries_secs' : 300,\n",
    "        'label_smoothing' : 0.1,\n",
    "        'learning_rate_decay_factor' : 0.98,\n",
    "        'num_epochs_per_decay' : 2.5,\n",
    "        'moving_average_decay' : 0.9999,\n",
    "        'batch_size' : 128,\n",
    "        'max_number_of_steps' : 10000,\n",
    "        'eval_batch_size' : 1000,\n",
    "        'train_num_data' : train_num_data,\n",
    "        'test_num_data': test_num_data,\n",
    "#         'finetune_checkpoint_path' : 'fine_tune_checkpoint/mobilenet_v1_0.25_128.ckpt',\n",
    "        'finetune_checkpoint_path' : 'fine_tune_checkpoint/model.ckpt-10000',\n",
    "        'checkpoint_exclude_scopes' : 'MobilenetV1/Logits,MobilenetV1/AuxLogits',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"sagemaker\"\n"
     ]
    }
   ],
   "source": [
    "training_job_name = \"{}-img-classifier-training-job\".format(int(time.time()))\n",
    "estimator = TensorFlow(entry_point='image_classifier.py',\n",
    "                       source_dir='src_dir',\n",
    "                       role=role,\n",
    "                       train_instance_count=1,\n",
    "                       train_instance_type='ml.p3.2xlarge',\n",
    "                       train_use_spot_instances=True,  # spot instance 활용\n",
    "                       train_volume_size=400,\n",
    "                       train_max_run=12*60*60,\n",
    "                       train_max_wait=12*60*60,\n",
    "#                        train_instance_type='local_gpu',\n",
    "                       framework_version='1.14.0',\n",
    "                       py_version='py2',\n",
    "                       hyperparameters=hyperparameters\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fit 함수로 학습 시작하기 \n",
    "\n",
    "<p>학습을 시작하는 것은 <strong><code>estimator.fit (training_data_uri)</code></strong>이 호출되는 경우입니다. 여기에서 실제 데이터가 있는 S3의 위치가 입력으로 사용됩니다. <code>fit</code>에서는 <code>training</code>라는 기본 채널을 생성하며, 이 위치의 데이터는 S3에서 실제 컨테이너 환경에서는 <code>SM_CHANNEL_TRAINING</code> 위치로 복사되어 학습에 활용이 가능합니다. <code>fit</code>은 몇 가지 다른 유형의 입력도 허용하는데 자세한 내용은 <strong><a href=\"https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.EstimatorBase.fit\" target=\"_blank\" class ='btn-default'>API 문서</a></strong>를 참고하실 수 있습니다.</p>\n",
    "<p> 학습이 시작되면 Tensorflow 컨테이너에서는 <code>image_classifier.py</code>를 실행되며, <code>Estimator</code>에서 <code>hyperparameters</code> 와 <code>model_dir</code>을 스크립트의 파라미터로 전달합니다. <code>model_dir</code>을 별도로 전달하지 않으며, 기본값은<strong>s3://[DEFAULT_BUCKET]/[TRAINING_JOB_NAME] </strong>이 되며 실제 스크립트 실행은 다음과 같습니다. </p>\n",
    "    \n",
    "```bash\n",
    "python image_classifier.py --model_dir s3://[DEFAULT_BUCKET]/[TRAINING_JOB_NAME]\n",
    "```\n",
    "<p>학습이 완료되면 training job은 Tensorflow serving을 위해 saved model을 S3에 upload합니다.</p>\n",
    "<p><code>fit</code>에서 <strong>wait=True</strong>로 설정할 경우 <strong>Synchronous</strong> 방식으로 동직하게 되며, <strong>wait=False</strong>일 경우 <strong>Aynchronous</strong> 방식으로 동작되어 여러 개의 Training job을 동시에 실행할 수 있습니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_job_name : 1592092163-img-classifier-training-job\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(\n",
    "    inputs = {'training': inputs},\n",
    "    job_name=training_job_name,\n",
    "    logs='All',\n",
    "    wait=False\n",
    ")\n",
    "print(\"training_job_name : {}\".format(training_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Aynchronous</strong>로 진행된 Training job은 아래와 같은 방법으로 진행상황을 실시간으로 확인할 수 있습니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-13 23:49:34 Starting - Starting the training job...\n",
      "2020-06-13 23:49:37 Starting - Launching requested ML instances......\n",
      "2020-06-13 23:50:49 Starting - Preparing the instances for training......\n",
      "2020-06-13 23:51:48 Downloading - Downloading input data...\n",
      "2020-06-13 23:52:13 Training - Downloading the training image...\n",
      "2020-06-13 23:52:42 Training - Training image download completed. Training in progress.\u001b[34mparser.parse_known_args() : (Namespace(adadelta_rho=0.95, adagrad_initial_accumulator_value=0.1, adam_beta1=0.9, adam_beta2=0.999, batch_size=128, checkpoint_exclude_scopes='MobilenetV1/Logits,MobilenetV1/AuxLogits', clone_on_cpu=False, current_host='algo-1', data_config={u'training': {u'TrainingInputMode': u'File', u'RecordWrapperType': u'None', u'S3DistributionType': u'FullyReplicated'}}, dataset_dir='/opt/ml/input/data/training', dataset_name='captured_dataset', end_learning_rate=0.01, eval_batch_size=1000, finetune_checkpoint_path='fine_tune_checkpoint/model.ckpt-10000', ftrl_initial_accumulator_value=0.1, ftrl_l1=0.0, ftrl_l2=0.0, ftrl_learning_rate_power=-0.5, fw_params={}, hosts=[u'algo-1'], ignore_missing_vars=False, image_size=128, is_training=False, is_video_model=False, label_smoothing=0.1, labels_offset=0, learning_rate=0.01, learning_rate_decay_factor=0.98, learning_rate_decay_type='exponential', log_every_n_steps=10, master='', max_eval_num_batches=None, max_number_of_steps=10000, model_dir='s3://sagemaker-us-east-1-143656149352/1592092163-img-classifier-training-job/model', model_name='mobilenet_v1_025', momentum=0.9, moving_average_decay=0.9999, num_clones=1, num_epochs_per_decay=2.5, num_frames=None, num_gpus='1', num_preprocessing_threads=4, num_ps_tasks=0, num_readers=4, opt_epsilon=1.0, optimizer='rmsprop', output_data_dir='/opt/ml/output/data', output_dir='/opt/ml/output', preprocessing_name='mobilenet_v1', quantize=False, quantize_delay=-1, replicas_to_aggregate=1, rmsprop_decay=0.9, rmsprop_momentum=0.9, save_interval_secs=600, save_summaries_secs=300, sync_replicas=False, task=0, test_num_data=998, train_dir='/opt/ml/model', train_num_data=3992, trainable_scopes=None, use_grayscale=False, warmup_epochs=0, weight_decay=4e-05, worker_replicas=1, write_text_graphdef=False), [])\u001b[0m\n",
      "\u001b[34mWARNING: Logging before flag parsing goes to stderr.\u001b[0m\n",
      "\u001b[34mW0613 23:52:56.958700 140528830564096 deprecation_wrapper.py:119] From image_classifier.py:603: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:52:56.958961 140528830564096 deprecation_wrapper.py:119] From image_classifier.py:603: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:52:56.961184 140528830564096 deprecation.py:323] From image_classifier.py:617: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease switch to tf.train.create_global_step\u001b[0m\n",
      "\u001b[34mfile_pattern : None\u001b[0m\n",
      "\u001b[34mW0613 23:52:56.966856 140528830564096 deprecation_wrapper.py:119] From /opt/ml/code/datasets/captured_dataset.py:72: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:52:56.967638 140528830564096 deprecation_wrapper.py:119] From /opt/ml/code/datasets/captured_dataset.py:88: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:52:56.967808 140528830564096 deprecation_wrapper.py:119] From /opt/ml/code/datasets/dataset_utils.py:206: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\u001b[0m\n",
      "\u001b[34mlabels_to_names : {0: u'background', 1: u'dog'}\u001b[0m\n",
      "\u001b[34mtrain_args.use_grayscale : False\u001b[0m\n",
      "\u001b[34mW0613 23:52:56.968569 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py:246: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\u001b[0m\n",
      "\u001b[34mW0613 23:52:56.974067 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\u001b[0m\n",
      "\u001b[34mW0613 23:52:56.974904 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\u001b[0m\n",
      "\u001b[34mW0613 23:52:56.976532 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:199: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mW0613 23:52:56.977586 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mW0613 23:52:56.985049 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py:95: __init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\u001b[0m\n",
      "\u001b[34mW0613 23:52:57.090347 140528830564096 deprecation_wrapper.py:119] From /opt/ml/code/preprocessing/inception_preprocessing.py:206: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:52:57.092467 140528830564096 deprecation.py:323] From /opt/ml/code/preprocessing/inception_preprocessing.py:148: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34m`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\u001b[0m\n",
      "\u001b[34mW0613 23:52:57.098098 140528830564096 deprecation_wrapper.py:119] From /opt/ml/code/preprocessing/inception_preprocessing.py:38: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:52:57.102085 140528830564096 deprecation_wrapper.py:119] From /opt/ml/code/preprocessing/inception_preprocessing.py:230: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:52:57.143131 140528830564096 deprecation.py:323] From image_classifier.py:666: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\u001b[0m\n",
      "\u001b[34mW0613 23:52:57.161370 140528830564096 deprecation_wrapper.py:119] From image_classifier.py:693: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:52:57.161581 140528830564096 deprecation_wrapper.py:119] From image_classifier.py:693: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:52:57.162020 140528830564096 deprecation_wrapper.py:119] From /opt/ml/code/deployment/model_deploy.py:192: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:52:58.856478 140528830564096 deprecation.py:323] From image_classifier.py:689: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\u001b[0m\n",
      "\u001b[34mW0613 23:52:58.863362 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:373: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\n",
      "\u001b[0m\n",
      "\u001b[34mFuture major versions of TensorFlow will allow gradients to flow\u001b[0m\n",
      "\u001b[34minto the labels input on backprop by default.\n",
      "\u001b[0m\n",
      "\u001b[34mSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:52:58.897912 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:374: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.losses.compute_weighted_loss instead.\u001b[0m\n",
      "\u001b[34mW0613 23:52:58.906433 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:152: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mDeprecated in favor of operator or tf.math.divide.\u001b[0m\n",
      "\u001b[34mW0613 23:52:58.908186 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:154: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.where in 2.0, which has the same broadcast rule as np.where\u001b[0m\n",
      "\u001b[34mW0613 23:52:58.916121 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:121: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.losses.add_loss instead.\u001b[0m\n",
      "\u001b[34mW0613 23:52:58.916584 140528830564096 deprecation_wrapper.py:119] From image_classifier.py:707: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:52:58.917772 140528830564096 deprecation_wrapper.py:119] From image_classifier.py:708: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:52:59.622174 140528830564096 deprecation_wrapper.py:119] From image_classifier.py:262: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:52:59.627568 140528830564096 deprecation_wrapper.py:119] From image_classifier.py:333: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:52:59.628928 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/moving_averages.py:433: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\u001b[0m\n",
      "\u001b[34mW0613 23:53:03.533359 140528830564096 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/rmsprop.py:119: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m\n",
      "\u001b[34mW0613 23:53:04.530095 140528830564096 deprecation_wrapper.py:119] From image_classifier.py:782: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:53:04.537925 140528830564096 deprecation_wrapper.py:119] From image_classifier.py:379: The name tf.gfile.IsDirectory is deprecated. Please use tf.io.gfile.isdir instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0613 23:53:04.538168 140528830564096 deprecation_wrapper.py:119] From image_classifier.py:385: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\u001b[0m\n",
      "\u001b[34mI0613 23:53:04.538252 140528830564096 image_classifier.py:385] Fine-tuning from fine_tune_checkpoint/model.ckpt-10000\u001b[0m\n",
      "\u001b[34mW0613 23:53:05.383754 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:742: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease switch to tf.train.MonitoredTrainingSession\u001b[0m\n",
      "\u001b[34m2020-06-13 23:53:08.828136: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34mW0613 23:53:09.367649 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1282: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse standard file APIs to check for files with this prefix.\u001b[0m\n",
      "\u001b[34mI0613 23:53:09.369738 140528830564096 saver.py:1286] Restoring parameters from fine_tune_checkpoint/model.ckpt-10000\u001b[0m\n",
      "\u001b[34mI0613 23:53:09.652040 140528830564096 session_manager.py:500] Running local_init_op.\u001b[0m\n",
      "\u001b[34mI0613 23:53:09.749730 140528830564096 session_manager.py:502] Done running local_init_op.\u001b[0m\n",
      "\u001b[34mI0613 23:53:13.883528 140528830564096 learning.py:754] Starting Session.\u001b[0m\n",
      "\u001b[34mI0613 23:53:14.008424 140523536901888 supervisor.py:1117] Saving checkpoint to path /opt/ml/model/model.ckpt\u001b[0m\n",
      "\u001b[34mI0613 23:53:14.017405 140528830564096 learning.py:768] Starting Queues.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0613 23:53:15.985513 140523528509184 supervisor.py:1099] global_step/sec: 0\u001b[0m\n",
      "\u001b[34mI0613 23:53:21.654248 140523520116480 supervisor.py:1050] Recording summary at step 1.\u001b[0m\n",
      "\u001b[34mI0613 23:53:22.603507 140528830564096 learning.py:507] global step 10: loss = 0.5244 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:23.470480 140528830564096 learning.py:507] global step 20: loss = 0.2916 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:24.354423 140528830564096 learning.py:507] global step 30: loss = 0.3137 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:25.225996 140528830564096 learning.py:507] global step 40: loss = 0.2704 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:26.105437 140528830564096 learning.py:507] global step 50: loss = 0.2753 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:26.963731 140528830564096 learning.py:507] global step 60: loss = 0.2701 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:27.867172 140528830564096 learning.py:507] global step 70: loss = 0.2654 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:28.756453 140528830564096 learning.py:507] global step 80: loss = 0.2664 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:29.637392 140528830564096 learning.py:507] global step 90: loss = 0.2646 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:30.528213 140528830564096 learning.py:507] global step 100: loss = 0.2642 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:31.419924 140528830564096 learning.py:507] global step 110: loss = 0.2630 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:32.321914 140528830564096 learning.py:507] global step 120: loss = 0.2663 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:33.218971 140528830564096 learning.py:507] global step 130: loss = 0.2621 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:34.104195 140528830564096 learning.py:507] global step 140: loss = 0.2621 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:34.971409 140528830564096 learning.py:507] global step 150: loss = 0.2607 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:35.862207 140528830564096 learning.py:507] global step 160: loss = 0.2596 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:36.746440 140528830564096 learning.py:507] global step 170: loss = 0.2608 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:37.633230 140528830564096 learning.py:507] global step 180: loss = 0.2598 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:38.514060 140528830564096 learning.py:507] global step 190: loss = 0.2615 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:39.394831 140528830564096 learning.py:507] global step 200: loss = 0.2591 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:40.276746 140528830564096 learning.py:507] global step 210: loss = 0.2593 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:41.173827 140528830564096 learning.py:507] global step 220: loss = 0.2590 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:42.042793 140528830564096 learning.py:507] global step 230: loss = 0.2583 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:42.931906 140528830564096 learning.py:507] global step 240: loss = 0.2575 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:43.831199 140528830564096 learning.py:507] global step 250: loss = 0.2595 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:44.703509 140528830564096 learning.py:507] global step 260: loss = 0.2593 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:45.596120 140528830564096 learning.py:507] global step 270: loss = 0.2592 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:46.466097 140528830564096 learning.py:507] global step 280: loss = 0.2581 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:47.349389 140528830564096 learning.py:507] global step 290: loss = 0.2575 (0.095 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:48.260487 140528830564096 learning.py:507] global step 300: loss = 0.2580 (0.098 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:49.150075 140528830564096 learning.py:507] global step 310: loss = 0.2575 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:50.038130 140528830564096 learning.py:507] global step 320: loss = 0.2575 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:50.910523 140528830564096 learning.py:507] global step 330: loss = 0.2570 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:51.797671 140528830564096 learning.py:507] global step 340: loss = 0.2584 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:52.676697 140528830564096 learning.py:507] global step 350: loss = 0.2565 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:53.544095 140528830564096 learning.py:507] global step 360: loss = 0.2576 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:54.431037 140528830564096 learning.py:507] global step 370: loss = 0.2574 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:55.301551 140528830564096 learning.py:507] global step 380: loss = 0.2569 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:56.183428 140528830564096 learning.py:507] global step 390: loss = 0.2571 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:57.067872 140528830564096 learning.py:507] global step 400: loss = 0.2569 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:57.941416 140528830564096 learning.py:507] global step 410: loss = 0.2568 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:58.829287 140528830564096 learning.py:507] global step 420: loss = 0.2563 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:53:59.720777 140528830564096 learning.py:507] global step 430: loss = 0.2563 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:00.610383 140528830564096 learning.py:507] global step 440: loss = 0.2566 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:01.505861 140528830564096 learning.py:507] global step 450: loss = 0.2559 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:02.392333 140528830564096 learning.py:507] global step 460: loss = 0.2563 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:03.266004 140528830564096 learning.py:507] global step 470: loss = 0.2560 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:04.154944 140528830564096 learning.py:507] global step 480: loss = 0.2569 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:05.042637 140528830564096 learning.py:507] global step 490: loss = 0.2570 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:05.929261 140528830564096 learning.py:507] global step 500: loss = 0.2564 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:06.810823 140528830564096 learning.py:507] global step 510: loss = 0.2556 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:07.692442 140528830564096 learning.py:507] global step 520: loss = 0.2561 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:08.569735 140528830564096 learning.py:507] global step 530: loss = 0.2558 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:09.470952 140528830564096 learning.py:507] global step 540: loss = 0.2555 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:10.346033 140528830564096 learning.py:507] global step 550: loss = 0.2562 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:11.227041 140528830564096 learning.py:507] global step 560: loss = 0.2561 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:12.121436 140528830564096 learning.py:507] global step 570: loss = 0.2563 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:13.013524 140528830564096 learning.py:507] global step 580: loss = 0.2557 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:13.912367 140528830564096 learning.py:507] global step 590: loss = 0.2560 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:14.775525 140528830564096 learning.py:507] global step 600: loss = 0.2557 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:15.642703 140528830564096 learning.py:507] global step 610: loss = 0.2552 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:16.520304 140528830564096 learning.py:507] global step 620: loss = 0.2556 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:17.389396 140528830564096 learning.py:507] global step 630: loss = 0.2556 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:18.271286 140528830564096 learning.py:507] global step 640: loss = 0.2561 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:19.155910 140528830564096 learning.py:507] global step 650: loss = 0.2553 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:20.046370 140528830564096 learning.py:507] global step 660: loss = 0.2555 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:20.937014 140528830564096 learning.py:507] global step 670: loss = 0.2552 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:21.818948 140528830564096 learning.py:507] global step 680: loss = 0.2550 (0.093 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:22.699440 140528830564096 learning.py:507] global step 690: loss = 0.2552 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:23.572397 140528830564096 learning.py:507] global step 700: loss = 0.2551 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:24.467844 140528830564096 learning.py:507] global step 710: loss = 0.2549 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:25.338429 140528830564096 learning.py:507] global step 720: loss = 0.2549 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:26.228010 140528830564096 learning.py:507] global step 730: loss = 0.2553 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:27.105216 140528830564096 learning.py:507] global step 740: loss = 0.2550 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:27.973341 140528830564096 learning.py:507] global step 750: loss = 0.2549 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:28.873152 140528830564096 learning.py:507] global step 760: loss = 0.2548 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:29.755059 140528830564096 learning.py:507] global step 770: loss = 0.2550 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:30.648258 140528830564096 learning.py:507] global step 780: loss = 0.2548 (0.087 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0613 23:54:31.517281 140528830564096 learning.py:507] global step 790: loss = 0.2547 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:32.413651 140528830564096 learning.py:507] global step 800: loss = 0.2548 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:33.295515 140528830564096 learning.py:507] global step 810: loss = 0.2553 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:34.172312 140528830564096 learning.py:507] global step 820: loss = 0.2549 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:35.051367 140528830564096 learning.py:507] global step 830: loss = 0.2550 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:35.931350 140528830564096 learning.py:507] global step 840: loss = 0.2548 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:36.813404 140528830564096 learning.py:507] global step 850: loss = 0.2546 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:37.682575 140528830564096 learning.py:507] global step 860: loss = 0.2550 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:38.561326 140528830564096 learning.py:507] global step 870: loss = 0.2546 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:39.443007 140528830564096 learning.py:507] global step 880: loss = 0.2546 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:40.334696 140528830564096 learning.py:507] global step 890: loss = 0.2547 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:41.217267 140528830564096 learning.py:507] global step 900: loss = 0.2545 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:42.101366 140528830564096 learning.py:507] global step 910: loss = 0.2546 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:42.974086 140528830564096 learning.py:507] global step 920: loss = 0.2550 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:43.851547 140528830564096 learning.py:507] global step 930: loss = 0.2545 (0.100 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:44.732398 140528830564096 learning.py:507] global step 940: loss = 0.2550 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:45.596503 140528830564096 learning.py:507] global step 950: loss = 0.2548 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:46.475133 140528830564096 learning.py:507] global step 960: loss = 0.2545 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:47.350858 140528830564096 learning.py:507] global step 970: loss = 0.2547 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:48.251611 140528830564096 learning.py:507] global step 980: loss = 0.2546 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:49.152107 140528830564096 learning.py:507] global step 990: loss = 0.2543 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:50.033175 140528830564096 learning.py:507] global step 1000: loss = 0.2544 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:50.915931 140528830564096 learning.py:507] global step 1010: loss = 0.2544 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:51.790431 140528830564096 learning.py:507] global step 1020: loss = 0.2549 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:52.657530 140528830564096 learning.py:507] global step 1030: loss = 0.2545 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:53.542467 140528830564096 learning.py:507] global step 1040: loss = 0.2544 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:54.423944 140528830564096 learning.py:507] global step 1050: loss = 0.2545 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:55.297029 140528830564096 learning.py:507] global step 1060: loss = 0.2546 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:56.198288 140528830564096 learning.py:507] global step 1070: loss = 0.2546 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:57.067950 140528830564096 learning.py:507] global step 1080: loss = 0.2541 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:57.964696 140528830564096 learning.py:507] global step 1090: loss = 0.2543 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:58.838180 140528830564096 learning.py:507] global step 1100: loss = 0.2544 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:54:59.727865 140528830564096 learning.py:507] global step 1110: loss = 0.2542 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:00.622905 140528830564096 learning.py:507] global step 1120: loss = 0.2542 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:01.510772 140528830564096 learning.py:507] global step 1130: loss = 0.2541 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:02.382786 140528830564096 learning.py:507] global step 1140: loss = 0.2542 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:03.255239 140528830564096 learning.py:507] global step 1150: loss = 0.2544 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:04.153940 140528830564096 learning.py:507] global step 1160: loss = 0.2545 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:05.043165 140528830564096 learning.py:507] global step 1170: loss = 0.2542 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:05.923152 140528830564096 learning.py:507] global step 1180: loss = 0.2541 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:06.802551 140528830564096 learning.py:507] global step 1190: loss = 0.2543 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:07.702951 140528830564096 learning.py:507] global step 1200: loss = 0.2541 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:08.587380 140528830564096 learning.py:507] global step 1210: loss = 0.2544 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:09.469984 140528830564096 learning.py:507] global step 1220: loss = 0.2543 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:10.348995 140528830564096 learning.py:507] global step 1230: loss = 0.2542 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:11.226860 140528830564096 learning.py:507] global step 1240: loss = 0.2542 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:12.097018 140528830564096 learning.py:507] global step 1250: loss = 0.2543 (0.081 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:12.970628 140528830564096 learning.py:507] global step 1260: loss = 0.2542 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:13.858314 140528830564096 learning.py:507] global step 1270: loss = 0.2541 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:14.770093 140528830564096 learning.py:507] global step 1280: loss = 0.2544 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:15.657253 140528830564096 learning.py:507] global step 1290: loss = 0.2542 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:16.537738 140528830564096 learning.py:507] global step 1300: loss = 0.2542 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:17.420459 140528830564096 learning.py:507] global step 1310: loss = 0.2541 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:18.303813 140528830564096 learning.py:507] global step 1320: loss = 0.2540 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:19.180274 140528830564096 learning.py:507] global step 1330: loss = 0.2541 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:20.061923 140528830564096 learning.py:507] global step 1340: loss = 0.2541 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:20.929771 140528830564096 learning.py:507] global step 1350: loss = 0.2540 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:21.812223 140528830564096 learning.py:507] global step 1360: loss = 0.2541 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:22.700978 140528830564096 learning.py:507] global step 1370: loss = 0.2540 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:23.586401 140528830564096 learning.py:507] global step 1380: loss = 0.2540 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:24.472848 140528830564096 learning.py:507] global step 1390: loss = 0.2541 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:25.359488 140528830564096 learning.py:507] global step 1400: loss = 0.2543 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:26.244560 140528830564096 learning.py:507] global step 1410: loss = 0.2539 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:27.116460 140528830564096 learning.py:507] global step 1420: loss = 0.2540 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:28.005873 140528830564096 learning.py:507] global step 1430: loss = 0.2541 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:28.870011 140528830564096 learning.py:507] global step 1440: loss = 0.2540 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:29.757021 140528830564096 learning.py:507] global step 1450: loss = 0.2541 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:30.633986 140528830564096 learning.py:507] global step 1460: loss = 0.2540 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:31.529026 140528830564096 learning.py:507] global step 1470: loss = 0.2539 (0.093 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:32.411969 140528830564096 learning.py:507] global step 1480: loss = 0.2540 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:33.291455 140528830564096 learning.py:507] global step 1490: loss = 0.2539 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:34.183307 140528830564096 learning.py:507] global step 1500: loss = 0.2540 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:35.093190 140528830564096 learning.py:507] global step 1510: loss = 0.2541 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:35.960133 140528830564096 learning.py:507] global step 1520: loss = 0.2541 (0.088 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0613 23:55:36.825424 140528830564096 learning.py:507] global step 1530: loss = 0.2538 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:37.715444 140528830564096 learning.py:507] global step 1540: loss = 0.2540 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:38.597652 140528830564096 learning.py:507] global step 1550: loss = 0.2538 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:39.493225 140528830564096 learning.py:507] global step 1560: loss = 0.2539 (0.094 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:40.386609 140528830564096 learning.py:507] global step 1570: loss = 0.2540 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:41.275981 140528830564096 learning.py:507] global step 1580: loss = 0.2539 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:42.145951 140528830564096 learning.py:507] global step 1590: loss = 0.2539 (0.081 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:43.034599 140528830564096 learning.py:507] global step 1600: loss = 0.2539 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:43.923887 140528830564096 learning.py:507] global step 1610: loss = 0.2539 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:44.830070 140528830564096 learning.py:507] global step 1620: loss = 0.2539 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:45.728262 140528830564096 learning.py:507] global step 1630: loss = 0.2537 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:46.605550 140528830564096 learning.py:507] global step 1640: loss = 0.2538 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:47.495809 140528830564096 learning.py:507] global step 1650: loss = 0.2541 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:48.376733 140528830564096 learning.py:507] global step 1660: loss = 0.2536 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:49.262326 140528830564096 learning.py:507] global step 1670: loss = 0.2539 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:50.172583 140528830564096 learning.py:507] global step 1680: loss = 0.2538 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:51.057945 140528830564096 learning.py:507] global step 1690: loss = 0.2538 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:51.952367 140528830564096 learning.py:507] global step 1700: loss = 0.2538 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:52.836762 140528830564096 learning.py:507] global step 1710: loss = 0.2540 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:53.722004 140528830564096 learning.py:507] global step 1720: loss = 0.2537 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:54.601196 140528830564096 learning.py:507] global step 1730: loss = 0.2538 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:55.489094 140528830564096 learning.py:507] global step 1740: loss = 0.2538 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:56.358129 140528830564096 learning.py:507] global step 1750: loss = 0.2537 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:57.239063 140528830564096 learning.py:507] global step 1760: loss = 0.2537 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:58.114603 140528830564096 learning.py:507] global step 1770: loss = 0.2539 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:59.011580 140528830564096 learning.py:507] global step 1780: loss = 0.2538 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:55:59.877311 140528830564096 learning.py:507] global step 1790: loss = 0.2538 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:00.755506 140528830564096 learning.py:507] global step 1800: loss = 0.2537 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:01.627221 140528830564096 learning.py:507] global step 1810: loss = 0.2538 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:02.521476 140528830564096 learning.py:507] global step 1820: loss = 0.2536 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:03.401066 140528830564096 learning.py:507] global step 1830: loss = 0.2538 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:04.259157 140528830564096 learning.py:507] global step 1840: loss = 0.2538 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:05.148005 140528830564096 learning.py:507] global step 1850: loss = 0.2536 (0.093 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:06.018281 140528830564096 learning.py:507] global step 1860: loss = 0.2537 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:06.899307 140528830564096 learning.py:507] global step 1870: loss = 0.2535 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:07.776001 140528830564096 learning.py:507] global step 1880: loss = 0.2536 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:08.662097 140528830564096 learning.py:507] global step 1890: loss = 0.2538 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:09.538712 140528830564096 learning.py:507] global step 1900: loss = 0.2536 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:10.419337 140528830564096 learning.py:507] global step 1910: loss = 0.2535 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:11.289345 140528830564096 learning.py:507] global step 1920: loss = 0.2536 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:12.165385 140528830564096 learning.py:507] global step 1930: loss = 0.2534 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:13.039227 140528830564096 learning.py:507] global step 1940: loss = 0.2535 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:13.930881 140528830564096 learning.py:507] global step 1950: loss = 0.2536 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:14.813504 140528830564096 learning.py:507] global step 1960: loss = 0.2536 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:15.702044 140528830564096 learning.py:507] global step 1970: loss = 0.2536 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:16.571723 140528830564096 learning.py:507] global step 1980: loss = 0.2535 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:17.450725 140528830564096 learning.py:507] global step 1990: loss = 0.2539 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:18.334887 140528830564096 learning.py:507] global step 2000: loss = 0.2537 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:19.232180 140528830564096 learning.py:507] global step 2010: loss = 0.2535 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:20.121248 140528830564096 learning.py:507] global step 2020: loss = 0.2536 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:20.994751 140528830564096 learning.py:507] global step 2030: loss = 0.2536 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:21.856765 140528830564096 learning.py:507] global step 2040: loss = 0.2537 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:22.730201 140528830564096 learning.py:507] global step 2050: loss = 0.2537 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:23.615118 140528830564096 learning.py:507] global step 2060: loss = 0.2537 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:24.486700 140528830564096 learning.py:507] global step 2070: loss = 0.2537 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:25.375798 140528830564096 learning.py:507] global step 2080: loss = 0.2535 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:26.256089 140528830564096 learning.py:507] global step 2090: loss = 0.2535 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:27.134670 140528830564096 learning.py:507] global step 2100: loss = 0.2534 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:28.019164 140528830564096 learning.py:507] global step 2110: loss = 0.2536 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:28.905769 140528830564096 learning.py:507] global step 2120: loss = 0.2535 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:29.796962 140528830564096 learning.py:507] global step 2130: loss = 0.2535 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:30.672435 140528830564096 learning.py:507] global step 2140: loss = 0.2535 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:31.573790 140528830564096 learning.py:507] global step 2150: loss = 0.2537 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:32.454875 140528830564096 learning.py:507] global step 2160: loss = 0.2535 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:33.331057 140528830564096 learning.py:507] global step 2170: loss = 0.2536 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:34.214477 140528830564096 learning.py:507] global step 2180: loss = 0.2535 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:35.120811 140528830564096 learning.py:507] global step 2190: loss = 0.2536 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:36.003684 140528830564096 learning.py:507] global step 2200: loss = 0.2534 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:36.907529 140528830564096 learning.py:507] global step 2210: loss = 0.2536 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:37.780572 140528830564096 learning.py:507] global step 2220: loss = 0.2536 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:38.667088 140528830564096 learning.py:507] global step 2230: loss = 0.2535 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:39.557936 140528830564096 learning.py:507] global step 2240: loss = 0.2534 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:40.439707 140528830564096 learning.py:507] global step 2250: loss = 0.2535 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:41.330174 140528830564096 learning.py:507] global step 2260: loss = 0.2536 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:42.220155 140528830564096 learning.py:507] global step 2270: loss = 0.2535 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:43.104119 140528830564096 learning.py:507] global step 2280: loss = 0.2534 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:43.989001 140528830564096 learning.py:507] global step 2290: loss = 0.2534 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:44.877065 140528830564096 learning.py:507] global step 2300: loss = 0.2535 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:45.745995 140528830564096 learning.py:507] global step 2310: loss = 0.2534 (0.085 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0613 23:56:46.622405 140528830564096 learning.py:507] global step 2320: loss = 0.2535 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:47.510406 140528830564096 learning.py:507] global step 2330: loss = 0.2534 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:48.389355 140528830564096 learning.py:507] global step 2340: loss = 0.2534 (0.094 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:49.268731 140528830564096 learning.py:507] global step 2350: loss = 0.2535 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:50.138856 140528830564096 learning.py:507] global step 2360: loss = 0.2535 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:51.005786 140528830564096 learning.py:507] global step 2370: loss = 0.2536 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:51.883419 140528830564096 learning.py:507] global step 2380: loss = 0.2534 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:52.768472 140528830564096 learning.py:507] global step 2390: loss = 0.2533 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:53.642479 140528830564096 learning.py:507] global step 2400: loss = 0.2533 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:54.546895 140528830564096 learning.py:507] global step 2410: loss = 0.2534 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:55.424688 140528830564096 learning.py:507] global step 2420: loss = 0.2535 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:56.312469 140528830564096 learning.py:507] global step 2430: loss = 0.2534 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:57.185199 140528830564096 learning.py:507] global step 2440: loss = 0.2534 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:58.073812 140528830564096 learning.py:507] global step 2450: loss = 0.2534 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:58.963639 140528830564096 learning.py:507] global step 2460: loss = 0.2536 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:56:59.851469 140528830564096 learning.py:507] global step 2470: loss = 0.2536 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:00.731725 140528830564096 learning.py:507] global step 2480: loss = 0.2534 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:01.614743 140528830564096 learning.py:507] global step 2490: loss = 0.2533 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:02.514035 140528830564096 learning.py:507] global step 2500: loss = 0.2534 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:03.384943 140528830564096 learning.py:507] global step 2510: loss = 0.2533 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:04.254842 140528830564096 learning.py:507] global step 2520: loss = 0.2533 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:05.122560 140528830564096 learning.py:507] global step 2530: loss = 0.2533 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:05.986825 140528830564096 learning.py:507] global step 2540: loss = 0.2536 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:06.871957 140528830564096 learning.py:507] global step 2550: loss = 0.2534 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:07.755027 140528830564096 learning.py:507] global step 2560: loss = 0.2534 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:08.626785 140528830564096 learning.py:507] global step 2570: loss = 0.2534 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:09.524209 140528830564096 learning.py:507] global step 2580: loss = 0.2534 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:10.424515 140528830564096 learning.py:507] global step 2590: loss = 0.2533 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:11.301553 140528830564096 learning.py:507] global step 2600: loss = 0.2533 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:12.193679 140528830564096 learning.py:507] global step 2610: loss = 0.2533 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:13.088706 140528830564096 learning.py:507] global step 2620: loss = 0.2535 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:13.980832 140528830564096 learning.py:507] global step 2630: loss = 0.2534 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:14.853494 140528830564096 learning.py:507] global step 2640: loss = 0.2533 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:15.750405 140528830564096 learning.py:507] global step 2650: loss = 0.2533 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:16.645163 140528830564096 learning.py:507] global step 2660: loss = 0.2532 (0.096 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:17.507222 140528830564096 learning.py:507] global step 2670: loss = 0.2534 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:18.398569 140528830564096 learning.py:507] global step 2680: loss = 0.2535 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:19.278207 140528830564096 learning.py:507] global step 2690: loss = 0.2534 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:20.149205 140528830564096 learning.py:507] global step 2700: loss = 0.2533 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:21.018214 140528830564096 learning.py:507] global step 2710: loss = 0.2532 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:21.892370 140528830564096 learning.py:507] global step 2720: loss = 0.2534 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:22.780503 140528830564096 learning.py:507] global step 2730: loss = 0.2532 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:23.677424 140528830564096 learning.py:507] global step 2740: loss = 0.2534 (0.093 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:24.569952 140528830564096 learning.py:507] global step 2750: loss = 0.2534 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:25.456688 140528830564096 learning.py:507] global step 2760: loss = 0.2534 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:26.333072 140528830564096 learning.py:507] global step 2770: loss = 0.2533 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:27.202151 140528830564096 learning.py:507] global step 2780: loss = 0.2532 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:28.097543 140528830564096 learning.py:507] global step 2790: loss = 0.2533 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:28.982019 140528830564096 learning.py:507] global step 2800: loss = 0.2533 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:29.845726 140528830564096 learning.py:507] global step 2810: loss = 0.2533 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:30.710423 140528830564096 learning.py:507] global step 2820: loss = 0.2533 (0.080 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:31.586491 140528830564096 learning.py:507] global step 2830: loss = 0.2532 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:32.453020 140528830564096 learning.py:507] global step 2840: loss = 0.2533 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:33.350630 140528830564096 learning.py:507] global step 2850: loss = 0.2532 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:34.215672 140528830564096 learning.py:507] global step 2860: loss = 0.2533 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:35.102125 140528830564096 learning.py:507] global step 2870: loss = 0.2533 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:35.986196 140528830564096 learning.py:507] global step 2880: loss = 0.2534 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:36.869234 140528830564096 learning.py:507] global step 2890: loss = 0.2534 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:37.749025 140528830564096 learning.py:507] global step 2900: loss = 0.2532 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:38.627847 140528830564096 learning.py:507] global step 2910: loss = 0.2532 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:39.506732 140528830564096 learning.py:507] global step 2920: loss = 0.2533 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:40.380626 140528830564096 learning.py:507] global step 2930: loss = 0.2533 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:41.257189 140528830564096 learning.py:507] global step 2940: loss = 0.2534 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:42.150054 140528830564096 learning.py:507] global step 2950: loss = 0.2532 (0.093 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:43.029184 140528830564096 learning.py:507] global step 2960: loss = 0.2532 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:43.912806 140528830564096 learning.py:507] global step 2970: loss = 0.2532 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:44.804482 140528830564096 learning.py:507] global step 2980: loss = 0.2532 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:45.702521 140528830564096 learning.py:507] global step 2990: loss = 0.2534 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:46.592348 140528830564096 learning.py:507] global step 3000: loss = 0.2532 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:47.464752 140528830564096 learning.py:507] global step 3010: loss = 0.2531 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:48.341747 140528830564096 learning.py:507] global step 3020: loss = 0.2532 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:49.236134 140528830564096 learning.py:507] global step 3030: loss = 0.2533 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:50.106256 140528830564096 learning.py:507] global step 3040: loss = 0.2533 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:50.989376 140528830564096 learning.py:507] global step 3050: loss = 0.2532 (0.081 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:51.863924 140528830564096 learning.py:507] global step 3060: loss = 0.2532 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:52.747683 140528830564096 learning.py:507] global step 3070: loss = 0.2531 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:53.639448 140528830564096 learning.py:507] global step 3080: loss = 0.2533 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:54.518659 140528830564096 learning.py:507] global step 3090: loss = 0.2533 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:55.420063 140528830564096 learning.py:507] global step 3100: loss = 0.2532 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:56.311964 140528830564096 learning.py:507] global step 3110: loss = 0.2532 (0.089 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0613 23:57:57.201728 140528830564096 learning.py:507] global step 3120: loss = 0.2530 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:58.097754 140528830564096 learning.py:507] global step 3130: loss = 0.2531 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:58.974200 140528830564096 learning.py:507] global step 3140: loss = 0.2532 (0.081 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:57:59.859714 140528830564096 learning.py:507] global step 3150: loss = 0.2533 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:00.726707 140528830564096 learning.py:507] global step 3160: loss = 0.2532 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:01.601855 140528830564096 learning.py:507] global step 3170: loss = 0.2531 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:02.503983 140528830564096 learning.py:507] global step 3180: loss = 0.2531 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:03.392554 140528830564096 learning.py:507] global step 3190: loss = 0.2532 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:04.265435 140528830564096 learning.py:507] global step 3200: loss = 0.2532 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:05.146959 140528830564096 learning.py:507] global step 3210: loss = 0.2532 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:06.032834 140528830564096 learning.py:507] global step 3220: loss = 0.2532 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:06.924104 140528830564096 learning.py:507] global step 3230: loss = 0.2532 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:07.806721 140528830564096 learning.py:507] global step 3240: loss = 0.2531 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:08.692126 140528830564096 learning.py:507] global step 3250: loss = 0.2533 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:09.563977 140528830564096 learning.py:507] global step 3260: loss = 0.2532 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:10.441303 140528830564096 learning.py:507] global step 3270: loss = 0.2530 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:11.301791 140528830564096 learning.py:507] global step 3280: loss = 0.2531 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:12.198312 140528830564096 learning.py:507] global step 3290: loss = 0.2532 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:13.081231 140528830564096 learning.py:507] global step 3300: loss = 0.2532 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:13.969186 140528830564096 learning.py:507] global step 3310: loss = 0.2531 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:14.567974 140523520116480 supervisor.py:1050] Recording summary at step 3312.\u001b[0m\n",
      "\u001b[34mI0613 23:58:14.953699 140523528509184 supervisor.py:1099] global_step/sec: 11.0948\u001b[0m\n",
      "\u001b[34mI0613 23:58:15.220356 140528830564096 learning.py:507] global step 3320: loss = 0.2531 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:16.090455 140528830564096 learning.py:507] global step 3330: loss = 0.2533 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:16.977188 140528830564096 learning.py:507] global step 3340: loss = 0.2532 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:17.851763 140528830564096 learning.py:507] global step 3350: loss = 0.2530 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:18.732124 140528830564096 learning.py:507] global step 3360: loss = 0.2532 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:19.618828 140528830564096 learning.py:507] global step 3370: loss = 0.2532 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:20.504046 140528830564096 learning.py:507] global step 3380: loss = 0.2531 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:21.402069 140528830564096 learning.py:507] global step 3390: loss = 0.2530 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:22.279422 140528830564096 learning.py:507] global step 3400: loss = 0.2531 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:23.162089 140528830564096 learning.py:507] global step 3410: loss = 0.2532 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:24.049695 140528830564096 learning.py:507] global step 3420: loss = 0.2531 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:24.940648 140528830564096 learning.py:507] global step 3430: loss = 0.2531 (0.099 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:25.820334 140528830564096 learning.py:507] global step 3440: loss = 0.2531 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:26.695041 140528830564096 learning.py:507] global step 3450: loss = 0.2530 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:27.579724 140528830564096 learning.py:507] global step 3460: loss = 0.2532 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:28.468054 140528830564096 learning.py:507] global step 3470: loss = 0.2532 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:29.344961 140528830564096 learning.py:507] global step 3480: loss = 0.2532 (0.081 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:30.213697 140528830564096 learning.py:507] global step 3490: loss = 0.2531 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:31.104650 140528830564096 learning.py:507] global step 3500: loss = 0.2532 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:31.991550 140528830564096 learning.py:507] global step 3510: loss = 0.2531 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:32.865572 140528830564096 learning.py:507] global step 3520: loss = 0.2531 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:33.756722 140528830564096 learning.py:507] global step 3530: loss = 0.2531 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:34.636612 140528830564096 learning.py:507] global step 3540: loss = 0.2530 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:35.517858 140528830564096 learning.py:507] global step 3550: loss = 0.2531 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:36.414491 140528830564096 learning.py:507] global step 3560: loss = 0.2531 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:37.295790 140528830564096 learning.py:507] global step 3570: loss = 0.2531 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:38.177999 140528830564096 learning.py:507] global step 3580: loss = 0.2532 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:39.062375 140528830564096 learning.py:507] global step 3590: loss = 0.2530 (0.093 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:39.933425 140528830564096 learning.py:507] global step 3600: loss = 0.2530 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:40.820602 140528830564096 learning.py:507] global step 3610: loss = 0.2530 (0.093 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:41.706959 140528830564096 learning.py:507] global step 3620: loss = 0.2531 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:42.588819 140528830564096 learning.py:507] global step 3630: loss = 0.2531 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:43.462841 140528830564096 learning.py:507] global step 3640: loss = 0.2530 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:44.344006 140528830564096 learning.py:507] global step 3650: loss = 0.2530 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:45.207643 140528830564096 learning.py:507] global step 3660: loss = 0.2530 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:46.092813 140528830564096 learning.py:507] global step 3670: loss = 0.2531 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:46.979842 140528830564096 learning.py:507] global step 3680: loss = 0.2530 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:47.860747 140528830564096 learning.py:507] global step 3690: loss = 0.2530 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:48.747823 140528830564096 learning.py:507] global step 3700: loss = 0.2530 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:49.628648 140528830564096 learning.py:507] global step 3710: loss = 0.2530 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:50.527178 140528830564096 learning.py:507] global step 3720: loss = 0.2530 (0.093 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:51.427819 140528830564096 learning.py:507] global step 3730: loss = 0.2531 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:52.306972 140528830564096 learning.py:507] global step 3740: loss = 0.2530 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:53.201687 140528830564096 learning.py:507] global step 3750: loss = 0.2530 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:54.091247 140528830564096 learning.py:507] global step 3760: loss = 0.2530 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:54.976614 140528830564096 learning.py:507] global step 3770: loss = 0.2531 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:55.859047 140528830564096 learning.py:507] global step 3780: loss = 0.2530 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:56.780416 140528830564096 learning.py:507] global step 3790: loss = 0.2531 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:57.665958 140528830564096 learning.py:507] global step 3800: loss = 0.2529 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:58.539139 140528830564096 learning.py:507] global step 3810: loss = 0.2530 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:58:59.407609 140528830564096 learning.py:507] global step 3820: loss = 0.2530 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:00.283229 140528830564096 learning.py:507] global step 3830: loss = 0.2531 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:01.167869 140528830564096 learning.py:507] global step 3840: loss = 0.2531 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:02.053728 140528830564096 learning.py:507] global step 3850: loss = 0.2530 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:02.938127 140528830564096 learning.py:507] global step 3860: loss = 0.2529 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:03.806936 140528830564096 learning.py:507] global step 3870: loss = 0.2529 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:04.681727 140528830564096 learning.py:507] global step 3880: loss = 0.2529 (0.080 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:05.565434 140528830564096 learning.py:507] global step 3890: loss = 0.2531 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:06.434961 140528830564096 learning.py:507] global step 3900: loss = 0.2529 (0.085 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0613 23:59:07.303495 140528830564096 learning.py:507] global step 3910: loss = 0.2530 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:08.195570 140528830564096 learning.py:507] global step 3920: loss = 0.2530 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:09.104314 140528830564096 learning.py:507] global step 3930: loss = 0.2531 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:09.988754 140528830564096 learning.py:507] global step 3940: loss = 0.2529 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:10.887833 140528830564096 learning.py:507] global step 3950: loss = 0.2530 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:11.777565 140528830564096 learning.py:507] global step 3960: loss = 0.2529 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:12.670198 140528830564096 learning.py:507] global step 3970: loss = 0.2530 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:13.556504 140528830564096 learning.py:507] global step 3980: loss = 0.2530 (0.093 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:14.439172 140528830564096 learning.py:507] global step 3990: loss = 0.2530 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:15.323282 140528830564096 learning.py:507] global step 4000: loss = 0.2530 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:16.209729 140528830564096 learning.py:507] global step 4010: loss = 0.2530 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:17.108340 140528830564096 learning.py:507] global step 4020: loss = 0.2530 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:17.994302 140528830564096 learning.py:507] global step 4030: loss = 0.2530 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:18.887031 140528830564096 learning.py:507] global step 4040: loss = 0.2530 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:19.777302 140528830564096 learning.py:507] global step 4050: loss = 0.2530 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:20.631041 140528830564096 learning.py:507] global step 4060: loss = 0.2530 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:21.517838 140528830564096 learning.py:507] global step 4070: loss = 0.2530 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:22.381326 140528830564096 learning.py:507] global step 4080: loss = 0.2531 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:23.278316 140528830564096 learning.py:507] global step 4090: loss = 0.2529 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:24.157032 140528830564096 learning.py:507] global step 4100: loss = 0.2530 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:25.043950 140528830564096 learning.py:507] global step 4110: loss = 0.2529 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:25.922686 140528830564096 learning.py:507] global step 4120: loss = 0.2531 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:26.820125 140528830564096 learning.py:507] global step 4130: loss = 0.2530 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:27.698589 140528830564096 learning.py:507] global step 4140: loss = 0.2530 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:28.583523 140528830564096 learning.py:507] global step 4150: loss = 0.2529 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:29.488130 140528830564096 learning.py:507] global step 4160: loss = 0.2530 (0.094 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:30.364320 140528830564096 learning.py:507] global step 4170: loss = 0.2529 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:31.235959 140528830564096 learning.py:507] global step 4180: loss = 0.2528 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:32.115588 140528830564096 learning.py:507] global step 4190: loss = 0.2530 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:32.998178 140528830564096 learning.py:507] global step 4200: loss = 0.2529 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:33.862627 140528830564096 learning.py:507] global step 4210: loss = 0.2528 (0.080 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:34.761603 140528830564096 learning.py:507] global step 4220: loss = 0.2530 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:35.628645 140528830564096 learning.py:507] global step 4230: loss = 0.2529 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:36.520499 140528830564096 learning.py:507] global step 4240: loss = 0.2531 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:37.386490 140528830564096 learning.py:507] global step 4250: loss = 0.2529 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:38.280960 140528830564096 learning.py:507] global step 4260: loss = 0.2529 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:39.162566 140528830564096 learning.py:507] global step 4270: loss = 0.2529 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:40.027555 140528830564096 learning.py:507] global step 4280: loss = 0.2529 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:40.901721 140528830564096 learning.py:507] global step 4290: loss = 0.2530 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:41.780284 140528830564096 learning.py:507] global step 4300: loss = 0.2528 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:42.663847 140528830564096 learning.py:507] global step 4310: loss = 0.2530 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:43.543459 140528830564096 learning.py:507] global step 4320: loss = 0.2531 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:44.423661 140528830564096 learning.py:507] global step 4330: loss = 0.2529 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:45.319746 140528830564096 learning.py:507] global step 4340: loss = 0.2529 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:46.199054 140528830564096 learning.py:507] global step 4350: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:47.099230 140528830564096 learning.py:507] global step 4360: loss = 0.2528 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:47.974246 140528830564096 learning.py:507] global step 4370: loss = 0.2529 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:48.854890 140528830564096 learning.py:507] global step 4380: loss = 0.2530 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:49.748784 140528830564096 learning.py:507] global step 4390: loss = 0.2529 (0.094 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:50.655975 140528830564096 learning.py:507] global step 4400: loss = 0.2529 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:51.570791 140528830564096 learning.py:507] global step 4410: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:52.475265 140528830564096 learning.py:507] global step 4420: loss = 0.2530 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:53.351905 140528830564096 learning.py:507] global step 4430: loss = 0.2529 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:54.211853 140528830564096 learning.py:507] global step 4440: loss = 0.2528 (0.080 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:55.098332 140528830564096 learning.py:507] global step 4450: loss = 0.2528 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:55.964623 140528830564096 learning.py:507] global step 4460: loss = 0.2530 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:56.850348 140528830564096 learning.py:507] global step 4470: loss = 0.2529 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:57.741329 140528830564096 learning.py:507] global step 4480: loss = 0.2530 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:58.621366 140528830564096 learning.py:507] global step 4490: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0613 23:59:59.503096 140528830564096 learning.py:507] global step 4500: loss = 0.2529 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:00.372078 140528830564096 learning.py:507] global step 4510: loss = 0.2530 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:01.256975 140528830564096 learning.py:507] global step 4520: loss = 0.2529 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:02.145481 140528830564096 learning.py:507] global step 4530: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:03.033859 140528830564096 learning.py:507] global step 4540: loss = 0.2528 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:03.920427 140528830564096 learning.py:507] global step 4550: loss = 0.2529 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:04.813121 140528830564096 learning.py:507] global step 4560: loss = 0.2529 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:05.693275 140528830564096 learning.py:507] global step 4570: loss = 0.2528 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:06.572895 140528830564096 learning.py:507] global step 4580: loss = 0.2528 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:07.464101 140528830564096 learning.py:507] global step 4590: loss = 0.2529 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:08.343169 140528830564096 learning.py:507] global step 4600: loss = 0.2529 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:09.226371 140528830564096 learning.py:507] global step 4610: loss = 0.2529 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:10.110641 140528830564096 learning.py:507] global step 4620: loss = 0.2527 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:11.007740 140528830564096 learning.py:507] global step 4630: loss = 0.2528 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:11.879955 140528830564096 learning.py:507] global step 4640: loss = 0.2528 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:12.751074 140528830564096 learning.py:507] global step 4650: loss = 0.2528 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:13.618055 140528830564096 learning.py:507] global step 4660: loss = 0.2529 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:14.489238 140528830564096 learning.py:507] global step 4670: loss = 0.2529 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:15.366168 140528830564096 learning.py:507] global step 4680: loss = 0.2530 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:16.247613 140528830564096 learning.py:507] global step 4690: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:17.144973 140528830564096 learning.py:507] global step 4700: loss = 0.2529 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:18.030498 140528830564096 learning.py:507] global step 4710: loss = 0.2528 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:18.916827 140528830564096 learning.py:507] global step 4720: loss = 0.2528 (0.094 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:19.795218 140528830564096 learning.py:507] global step 4730: loss = 0.2529 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:20.688220 140528830564096 learning.py:507] global step 4740: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:21.592394 140528830564096 learning.py:507] global step 4750: loss = 0.2528 (0.090 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0614 00:00:22.478682 140528830564096 learning.py:507] global step 4760: loss = 0.2529 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:23.355639 140528830564096 learning.py:507] global step 4770: loss = 0.2529 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:24.245773 140528830564096 learning.py:507] global step 4780: loss = 0.2529 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:25.118259 140528830564096 learning.py:507] global step 4790: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:25.997298 140528830564096 learning.py:507] global step 4800: loss = 0.2528 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:26.894279 140528830564096 learning.py:507] global step 4810: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:27.777420 140528830564096 learning.py:507] global step 4820: loss = 0.2528 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:28.669300 140528830564096 learning.py:507] global step 4830: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:29.562195 140528830564096 learning.py:507] global step 4840: loss = 0.2529 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:30.449807 140528830564096 learning.py:507] global step 4850: loss = 0.2528 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:31.321310 140528830564096 learning.py:507] global step 4860: loss = 0.2528 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:32.221887 140528830564096 learning.py:507] global step 4870: loss = 0.2528 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:33.099452 140528830564096 learning.py:507] global step 4880: loss = 0.2528 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:33.960000 140528830564096 learning.py:507] global step 4890: loss = 0.2529 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:34.848028 140528830564096 learning.py:507] global step 4900: loss = 0.2529 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:35.730268 140528830564096 learning.py:507] global step 4910: loss = 0.2528 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:36.613739 140528830564096 learning.py:507] global step 4920: loss = 0.2528 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:37.506505 140528830564096 learning.py:507] global step 4930: loss = 0.2528 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:38.394318 140528830564096 learning.py:507] global step 4940: loss = 0.2529 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:39.283507 140528830564096 learning.py:507] global step 4950: loss = 0.2528 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:40.179425 140528830564096 learning.py:507] global step 4960: loss = 0.2528 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:41.062916 140528830564096 learning.py:507] global step 4970: loss = 0.2527 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:41.982758 140528830564096 learning.py:507] global step 4980: loss = 0.2528 (0.119 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:42.861273 140528830564096 learning.py:507] global step 4990: loss = 0.2528 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:43.740367 140528830564096 learning.py:507] global step 5000: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:44.621140 140528830564096 learning.py:507] global step 5010: loss = 0.2527 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:45.501162 140528830564096 learning.py:507] global step 5020: loss = 0.2528 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:46.377474 140528830564096 learning.py:507] global step 5030: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:47.282499 140528830564096 learning.py:507] global step 5040: loss = 0.2529 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:48.167476 140528830564096 learning.py:507] global step 5050: loss = 0.2528 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:49.062958 140528830564096 learning.py:507] global step 5060: loss = 0.2527 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:49.977890 140528830564096 learning.py:507] global step 5070: loss = 0.2528 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:50.889518 140528830564096 learning.py:507] global step 5080: loss = 0.2529 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:51.762604 140528830564096 learning.py:507] global step 5090: loss = 0.2527 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:52.645864 140528830564096 learning.py:507] global step 5100: loss = 0.2528 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:53.522233 140528830564096 learning.py:507] global step 5110: loss = 0.2527 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:54.396590 140528830564096 learning.py:507] global step 5120: loss = 0.2528 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:55.260632 140528830564096 learning.py:507] global step 5130: loss = 0.2529 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:56.128092 140528830564096 learning.py:507] global step 5140: loss = 0.2528 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:56.997136 140528830564096 learning.py:507] global step 5150: loss = 0.2528 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:57.886008 140528830564096 learning.py:507] global step 5160: loss = 0.2528 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:58.761521 140528830564096 learning.py:507] global step 5170: loss = 0.2528 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:00:59.642252 140528830564096 learning.py:507] global step 5180: loss = 0.2528 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:00.535558 140528830564096 learning.py:507] global step 5190: loss = 0.2527 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:01.416390 140528830564096 learning.py:507] global step 5200: loss = 0.2527 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:02.330091 140528830564096 learning.py:507] global step 5210: loss = 0.2527 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:03.204448 140528830564096 learning.py:507] global step 5220: loss = 0.2527 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:04.075989 140528830564096 learning.py:507] global step 5230: loss = 0.2527 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:04.947540 140528830564096 learning.py:507] global step 5240: loss = 0.2527 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:05.825449 140528830564096 learning.py:507] global step 5250: loss = 0.2527 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:06.707492 140528830564096 learning.py:507] global step 5260: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:07.593533 140528830564096 learning.py:507] global step 5270: loss = 0.2529 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:08.480994 140528830564096 learning.py:507] global step 5280: loss = 0.2527 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:09.348794 140528830564096 learning.py:507] global step 5290: loss = 0.2528 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:10.236320 140528830564096 learning.py:507] global step 5300: loss = 0.2528 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:11.112584 140528830564096 learning.py:507] global step 5310: loss = 0.2527 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:11.992820 140528830564096 learning.py:507] global step 5320: loss = 0.2527 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:12.888478 140528830564096 learning.py:507] global step 5330: loss = 0.2529 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:13.779382 140528830564096 learning.py:507] global step 5340: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:14.668593 140528830564096 learning.py:507] global step 5350: loss = 0.2527 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:15.546585 140528830564096 learning.py:507] global step 5360: loss = 0.2527 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:16.432616 140528830564096 learning.py:507] global step 5370: loss = 0.2527 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:17.306925 140528830564096 learning.py:507] global step 5380: loss = 0.2528 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:18.172662 140528830564096 learning.py:507] global step 5390: loss = 0.2527 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:19.038475 140528830564096 learning.py:507] global step 5400: loss = 0.2527 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:19.916434 140528830564096 learning.py:507] global step 5410: loss = 0.2527 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:20.814415 140528830564096 learning.py:507] global step 5420: loss = 0.2527 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:21.717400 140528830564096 learning.py:507] global step 5430: loss = 0.2526 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:22.595841 140528830564096 learning.py:507] global step 5440: loss = 0.2527 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:23.485922 140528830564096 learning.py:507] global step 5450: loss = 0.2527 (0.093 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:24.369437 140528830564096 learning.py:507] global step 5460: loss = 0.2528 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:25.242424 140528830564096 learning.py:507] global step 5470: loss = 0.2528 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:26.128129 140528830564096 learning.py:507] global step 5480: loss = 0.2527 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:27.003639 140528830564096 learning.py:507] global step 5490: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:27.893909 140528830564096 learning.py:507] global step 5500: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:28.769047 140528830564096 learning.py:507] global step 5510: loss = 0.2527 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:29.646188 140528830564096 learning.py:507] global step 5520: loss = 0.2528 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:30.547990 140528830564096 learning.py:507] global step 5530: loss = 0.2527 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:31.433917 140528830564096 learning.py:507] global step 5540: loss = 0.2528 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:32.317900 140528830564096 learning.py:507] global step 5550: loss = 0.2527 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:33.204327 140528830564096 learning.py:507] global step 5560: loss = 0.2526 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:34.091396 140528830564096 learning.py:507] global step 5570: loss = 0.2528 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:34.986135 140528830564096 learning.py:507] global step 5580: loss = 0.2528 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:35.876492 140528830564096 learning.py:507] global step 5590: loss = 0.2527 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:36.744815 140528830564096 learning.py:507] global step 5600: loss = 0.2527 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:37.642138 140528830564096 learning.py:507] global step 5610: loss = 0.2527 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:38.542840 140528830564096 learning.py:507] global step 5620: loss = 0.2526 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:39.444322 140528830564096 learning.py:507] global step 5630: loss = 0.2527 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:40.320133 140528830564096 learning.py:507] global step 5640: loss = 0.2527 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:41.207401 140528830564096 learning.py:507] global step 5650: loss = 0.2527 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:42.080969 140528830564096 learning.py:507] global step 5660: loss = 0.2527 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:42.962627 140528830564096 learning.py:507] global step 5670: loss = 0.2528 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:43.842391 140528830564096 learning.py:507] global step 5680: loss = 0.2527 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:44.723434 140528830564096 learning.py:507] global step 5690: loss = 0.2528 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:45.600681 140528830564096 learning.py:507] global step 5700: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:46.476922 140528830564096 learning.py:507] global step 5710: loss = 0.2528 (0.101 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:47.365932 140528830564096 learning.py:507] global step 5720: loss = 0.2528 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:48.247905 140528830564096 learning.py:507] global step 5730: loss = 0.2527 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:49.116082 140528830564096 learning.py:507] global step 5740: loss = 0.2528 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:49.991976 140528830564096 learning.py:507] global step 5750: loss = 0.2528 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:50.895545 140528830564096 learning.py:507] global step 5760: loss = 0.2527 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:51.782428 140528830564096 learning.py:507] global step 5770: loss = 0.2527 (0.085 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0614 00:01:52.656939 140528830564096 learning.py:507] global step 5780: loss = 0.2527 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:53.552510 140528830564096 learning.py:507] global step 5790: loss = 0.2527 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:54.424245 140528830564096 learning.py:507] global step 5800: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:55.307262 140528830564096 learning.py:507] global step 5810: loss = 0.2528 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:56.186045 140528830564096 learning.py:507] global step 5820: loss = 0.2527 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:57.071253 140528830564096 learning.py:507] global step 5830: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:57.945009 140528830564096 learning.py:507] global step 5840: loss = 0.2527 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:58.824727 140528830564096 learning.py:507] global step 5850: loss = 0.2527 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:01:59.711532 140528830564096 learning.py:507] global step 5860: loss = 0.2528 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:00.590284 140528830564096 learning.py:507] global step 5870: loss = 0.2528 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:01.464885 140528830564096 learning.py:507] global step 5880: loss = 0.2527 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:02.350243 140528830564096 learning.py:507] global step 5890: loss = 0.2527 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:03.235300 140528830564096 learning.py:507] global step 5900: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:04.122292 140528830564096 learning.py:507] global step 5910: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:05.013425 140528830564096 learning.py:507] global step 5920: loss = 0.2526 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:05.900144 140528830564096 learning.py:507] global step 5930: loss = 0.2527 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:06.780252 140528830564096 learning.py:507] global step 5940: loss = 0.2526 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:07.654536 140528830564096 learning.py:507] global step 5950: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:08.526323 140528830564096 learning.py:507] global step 5960: loss = 0.2526 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:09.408099 140528830564096 learning.py:507] global step 5970: loss = 0.2526 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:10.281369 140528830564096 learning.py:507] global step 5980: loss = 0.2528 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:11.166951 140528830564096 learning.py:507] global step 5990: loss = 0.2526 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:12.051847 140528830564096 learning.py:507] global step 6000: loss = 0.2527 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:12.919054 140528830564096 learning.py:507] global step 6010: loss = 0.2526 (0.097 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:13.794799 140528830564096 learning.py:507] global step 6020: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:14.671039 140528830564096 learning.py:507] global step 6030: loss = 0.2527 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:15.552364 140528830564096 learning.py:507] global step 6040: loss = 0.2527 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:16.441268 140528830564096 learning.py:507] global step 6050: loss = 0.2527 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:17.314629 140528830564096 learning.py:507] global step 6060: loss = 0.2527 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:18.201483 140528830564096 learning.py:507] global step 6070: loss = 0.2527 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:19.090460 140528830564096 learning.py:507] global step 6080: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:19.964735 140528830564096 learning.py:507] global step 6090: loss = 0.2527 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:20.836980 140528830564096 learning.py:507] global step 6100: loss = 0.2527 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:21.724203 140528830564096 learning.py:507] global step 6110: loss = 0.2527 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:22.607563 140528830564096 learning.py:507] global step 6120: loss = 0.2527 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:23.500955 140528830564096 learning.py:507] global step 6130: loss = 0.2527 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:24.385765 140528830564096 learning.py:507] global step 6140: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:25.261717 140528830564096 learning.py:507] global step 6150: loss = 0.2526 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:26.152622 140528830564096 learning.py:507] global step 6160: loss = 0.2526 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:27.043625 140528830564096 learning.py:507] global step 6170: loss = 0.2527 (0.093 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:27.927651 140528830564096 learning.py:507] global step 6180: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:28.808197 140528830564096 learning.py:507] global step 6190: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:29.709355 140528830564096 learning.py:507] global step 6200: loss = 0.2527 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:30.590887 140528830564096 learning.py:507] global step 6210: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:31.493352 140528830564096 learning.py:507] global step 6220: loss = 0.2527 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:32.371022 140528830564096 learning.py:507] global step 6230: loss = 0.2527 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:33.256138 140528830564096 learning.py:507] global step 6240: loss = 0.2526 (0.094 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:34.144187 140528830564096 learning.py:507] global step 6250: loss = 0.2526 (0.094 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:35.040160 140528830564096 learning.py:507] global step 6260: loss = 0.2527 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:35.921922 140528830564096 learning.py:507] global step 6270: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:36.807049 140528830564096 learning.py:507] global step 6280: loss = 0.2528 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:37.685199 140528830564096 learning.py:507] global step 6290: loss = 0.2527 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:38.548923 140528830564096 learning.py:507] global step 6300: loss = 0.2527 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:39.440232 140528830564096 learning.py:507] global step 6310: loss = 0.2527 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:40.322880 140528830564096 learning.py:507] global step 6320: loss = 0.2527 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:41.208604 140528830564096 learning.py:507] global step 6330: loss = 0.2526 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:42.076622 140528830564096 learning.py:507] global step 6340: loss = 0.2527 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:42.955938 140528830564096 learning.py:507] global step 6350: loss = 0.2527 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:43.834394 140528830564096 learning.py:507] global step 6360: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:44.731482 140528830564096 learning.py:507] global step 6370: loss = 0.2527 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:45.616280 140528830564096 learning.py:507] global step 6380: loss = 0.2527 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:46.486092 140528830564096 learning.py:507] global step 6390: loss = 0.2526 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:47.371989 140528830564096 learning.py:507] global step 6400: loss = 0.2527 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:48.252336 140528830564096 learning.py:507] global step 6410: loss = 0.2526 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:49.123508 140528830564096 learning.py:507] global step 6420: loss = 0.2526 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:50.005234 140528830564096 learning.py:507] global step 6430: loss = 0.2526 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:50.882796 140528830564096 learning.py:507] global step 6440: loss = 0.2526 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:51.762290 140528830564096 learning.py:507] global step 6450: loss = 0.2526 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:52.644826 140528830564096 learning.py:507] global step 6460: loss = 0.2526 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:53.540250 140528830564096 learning.py:507] global step 6470: loss = 0.2526 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:54.426861 140528830564096 learning.py:507] global step 6480: loss = 0.2526 (0.095 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:55.304858 140528830564096 learning.py:507] global step 6490: loss = 0.2525 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:56.186932 140528830564096 learning.py:507] global step 6500: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:57.070663 140528830564096 learning.py:507] global step 6510: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:57.982726 140528830564096 learning.py:507] global step 6520: loss = 0.2526 (0.107 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:58.862521 140528830564096 learning.py:507] global step 6530: loss = 0.2526 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:02:59.753518 140528830564096 learning.py:507] global step 6540: loss = 0.2526 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:00.638137 140528830564096 learning.py:507] global step 6550: loss = 0.2526 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:01.525536 140528830564096 learning.py:507] global step 6560: loss = 0.2527 (0.090 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0614 00:03:02.424354 140528830564096 learning.py:507] global step 6570: loss = 0.2526 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:03.307883 140528830564096 learning.py:507] global step 6580: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:04.187731 140528830564096 learning.py:507] global step 6590: loss = 0.2526 (0.097 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:05.061335 140528830564096 learning.py:507] global step 6600: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:05.939274 140528830564096 learning.py:507] global step 6610: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:06.821885 140528830564096 learning.py:507] global step 6620: loss = 0.2527 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:07.723145 140528830564096 learning.py:507] global step 6630: loss = 0.2526 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:08.614053 140528830564096 learning.py:507] global step 6640: loss = 0.2526 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:09.509030 140528830564096 learning.py:507] global step 6650: loss = 0.2527 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:10.407037 140528830564096 learning.py:507] global step 6660: loss = 0.2526 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:11.300278 140528830564096 learning.py:507] global step 6670: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:12.182672 140528830564096 learning.py:507] global step 6680: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:13.057919 140528830564096 learning.py:507] global step 6690: loss = 0.2528 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:13.926327 140528830564096 learning.py:507] global step 6700: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:14.009289 140523536901888 supervisor.py:1117] Saving checkpoint to path /opt/ml/model/model.ckpt\u001b[0m\n",
      "\u001b[34mI0614 00:03:14.612822 140523520116480 supervisor.py:1050] Recording summary at step 6703.\u001b[0m\n",
      "\u001b[34mI0614 00:03:14.988934 140523528509184 supervisor.py:1099] global_step/sec: 11.2953\u001b[0m\n",
      "\u001b[34mI0614 00:03:15.351761 140528830564096 learning.py:507] global step 6710: loss = 0.2526 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:16.237529 140528830564096 learning.py:507] global step 6720: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:17.113877 140528830564096 learning.py:507] global step 6730: loss = 0.2526 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:17.993262 140528830564096 learning.py:507] global step 6740: loss = 0.2525 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:18.894675 140528830564096 learning.py:507] global step 6750: loss = 0.2527 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:19.769113 140528830564096 learning.py:507] global step 6760: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:20.642215 140528830564096 learning.py:507] global step 6770: loss = 0.2526 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:21.531992 140528830564096 learning.py:507] global step 6780: loss = 0.2527 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:22.418071 140528830564096 learning.py:507] global step 6790: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:23.310549 140528830564096 learning.py:507] global step 6800: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:24.199619 140528830564096 learning.py:507] global step 6810: loss = 0.2526 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:25.078682 140528830564096 learning.py:507] global step 6820: loss = 0.2526 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:25.960607 140528830564096 learning.py:507] global step 6830: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:26.844897 140528830564096 learning.py:507] global step 6840: loss = 0.2527 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:27.739443 140528830564096 learning.py:507] global step 6850: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:28.623063 140528830564096 learning.py:507] global step 6860: loss = 0.2526 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:29.519454 140528830564096 learning.py:507] global step 6870: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:30.389622 140528830564096 learning.py:507] global step 6880: loss = 0.2526 (0.080 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:31.276269 140528830564096 learning.py:507] global step 6890: loss = 0.2527 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:32.158750 140528830564096 learning.py:507] global step 6900: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:33.046231 140528830564096 learning.py:507] global step 6910: loss = 0.2527 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:33.920200 140528830564096 learning.py:507] global step 6920: loss = 0.2527 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:34.798949 140528830564096 learning.py:507] global step 6930: loss = 0.2526 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:35.679182 140528830564096 learning.py:507] global step 6940: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:36.563150 140528830564096 learning.py:507] global step 6950: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:37.450848 140528830564096 learning.py:507] global step 6960: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:38.331875 140528830564096 learning.py:507] global step 6970: loss = 0.2526 (0.097 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:39.206479 140528830564096 learning.py:507] global step 6980: loss = 0.2527 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:40.077037 140528830564096 learning.py:507] global step 6990: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:40.944408 140528830564096 learning.py:507] global step 7000: loss = 0.2525 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:41.842525 140528830564096 learning.py:507] global step 7010: loss = 0.2526 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:42.727377 140528830564096 learning.py:507] global step 7020: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:43.611803 140528830564096 learning.py:507] global step 7030: loss = 0.2526 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:44.501245 140528830564096 learning.py:507] global step 7040: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:45.388408 140528830564096 learning.py:507] global step 7050: loss = 0.2525 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:46.273822 140528830564096 learning.py:507] global step 7060: loss = 0.2526 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:47.136117 140528830564096 learning.py:507] global step 7070: loss = 0.2526 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:48.030793 140528830564096 learning.py:507] global step 7080: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:48.923892 140528830564096 learning.py:507] global step 7090: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:49.809597 140528830564096 learning.py:507] global step 7100: loss = 0.2525 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:50.689233 140528830564096 learning.py:507] global step 7110: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:51.585794 140528830564096 learning.py:507] global step 7120: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:52.461754 140528830564096 learning.py:507] global step 7130: loss = 0.2525 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:53.342392 140528830564096 learning.py:507] global step 7140: loss = 0.2526 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:54.225230 140528830564096 learning.py:507] global step 7150: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:55.140348 140528830564096 learning.py:507] global step 7160: loss = 0.2525 (0.094 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:56.015870 140528830564096 learning.py:507] global step 7170: loss = 0.2527 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:56.897197 140528830564096 learning.py:507] global step 7180: loss = 0.2526 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:57.793021 140528830564096 learning.py:507] global step 7190: loss = 0.2526 (0.093 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:58.710808 140528830564096 learning.py:507] global step 7200: loss = 0.2525 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:03:59.597840 140528830564096 learning.py:507] global step 7210: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:00.483735 140528830564096 learning.py:507] global step 7220: loss = 0.2525 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:01.357886 140528830564096 learning.py:507] global step 7230: loss = 0.2527 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:02.240884 140528830564096 learning.py:507] global step 7240: loss = 0.2525 (0.093 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:03.114522 140528830564096 learning.py:507] global step 7250: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:04.009988 140528830564096 learning.py:507] global step 7260: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:04.907497 140528830564096 learning.py:507] global step 7270: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:05.789191 140528830564096 learning.py:507] global step 7280: loss = 0.2525 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:06.669080 140528830564096 learning.py:507] global step 7290: loss = 0.2527 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:07.546000 140528830564096 learning.py:507] global step 7300: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:08.433032 140528830564096 learning.py:507] global step 7310: loss = 0.2526 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:09.316098 140528830564096 learning.py:507] global step 7320: loss = 0.2525 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:10.230957 140528830564096 learning.py:507] global step 7330: loss = 0.2525 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:11.098143 140528830564096 learning.py:507] global step 7340: loss = 0.2525 (0.085 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0614 00:04:11.980973 140528830564096 learning.py:507] global step 7350: loss = 0.2525 (0.081 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:12.852768 140528830564096 learning.py:507] global step 7360: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:13.741362 140528830564096 learning.py:507] global step 7370: loss = 0.2525 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:14.623773 140528830564096 learning.py:507] global step 7380: loss = 0.2525 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:15.510663 140528830564096 learning.py:507] global step 7390: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:16.387057 140528830564096 learning.py:507] global step 7400: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:17.266175 140528830564096 learning.py:507] global step 7410: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:18.157604 140528830564096 learning.py:507] global step 7420: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:19.048063 140528830564096 learning.py:507] global step 7430: loss = 0.2526 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:19.935017 140528830564096 learning.py:507] global step 7440: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:20.823041 140528830564096 learning.py:507] global step 7450: loss = 0.2526 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:21.719564 140528830564096 learning.py:507] global step 7460: loss = 0.2526 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:22.602057 140528830564096 learning.py:507] global step 7470: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:23.476610 140528830564096 learning.py:507] global step 7480: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:24.366642 140528830564096 learning.py:507] global step 7490: loss = 0.2524 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:25.250827 140528830564096 learning.py:507] global step 7500: loss = 0.2525 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:26.148075 140528830564096 learning.py:507] global step 7510: loss = 0.2525 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:27.033060 140528830564096 learning.py:507] global step 7520: loss = 0.2526 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:27.918001 140528830564096 learning.py:507] global step 7530: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:28.840873 140528830564096 learning.py:507] global step 7540: loss = 0.2525 (0.108 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:29.722275 140528830564096 learning.py:507] global step 7550: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:30.596074 140528830564096 learning.py:507] global step 7560: loss = 0.2526 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:31.479615 140528830564096 learning.py:507] global step 7570: loss = 0.2526 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:32.365432 140528830564096 learning.py:507] global step 7580: loss = 0.2526 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:33.250477 140528830564096 learning.py:507] global step 7590: loss = 0.2526 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:34.141599 140528830564096 learning.py:507] global step 7600: loss = 0.2525 (0.093 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:35.030216 140528830564096 learning.py:507] global step 7610: loss = 0.2525 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:35.906749 140528830564096 learning.py:507] global step 7620: loss = 0.2525 (0.081 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:36.789324 140528830564096 learning.py:507] global step 7630: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:37.674954 140528830564096 learning.py:507] global step 7640: loss = 0.2526 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:38.538716 140528830564096 learning.py:507] global step 7650: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:39.432323 140528830564096 learning.py:507] global step 7660: loss = 0.2525 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:40.304742 140528830564096 learning.py:507] global step 7670: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:41.186067 140528830564096 learning.py:507] global step 7680: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:42.052824 140528830564096 learning.py:507] global step 7690: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:42.916975 140528830564096 learning.py:507] global step 7700: loss = 0.2526 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:43.811418 140528830564096 learning.py:507] global step 7710: loss = 0.2526 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:44.703207 140528830564096 learning.py:507] global step 7720: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:45.588272 140528830564096 learning.py:507] global step 7730: loss = 0.2525 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:46.466984 140528830564096 learning.py:507] global step 7740: loss = 0.2525 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:47.351794 140528830564096 learning.py:507] global step 7750: loss = 0.2525 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:48.234186 140528830564096 learning.py:507] global step 7760: loss = 0.2525 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:49.126826 140528830564096 learning.py:507] global step 7770: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:50.011833 140528830564096 learning.py:507] global step 7780: loss = 0.2525 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:50.901379 140528830564096 learning.py:507] global step 7790: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:51.777842 140528830564096 learning.py:507] global step 7800: loss = 0.2526 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:52.667954 140528830564096 learning.py:507] global step 7810: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:53.550657 140528830564096 learning.py:507] global step 7820: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:54.455569 140528830564096 learning.py:507] global step 7830: loss = 0.2525 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:55.334472 140528830564096 learning.py:507] global step 7840: loss = 0.2526 (0.081 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:56.217447 140528830564096 learning.py:507] global step 7850: loss = 0.2525 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:57.075818 140528830564096 learning.py:507] global step 7860: loss = 0.2526 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:57.946408 140528830564096 learning.py:507] global step 7870: loss = 0.2525 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:58.826510 140528830564096 learning.py:507] global step 7880: loss = 0.2525 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:04:59.700350 140528830564096 learning.py:507] global step 7890: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:00.585458 140528830564096 learning.py:507] global step 7900: loss = 0.2525 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:01.460429 140528830564096 learning.py:507] global step 7910: loss = 0.2525 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:02.357949 140528830564096 learning.py:507] global step 7920: loss = 0.2527 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:03.231260 140528830564096 learning.py:507] global step 7930: loss = 0.2525 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:04.112504 140528830564096 learning.py:507] global step 7940: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:05.001048 140528830564096 learning.py:507] global step 7950: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:05.896610 140528830564096 learning.py:507] global step 7960: loss = 0.2525 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:06.761663 140528830564096 learning.py:507] global step 7970: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:07.650669 140528830564096 learning.py:507] global step 7980: loss = 0.2524 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:08.532254 140528830564096 learning.py:507] global step 7990: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:09.402084 140528830564096 learning.py:507] global step 8000: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:10.277735 140528830564096 learning.py:507] global step 8010: loss = 0.2526 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:11.154464 140528830564096 learning.py:507] global step 8020: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:12.047498 140528830564096 learning.py:507] global step 8030: loss = 0.2526 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:12.932037 140528830564096 learning.py:507] global step 8040: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:13.828207 140528830564096 learning.py:507] global step 8050: loss = 0.2525 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:14.704488 140528830564096 learning.py:507] global step 8060: loss = 0.2524 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:15.596230 140528830564096 learning.py:507] global step 8070: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:16.476011 140528830564096 learning.py:507] global step 8080: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:17.379183 140528830564096 learning.py:507] global step 8090: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:18.273838 140528830564096 learning.py:507] global step 8100: loss = 0.2525 (0.094 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:19.151406 140528830564096 learning.py:507] global step 8110: loss = 0.2525 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:20.045160 140528830564096 learning.py:507] global step 8120: loss = 0.2525 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:20.930629 140528830564096 learning.py:507] global step 8130: loss = 0.2525 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:21.815418 140528830564096 learning.py:507] global step 8140: loss = 0.2524 (0.085 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0614 00:05:22.727808 140528830564096 learning.py:507] global step 8150: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:23.627501 140528830564096 learning.py:507] global step 8160: loss = 0.2525 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:24.518642 140528830564096 learning.py:507] global step 8170: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:25.404161 140528830564096 learning.py:507] global step 8180: loss = 0.2524 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:26.285270 140528830564096 learning.py:507] global step 8190: loss = 0.2525 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:27.175021 140528830564096 learning.py:507] global step 8200: loss = 0.2524 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:28.060034 140528830564096 learning.py:507] global step 8210: loss = 0.2525 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:28.932909 140528830564096 learning.py:507] global step 8220: loss = 0.2526 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:29.806554 140528830564096 learning.py:507] global step 8230: loss = 0.2525 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:30.686496 140528830564096 learning.py:507] global step 8240: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:31.568135 140528830564096 learning.py:507] global step 8250: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:32.451313 140528830564096 learning.py:507] global step 8260: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:33.326155 140528830564096 learning.py:507] global step 8270: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:34.214584 140528830564096 learning.py:507] global step 8280: loss = 0.2524 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:35.105921 140528830564096 learning.py:507] global step 8290: loss = 0.2525 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:35.989078 140528830564096 learning.py:507] global step 8300: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:36.868340 140528830564096 learning.py:507] global step 8310: loss = 0.2524 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:37.752990 140528830564096 learning.py:507] global step 8320: loss = 0.2524 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:38.637455 140528830564096 learning.py:507] global step 8330: loss = 0.2525 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:39.533817 140528830564096 learning.py:507] global step 8340: loss = 0.2525 (0.100 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:40.405065 140528830564096 learning.py:507] global step 8350: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:41.304929 140528830564096 learning.py:507] global step 8360: loss = 0.2525 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:42.184592 140528830564096 learning.py:507] global step 8370: loss = 0.2524 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:43.054785 140528830564096 learning.py:507] global step 8380: loss = 0.2524 (0.079 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:43.928324 140528830564096 learning.py:507] global step 8390: loss = 0.2524 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:44.820200 140528830564096 learning.py:507] global step 8400: loss = 0.2524 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:45.719851 140528830564096 learning.py:507] global step 8410: loss = 0.2524 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:46.607026 140528830564096 learning.py:507] global step 8420: loss = 0.2524 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:47.479924 140528830564096 learning.py:507] global step 8430: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:48.367928 140528830564096 learning.py:507] global step 8440: loss = 0.2525 (0.095 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:49.239841 140528830564096 learning.py:507] global step 8450: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:50.119923 140528830564096 learning.py:507] global step 8460: loss = 0.2526 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:51.002403 140528830564096 learning.py:507] global step 8470: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:51.884720 140528830564096 learning.py:507] global step 8480: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:52.772773 140528830564096 learning.py:507] global step 8490: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:53.646349 140528830564096 learning.py:507] global step 8500: loss = 0.2526 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:54.531841 140528830564096 learning.py:507] global step 8510: loss = 0.2524 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:55.405535 140528830564096 learning.py:507] global step 8520: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:56.282433 140528830564096 learning.py:507] global step 8530: loss = 0.2525 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:57.167916 140528830564096 learning.py:507] global step 8540: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:58.071202 140528830564096 learning.py:507] global step 8550: loss = 0.2525 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:58.956299 140528830564096 learning.py:507] global step 8560: loss = 0.2523 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:05:59.833667 140528830564096 learning.py:507] global step 8570: loss = 0.2525 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:00.711625 140528830564096 learning.py:507] global step 8580: loss = 0.2525 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:01.589463 140528830564096 learning.py:507] global step 8590: loss = 0.2525 (0.081 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:02.482304 140528830564096 learning.py:507] global step 8600: loss = 0.2525 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:03.365936 140528830564096 learning.py:507] global step 8610: loss = 0.2524 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:04.250860 140528830564096 learning.py:507] global step 8620: loss = 0.2524 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:05.133023 140528830564096 learning.py:507] global step 8630: loss = 0.2524 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:06.016120 140528830564096 learning.py:507] global step 8640: loss = 0.2524 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:06.890044 140528830564096 learning.py:507] global step 8650: loss = 0.2526 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:07.773360 140528830564096 learning.py:507] global step 8660: loss = 0.2525 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:08.663367 140528830564096 learning.py:507] global step 8670: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:09.538103 140528830564096 learning.py:507] global step 8680: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:10.440412 140528830564096 learning.py:507] global step 8690: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:11.329566 140528830564096 learning.py:507] global step 8700: loss = 0.2524 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:12.210408 140528830564096 learning.py:507] global step 8710: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:13.089344 140528830564096 learning.py:507] global step 8720: loss = 0.2525 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:13.979959 140528830564096 learning.py:507] global step 8730: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:14.866677 140528830564096 learning.py:507] global step 8740: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:15.750154 140528830564096 learning.py:507] global step 8750: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:16.636956 140528830564096 learning.py:507] global step 8760: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:17.510211 140528830564096 learning.py:507] global step 8770: loss = 0.2524 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:18.396622 140528830564096 learning.py:507] global step 8780: loss = 0.2526 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:19.289148 140528830564096 learning.py:507] global step 8790: loss = 0.2525 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:20.189743 140528830564096 learning.py:507] global step 8800: loss = 0.2524 (0.094 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:21.082730 140528830564096 learning.py:507] global step 8810: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:21.965322 140528830564096 learning.py:507] global step 8820: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:22.848820 140528830564096 learning.py:507] global step 8830: loss = 0.2524 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:23.731602 140528830564096 learning.py:507] global step 8840: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:24.618215 140528830564096 learning.py:507] global step 8850: loss = 0.2526 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:25.502732 140528830564096 learning.py:507] global step 8860: loss = 0.2524 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:26.397530 140528830564096 learning.py:507] global step 8870: loss = 0.2525 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:27.283574 140528830564096 learning.py:507] global step 8880: loss = 0.2525 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:28.154557 140528830564096 learning.py:507] global step 8890: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:29.027872 140528830564096 learning.py:507] global step 8900: loss = 0.2524 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:29.903064 140528830564096 learning.py:507] global step 8910: loss = 0.2524 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:30.776804 140528830564096 learning.py:507] global step 8920: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:31.656058 140528830564096 learning.py:507] global step 8930: loss = 0.2524 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:32.537903 140528830564096 learning.py:507] global step 8940: loss = 0.2524 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:33.430670 140528830564096 learning.py:507] global step 8950: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:34.322177 140528830564096 learning.py:507] global step 8960: loss = 0.2524 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:35.216470 140528830564096 learning.py:507] global step 8970: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:36.095875 140528830564096 learning.py:507] global step 8980: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:36.963534 140528830564096 learning.py:507] global step 8990: loss = 0.2525 (0.088 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0614 00:06:37.849422 140528830564096 learning.py:507] global step 9000: loss = 0.2525 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:38.717046 140528830564096 learning.py:507] global step 9010: loss = 0.2524 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:39.588855 140528830564096 learning.py:507] global step 9020: loss = 0.2525 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:40.477314 140528830564096 learning.py:507] global step 9030: loss = 0.2524 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:41.359535 140528830564096 learning.py:507] global step 9040: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:42.239653 140528830564096 learning.py:507] global step 9050: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:43.121418 140528830564096 learning.py:507] global step 9060: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:43.992183 140528830564096 learning.py:507] global step 9070: loss = 0.2524 (0.080 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:44.872035 140528830564096 learning.py:507] global step 9080: loss = 0.2524 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:45.754724 140528830564096 learning.py:507] global step 9090: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:46.644069 140528830564096 learning.py:507] global step 9100: loss = 0.2524 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:47.532785 140528830564096 learning.py:507] global step 9110: loss = 0.2524 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:48.404544 140528830564096 learning.py:507] global step 9120: loss = 0.2524 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:49.296327 140528830564096 learning.py:507] global step 9130: loss = 0.2525 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:50.175894 140528830564096 learning.py:507] global step 9140: loss = 0.2524 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:51.084939 140528830564096 learning.py:507] global step 9150: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:51.961009 140528830564096 learning.py:507] global step 9160: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:52.876184 140528830564096 learning.py:507] global step 9170: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:53.759639 140528830564096 learning.py:507] global step 9180: loss = 0.2525 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:54.632750 140528830564096 learning.py:507] global step 9190: loss = 0.2525 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:55.503567 140528830564096 learning.py:507] global step 9200: loss = 0.2524 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:56.381726 140528830564096 learning.py:507] global step 9210: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:57.251311 140528830564096 learning.py:507] global step 9220: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:58.120285 140528830564096 learning.py:507] global step 9230: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:59.002684 140528830564096 learning.py:507] global step 9240: loss = 0.2525 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:06:59.892070 140528830564096 learning.py:507] global step 9250: loss = 0.2523 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:00.794553 140528830564096 learning.py:507] global step 9260: loss = 0.2524 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:01.677587 140528830564096 learning.py:507] global step 9270: loss = 0.2524 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:02.566798 140528830564096 learning.py:507] global step 9280: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:03.442739 140528830564096 learning.py:507] global step 9290: loss = 0.2524 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:04.332544 140528830564096 learning.py:507] global step 9300: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:05.208281 140528830564096 learning.py:507] global step 9310: loss = 0.2524 (0.081 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:06.087555 140528830564096 learning.py:507] global step 9320: loss = 0.2524 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:06.971162 140528830564096 learning.py:507] global step 9330: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:07.849559 140528830564096 learning.py:507] global step 9340: loss = 0.2525 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:08.721189 140528830564096 learning.py:507] global step 9350: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:09.599792 140528830564096 learning.py:507] global step 9360: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:10.485060 140528830564096 learning.py:507] global step 9370: loss = 0.2524 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:11.358510 140528830564096 learning.py:507] global step 9380: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:12.230757 140528830564096 learning.py:507] global step 9390: loss = 0.2524 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:13.119244 140528830564096 learning.py:507] global step 9400: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:14.001291 140528830564096 learning.py:507] global step 9410: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:14.887504 140528830564096 learning.py:507] global step 9420: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:15.762389 140528830564096 learning.py:507] global step 9430: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:16.632797 140528830564096 learning.py:507] global step 9440: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:17.500386 140528830564096 learning.py:507] global step 9450: loss = 0.2525 (0.082 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:18.392426 140528830564096 learning.py:507] global step 9460: loss = 0.2525 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:19.267088 140528830564096 learning.py:507] global step 9470: loss = 0.2524 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:20.148583 140528830564096 learning.py:507] global step 9480: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:21.043409 140528830564096 learning.py:507] global step 9490: loss = 0.2524 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:21.925926 140528830564096 learning.py:507] global step 9500: loss = 0.2523 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:22.819585 140528830564096 learning.py:507] global step 9510: loss = 0.2524 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:23.693418 140528830564096 learning.py:507] global step 9520: loss = 0.2523 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:24.581336 140528830564096 learning.py:507] global step 9530: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:25.456120 140528830564096 learning.py:507] global step 9540: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:26.317414 140528830564096 learning.py:507] global step 9550: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:27.206935 140528830564096 learning.py:507] global step 9560: loss = 0.2524 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:28.083621 140528830564096 learning.py:507] global step 9570: loss = 0.2524 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:28.996805 140528830564096 learning.py:507] global step 9580: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:29.881767 140528830564096 learning.py:507] global step 9590: loss = 0.2525 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:30.768763 140528830564096 learning.py:507] global step 9600: loss = 0.2526 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:31.666733 140528830564096 learning.py:507] global step 9610: loss = 0.2524 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:32.552124 140528830564096 learning.py:507] global step 9620: loss = 0.2525 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:33.450344 140528830564096 learning.py:507] global step 9630: loss = 0.2524 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:34.347655 140528830564096 learning.py:507] global step 9640: loss = 0.2524 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:35.241292 140528830564096 learning.py:507] global step 9650: loss = 0.2524 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:36.124955 140528830564096 learning.py:507] global step 9660: loss = 0.2525 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:37.017148 140528830564096 learning.py:507] global step 9670: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:37.909758 140528830564096 learning.py:507] global step 9680: loss = 0.2523 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:38.793020 140528830564096 learning.py:507] global step 9690: loss = 0.2523 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:39.672748 140528830564096 learning.py:507] global step 9700: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:40.536345 140528830564096 learning.py:507] global step 9710: loss = 0.2524 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:41.435411 140528830564096 learning.py:507] global step 9720: loss = 0.2525 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:42.315192 140528830564096 learning.py:507] global step 9730: loss = 0.2525 (0.081 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:43.201216 140528830564096 learning.py:507] global step 9740: loss = 0.2523 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:44.081852 140528830564096 learning.py:507] global step 9750: loss = 0.2525 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:44.964504 140528830564096 learning.py:507] global step 9760: loss = 0.2525 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:45.841041 140528830564096 learning.py:507] global step 9770: loss = 0.2525 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:46.715871 140528830564096 learning.py:507] global step 9780: loss = 0.2524 (0.086 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0614 00:07:47.590030 140528830564096 learning.py:507] global step 9790: loss = 0.2524 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:48.463098 140528830564096 learning.py:507] global step 9800: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:49.354479 140528830564096 learning.py:507] global step 9810: loss = 0.2524 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:50.251137 140528830564096 learning.py:507] global step 9820: loss = 0.2524 (0.090 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:51.144833 140528830564096 learning.py:507] global step 9830: loss = 0.2524 (0.093 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:52.037560 140528830564096 learning.py:507] global step 9840: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:52.928900 140528830564096 learning.py:507] global step 9850: loss = 0.2525 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:53.814305 140528830564096 learning.py:507] global step 9860: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:54.695044 140528830564096 learning.py:507] global step 9870: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:55.578247 140528830564096 learning.py:507] global step 9880: loss = 0.2523 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:56.455842 140528830564096 learning.py:507] global step 9890: loss = 0.2523 (0.083 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:57.329658 140528830564096 learning.py:507] global step 9900: loss = 0.2525 (0.084 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:58.215677 140528830564096 learning.py:507] global step 9910: loss = 0.2524 (0.091 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:59.091988 140528830564096 learning.py:507] global step 9920: loss = 0.2523 (0.088 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:07:59.951391 140528830564096 learning.py:507] global step 9930: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:08:00.840465 140528830564096 learning.py:507] global step 9940: loss = 0.2523 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:08:01.712198 140528830564096 learning.py:507] global step 9950: loss = 0.2524 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:08:02.600696 140528830564096 learning.py:507] global step 9960: loss = 0.2524 (0.085 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:08:03.484078 140528830564096 learning.py:507] global step 9970: loss = 0.2524 (0.086 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:08:04.379383 140528830564096 learning.py:507] global step 9980: loss = 0.2524 (0.089 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:08:05.267608 140528830564096 learning.py:507] global step 9990: loss = 0.2524 (0.087 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:08:06.175121 140528830564096 learning.py:507] global step 10000: loss = 0.2524 (0.092 sec/step)\u001b[0m\n",
      "\u001b[34mI0614 00:08:06.175777 140528830564096 learning.py:777] Stopping Training.\u001b[0m\n",
      "\u001b[34mI0614 00:08:06.175946 140528830564096 learning.py:785] Finished training! Saving model to disk.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/writer/writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.\n",
      "  warnings.warn(\"Attempting to use a closed FileWriter. \"\u001b[0m\n",
      "\u001b[34mW0614 00:08:07.386712 140528830564096 deprecation.py:323] From image_classifier.py:418: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease switch to tf.train.get_or_create_global_step\u001b[0m\n",
      "\u001b[34mfile_pattern : None\u001b[0m\n",
      "\u001b[34mlabels_to_names : {0: u'background', 1: u'dog'}\u001b[0m\n",
      "\u001b[34meval_args.use_grayscale : False\u001b[0m\n",
      "\u001b[34mW0614 00:08:07.501243 140528830564096 deprecation_wrapper.py:119] From /opt/ml/code/preprocessing/inception_preprocessing.py:301: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\n",
      "\u001b[0m\n",
      "\u001b[34mI0614 00:08:07.513164 140528830564096 regularizers.py:98] Scale of 0 disables regularizer.\u001b[0m\n",
      "\u001b[34mW0614 00:08:08.606024 140528830564096 deprecation.py:323] From image_classifier.py:489: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.\u001b[0m\n",
      "\u001b[34mW0614 00:08:08.622009 140528830564096 deprecation.py:323] From image_classifier.py:491: streaming_recall_at_k (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed after 2016-11-08.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease use `streaming_sparse_recall_at_k`, and reshape labels from [batch_size] to [batch_size, 1].\u001b[0m\n",
      "\u001b[34mW0614 00:08:08.624022 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py:2166: streaming_mean (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease switch to tf.metrics.mean\u001b[0m\n",
      "\u001b[34mW0614 00:08:08.639210 140528830564096 deprecation.py:323] From image_classifier.py:498: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\u001b[0m\n",
      "\u001b[34m```python\n",
      "    sess = tf.compat.v1.Session()\n",
      "    with sess.as_default():\n",
      "        tensor = tf.range(10)\n",
      "        print_op = tf.print(tensor)\n",
      "        with tf.control_dependencies([print_op]):\n",
      "          out = tf.add(tensor, tensor)\n",
      "        sess.run(out)\n",
      "    ```\u001b[0m\n",
      "\u001b[34mAdditionally, to use tf.print in python 2.7, users must make sure to import\u001b[0m\n",
      "\u001b[34mthe following:\n",
      "\n",
      "  `from __future__ import print_function`\n",
      "\u001b[0m\n",
      "\u001b[34mI0614 00:08:08.644814 140528830564096 image_classifier.py:514] Evaluating /opt/ml/model/model.ckpt-10000\u001b[0m\n",
      "\u001b[34mI0614 00:08:08.778510 140528830564096 evaluation.py:255] Starting evaluation at 2020-06-14T00:08:08Z\u001b[0m\n",
      "\u001b[34mI0614 00:08:09.019377 140528830564096 monitored_session.py:240] Graph was finalized.\u001b[0m\n",
      "\u001b[34mI0614 00:08:09.023058 140528830564096 saver.py:1286] Restoring parameters from /opt/ml/model/model.ckpt-10000\u001b[0m\n",
      "\u001b[34mI0614 00:08:09.487782 140528830564096 session_manager.py:500] Running local_init_op.\u001b[0m\n",
      "\u001b[34mI0614 00:08:09.517261 140528830564096 session_manager.py:502] Done running local_init_op.\u001b[0m\n",
      "\u001b[34mW0614 00:08:09.725606 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mI0614 00:08:11.320302 140528830564096 evaluation.py:167] Evaluation [1/1]\u001b[0m\n",
      "\u001b[34meval/Recall_5[1]\u001b[0m\n",
      "\u001b[34meval/Accuracy[1]\u001b[0m\n",
      "\u001b[34mI0614 00:08:11.480376 140528830564096 evaluation.py:275] Finished evaluation at 2020-06-14-00:08:11\u001b[0m\n",
      "\u001b[34mfile_pattern : None\u001b[0m\n",
      "\u001b[34mlabels_to_names : {0: u'background', 1: u'dog'}\u001b[0m\n",
      "\u001b[34mI0614 00:08:11.483850 140528830564096 regularizers.py:98] Scale of 0 disables regularizer.\u001b[0m\n",
      "\u001b[34mfreeze_graph input_checkpoint : /opt/ml/model/model.ckpt-10000\u001b[0m\n",
      "\u001b[34mW0614 00:08:12.496515 140528830564096 deprecation_wrapper.py:119] From /opt/ml/code/freeze_graph.py:165: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
      "\u001b[0m\n",
      "\u001b[34mI0614 00:08:12.632437 140528830564096 saver.py:1286] Restoring parameters from /opt/ml/model/model.ckpt-10000\u001b[0m\n",
      "\u001b[34mW0614 00:08:12.854244 140528830564096 deprecation.py:323] From /opt/ml/code/freeze_graph.py:234: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.compat.v1.graph_util.convert_variables_to_constants`\u001b[0m\n",
      "\u001b[34mW0614 00:08:12.854449 140528830564096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.compat.v1.graph_util.extract_sub_graph`\u001b[0m\n",
      "\u001b[34mI0614 00:08:12.975128 140528830564096 graph_util_impl.py:311] Froze 137 variables.\u001b[0m\n",
      "\u001b[34mI0614 00:08:12.995237 140528830564096 graph_util_impl.py:364] Converted 137 variables to const ops.\u001b[0m\n",
      "\u001b[34mWARNING: Logging before flag parsing goes to stderr.\u001b[0m\n",
      "\u001b[34mW0614 00:08:16.860050 139940935632640 training.py:183] Your model will NOT be servable with SageMaker TensorFlow Serving container.The model artifact was not saved in the TensorFlow SavedModel directory structure:\u001b[0m\n",
      "\u001b[34mhttps://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-06-14 00:08:27 Uploading - Uploading generated training model\n",
      "2020-06-14 00:08:27 Completed - Training job completed\n",
      "Training seconds: 999\n",
      "Billable seconds: 300\n",
      "Managed Spot Training savings: 70.0%\n"
     ]
    }
   ],
   "source": [
    "sm_sess = sagemaker.Session()\n",
    "sm_sess.logs_for_job(estimator.latest_training_job.name, wait=True, log_type='All')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>학습이 모두 완료된 다음에 S3에서 모델 산출물을 SageMaker Notebook 환경으로 내려받습니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-143656149352/1592092163-img-classifier-training-job/\n",
      "                           PRE debug-output/\n",
      "                           PRE output/\n",
      "                           PRE source/\n"
     ]
    }
   ],
   "source": [
    "artifacts_dir = estimator.model_dir.replace('model','')\n",
    "print(artifacts_dir)\n",
    "!aws s3 ls --human-readable {artifacts_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-143656149352/1592092163-img-classifier-training-job/output/\n",
      "2020-06-14 00:08:25   11.0 MiB model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_dir=artifacts_dir+'output/'\n",
    "print(model_dir)\n",
    "!aws s3 ls --human-readable {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./model_result/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-143656149352/1592092163-img-classifier-training-job/output/model.tar.gz to model_result/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import json , os\n",
    "\n",
    "path = './model_result'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "!aws s3 cp {model_dir}model.tar.gz {path}/model.tar.gz\n",
    "!tar -xzf {path}/model.tar.gz -C {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>최종 결과물에는 tflite를 생성할 수 있도록 했습니다. 압축을 푼 다음 tflite 를 다시 활용하기 위해 S3에 파일을 upload 합니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: img_datasets/labels.txt to s3://sagemaker-us-east-1-143656149352/workshop_final_result/labels.txt\n",
      "upload: model_result/mobilenetv1_model.tflite to s3://sagemaker-us-east-1-143656149352/workshop_final_result/mobilenetv1_model.tflite\n"
     ]
    }
   ],
   "source": [
    "final_result = 's3://{}/{}'.format(bucket, 'workshop_final_result')\n",
    "\n",
    "!aws s3 cp ./img_datasets/labels.txt {final_result}/labels.txt\n",
    "!aws s3 cp {path}/mobilenetv1_model.tflite {final_result}/mobilenetv1_model.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p></p>\n",
    "<p>Amazon SageMaker에서 모든 학습을 완료하였습니다. 이제 tflite를 이용하여 AI Chip에서 활용할 수 있도록 Convertor를 수행합니다. 이 작업은 Cloud9에서 수행합니다. </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
