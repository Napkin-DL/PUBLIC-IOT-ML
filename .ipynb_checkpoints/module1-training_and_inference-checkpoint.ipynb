{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1. Amazon SageMaker 학습 스크립트\n",
    "\n",
    "<p>이 예제는 LG에서 개발한 AI chip에서 동작할 수 있도록, Tensorflow 1.X, python2.7 버전에서 학습하기 위한 코드입니다. </p>\n",
    "<p>이 코드는 <strong><a href=\"https://github.com/tensorflow/models/tree/master/research/slim\" target=\"_blank\" class ='btn-default'>TensorFlow-Slim image classification model library</a></strong>를 참고하여 Sagemaker에서 학습할 수 있는 실행 스크립트로 수정하여 작성하였습니다. Amazon SageMaker로 실행 스크립트를 구성하는 이유는 노트북의 스크립트에서 일부 파라미터 수정으로 동일 모델 아키텍처에 대해 hyperparamter가 변경된 다양한 모델을 원하는 형태의 다수 인프라에서 동시에 학습 수행이 가능하며, 가장 높은 성능의 모델을 노트북 스크립트 내 명령어로 바로 hosting 서비스가 가능한 Endpoint 생성을 할 수 있습니다.</p>\n",
    "\n",
    "<p>이번 실습에서는 Amazon SageMaker가 어떤 방식으로 학습이 되는지 설명되는 구조와 함께 학습하는 방법을 간단하게 체험해 보는 시간을 갖도록 하겠습니다.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sagemaker notebook 설명\n",
    "<p>Sagemaker notebook은 완전 관리형 서비스로 컨테이너 기반으로 구성되어 있습니다. 사용자가 직접 컨테이너를 볼 수 없지만, 내부적으로는 아래와 같은 원리로 동작합니다. </p>\n",
    "<p><img src=\"./imgs/fig00.png\" width=\"700\", height=\"70\"></p>\n",
    "\n",
    "- **S3 (Simple Storage Serivce)** : Object Storage로서 학습할 데이터 파일과 학습 결과인 model, checkpoint, tensorboard를 위한 event 파일, 로그 정보 등을 저장하는데 사용합니다.\n",
    "- **SageMaker Notebook** : 학습을 위한 스크립트 작성과 디버깅, 그리고 실제 학습을 수행하기 위한 Python을 개발하기 위한 환경을 제공합니다.\n",
    "- **Amazon Elastic Container Registry(ECR)** :  Docker 컨테이너 이미지를 손쉽게 저장, 관리 및 배포할 수 있게 해주는 완전관리형 Docker 컨테이너 레지스트리입니다. Sagemaker는 기본적인 컨테이너를 제공하기 때문에 별도 ECR에 컨테이너 이미지를 등록할 필요는 없습니다. 하지만, 별도의 학습 및 배포 환경이 필요한 경우 custom 컨테이너 이미지를 만들어서 ECR에 등록한 후 이 환경을 활용할 수 있습니다.\n",
    "\n",
    "<p>학습과 추론을 하는 hosting 서비스는 각각 다른 컨테이너 환경에서 수행할 수 있으며, 쉽게 다량으로 컨테이너 환경을 확장할 수 있으므로 다량의 학습과 hosting이 동시에 가능합니다.   \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 환경 설정\n",
    "\n",
    "<p>Sagemaker 학습에 필요한 기본적인 package를 import 합니다. </p>\n",
    "<p>boto3는 HTTP API 호출을 숨기는 편한 추상화 모델을 가지고 있고, Amazon EC2 인스턴스 및 S3 버켓과 같은 AWS 리소스와 동작하는 파이선 클래스를 제공합니다. </p>\n",
    "<p>sagemaker python sdk는 Amazon SageMaker에서 기계 학습 모델을 교육 및 배포하기 위한 오픈 소스 라이브러리입니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install --upgrade pip\n",
    "# !{sys.executable} -m pip install tensorflow_gpu==1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sagemaker\n",
    "import boto3\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>SageMaker에서 앞으로 사용할 SageMaker Session 설정, Role 정보를 설정합니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. S3의 저장 데이터 위치 가져오기\n",
    "<p> 데이터를 정하기 위한 S3의 bucket 위치는 아래 data_bucket 이름으로 생성하며, 기본적으로 SageMaker에서 학습한 모델과 로그 정보를 남기는 위치는 자동으로 생성되는 bucket 이름으로 저장됩니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n"
     ]
    }
   ],
   "source": [
    "# create a s3 bucket to hold data, note that your account might already created a bucket with the same name\n",
    "account_id = sess.client('sts').get_caller_identity()[\"Account\"]\n",
    "data_bucket = 'sagemaker-experiments-{}-{}'.format(sess.region_name, account_id)\n",
    "bucket = 'sagemaker-{}-{}'.format(sess.region_name, account_id)\n",
    "\n",
    "try:\n",
    "    if sess.region_name == \"us-east-1\":\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket)\n",
    "    else:\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket, \n",
    "                                        CreateBucketConfiguration={'LocationConstraint': sess.region_name})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 이미지를 TFRecord 변경하기\n",
    "<p>이미지 파일을 학습하기 위해 SageMaker Notebook 환경으로 upload를 합니다. 폴더 구조는 아래와 같은 형태로 구성되어야 합니다. </p>\n",
    "<pre>\n",
    "<div style='line-height:80%'>\n",
    "    image_path/class1/Aimage_1<br>\n",
    "                      Aimage_2<br>\n",
    "                       ...<br>\n",
    "                      Aimage_N<br>\n",
    "    image_path/class2/Bimage_1<br>\n",
    "                      Bimage_2<br>\n",
    "                       ...<br>\n",
    "                      Bimage_M<br>\n",
    "</div>\n",
    "</pre>\n",
    "<p>생성된 TFRecord 파일은 아래 정의하신 dataset_dir에 저장이 됩니다. train과 test의 데이터 수는 향후 학습에서 활용하기 위해 train_num_data, test_num_data 변수에 저장합니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/ec2-user/SageMaker/PUBLIC-IOT-ML/src_dir/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import image_to_tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/home/ec2-user/SageMaker/PUBLIC-IOT-ML/img_datasets'\n",
    "image_path = '/home/ec2-user/SageMaker/PUBLIC-IOT-ML/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/git_dir/PUBLIC-IOT-ML/src_dir/datasets/image_to_tfrecord.py:157: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/git_dir/PUBLIC-IOT-ML/src_dir/datasets/image_to_tfrecord.py:161: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/git_dir/PUBLIC-IOT-ML/src_dir/datasets/dataset_utils.py:176: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/git_dir/PUBLIC-IOT-ML/src_dir/datasets/image_to_tfrecord.py:195: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/git_dir/PUBLIC-IOT-ML/src_dir/datasets/image_to_tfrecord.py:73: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/git_dir/PUBLIC-IOT-ML/src_dir/datasets/image_to_tfrecord.py:76: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "\n",
      "Finished converting the image dataset!\n"
     ]
    }
   ],
   "source": [
    "train_num_data, test_num_data = image_to_tfrecord.run(image_path, dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TFRecord를 S3에 upload 하기\n",
    "\n",
    "<p>SageMaker 학습을 위해 TFRecord 파일을 S3에 upload합니다. TFRecord 은 이전에 지정한 data_bucket 내 prefix 하위 폴더에 저장됩니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: img_datasets/labels.txt to s3://sagemaker-experiments-us-east-2-322537213286/captured_data/tfrecord/labels.txt\n",
      "upload: img_datasets/captureddata_val.tfrecord to s3://sagemaker-experiments-us-east-2-322537213286/captured_data/tfrecord/captureddata_val.tfrecord\n",
      "upload: img_datasets/captureddata_train.tfrecord to s3://sagemaker-experiments-us-east-2-322537213286/captured_data/tfrecord/captureddata_train.tfrecord\n"
     ]
    }
   ],
   "source": [
    "prefix = 'captured_data/tfrecord'\n",
    "!aws s3 cp $dataset_dir s3://{data_bucket}/{prefix}/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 학습 스크립트 코딩하기\n",
    "\n",
    "<p>SageMaker에서 학습하는 것이 아니더라도 실제 모델 아키텍처와 학습을 위한 optimizer와 loss 함수 등을 정의하는 python 파일을 구성하게 됩니다. SageMaker에서 활용하는 python 파일도 동일한 python 파일을 사용하게 됩니다. 연계되는 다른 소스코드 파일이 있는 경우에도 별도 소스코드 수정 없이 학습이 가능하며, SageMaker에서 사용하기 위해서는 기존 python 파일에 SageMaker 학습에 사용할 수 있는 환경변수들만 추가하면 됩니다. 예를 들어, 환경변수 중 <code>SM_MODEL_DIR</code>은 컨테이너 환경에서는 <code>/opt/ml/model</code>를 의미합니다. 다양한 환경변수는 <strong><a href=\"https://github.com/aws/sagemaker-containers\" target=\"_blank\" class ='btn-default'>SageMaker Containers-IMPORTANT ENVIRONMENT VARIABLES</a></strong>를 참고하시기 바랍니다. </p><p>SageMaker 학습이 끝나면 자동은 컨테이너 환경은 삭제가 됩니다. 따라서, 학습이 완료된 모델 산출물과 다양한 output 파일은 S3로 저장해야 합니다. SageMaker는 자동으로 <code>SM_MODEL_DIR</code>에 저장된 최종 모델 파일을 학습이 끝난 다음 model.tar.gz로 압축하여 컨테이너 환경에서 S3의 특정 bucket에 저장하게 됩니다.</p><p> 별도 bucket을 설정하지 않으며, 기본적으로 생성되는 bucket에 저장됩니다. 이외 학습에 이용되는 python source code는 SageMaker 학습이 시작되면서 S3에 저장되며, 별도로 <code>SM_MODEL_DIR</code>에 checkpoint 또는 log 파일을 저장하게 되면 학습이 끝난 이후 자동으로 컨테이너 환경에서 S3로 저장된 파일들이 이동하게 됩니다. 이런 과정을 이해한다면 학습 시 저장되는 다양한 정보들을 저장한 다음 학습이 끝난 후 S3에서 download 받아 활용할 수 있습니다. </p>\n",
    "\n",
    "<p>아래는 시간 관계 상 미리 작성한 python 학습 스크립트 코드 입니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;49;00m\r\n",
      "\u001b[37m# you may not use this file except in compliance with the License.\u001b[39;49;00m\r\n",
      "\u001b[37m# You may obtain a copy of the License at\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# http://www.apache.org/licenses/LICENSE-2.0\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# Unless required by applicable law or agreed to in writing, software\u001b[39;49;00m\r\n",
      "\u001b[37m# distributed under the License is distributed on an \"AS IS\" BASIS,\u001b[39;49;00m\r\n",
      "\u001b[37m# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[39;49;00m\r\n",
      "\u001b[37m# See the License for the specific language governing permissions and\u001b[39;49;00m\r\n",
      "\u001b[37m# limitations under the License.\u001b[39;49;00m\r\n",
      "\u001b[37m# ==============================================================================\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"Generic training script that trains a model using a given dataset.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m absolute_import, division, print_function\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcodecs\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mPIL\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mfreeze_graph\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mfg\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m dataset_factory\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdeployment\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m model_deploy\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mnets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m nets_factory\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m preprocessing_factory\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcontrib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m quantize \u001b[34mas\u001b[39;49;00m contrib_quantize\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcontrib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m slim \u001b[34mas\u001b[39;49;00m contrib_slim\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcore\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mprotobuf\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m saver_pb2\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpython\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mplatform\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gfile\r\n",
      "\r\n",
      "slim = contrib_slim\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m###############################\u001b[39;49;00m\r\n",
      "    \u001b[37m# SageMaker Default Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m###############################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mDirectory where checkpoints and event logs are written to.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dataset_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m,\r\n",
      "                        default=json.loads(os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-config\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=json.loads,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_INPUT_DATA_CONFIG\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--fw-params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=json.loads,\r\n",
      "                        default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_FRAMEWORK_PARAMS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "\r\n",
      "    \u001b[37m###############################\u001b[39;49;00m\r\n",
      "    \u001b[37m#  Default Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m###############################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--master\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe address of the TensorFlow master to use.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--warmup_epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mLinearly warmup learning rate from 0 to learning_rate over this many epochs.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_clones\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mNumber of model clones to deploy. Note For historical reasons\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mloss from all clones averaged out and learning rate decay happen per clone epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--clone_on_cpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mUse CPUs to deploy clones.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--worker_replicas\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m1\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mNumber of worker replicas.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_ps_tasks\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of parameter servers. If the value is 0,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mthen the parameters are handled locally by the worker.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--task\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mTask id of the replica running the training.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--save_interval_secs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m600\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe frequency with which the model is saved, in seconds.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--save_summaries_secs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m600\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe frequency with which summaries are saved, in seconds.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--log_every_n_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe frequency with which logs are print.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_preprocessing_threads\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of threads used to create the batches.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_readers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of parallel readers that read data from the dataset.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m##########################\u001b[39;49;00m\r\n",
      "    \u001b[37m# Optimization Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m##########################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--adam_beta1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe exponential decay rate for the 1st moment estimates.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--adam_beta2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.999\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe exponential decay rate for the 2nd moment estimates.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--adagrad_initial_accumulator_value\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.1\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mStarting value for the AdaGrad accumulators.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--adadelta_rho\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.95\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe decay rate for adadelta.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--optimizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mrmsprop\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe name of the optimizer, one of \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33madadelta\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33madagrad\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mftrl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mmomentum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33msgd\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m or \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mrmsprop\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--weight_decay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.00004\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe weight decay on the model weights.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--opt_epsilon\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m1.0\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mEpsilon term for the optimizer.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ftrl_learning_rate_power\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=-\u001b[34m0.5\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe learning rate power.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ftrl_initial_accumulator_value\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.1\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mStarting value for the FTRL accumulators.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ftrl_l1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe FTRL l1 regularization strength.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ftrl_l2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe FTRL l2 regularization strength.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe momentum for the MomentumOptimizer and RMSPropOptimizer.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--rmsprop_momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.9\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mMomentum.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--rmsprop_decay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.9\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mDecay term for RMSProp.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--quantize_delay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=-\u001b[34m1\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mNumber of steps to start quantized training.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mSet to -1 would disable quantized training.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m###########################\u001b[39;49;00m\r\n",
      "    \u001b[37m# Learning Rate Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m###########################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate_decay_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mexponential\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mSpecifies how the learning rate is decayed. One of \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mfixed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mexponential\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m or \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mpolynomial\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.01\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mInitial learning rate.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--end_learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.01\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mInitial learning rate.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--label_smoothing\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.1\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe amount of label smoothing.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate_decay_factor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.94\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mLearning rate decay factor.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_epochs_per_decay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m2.0\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mNumber of epochs after which learning rate decays. \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mNote: this flag counts epochs per clone but aggregates per sync replicas.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mSo 1.0 means that each clone will go over full epoch individually, \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mbut replicas will go once across all replicas.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sync_replicas\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhether or not to synchronize the replicas during training.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--replicas_to_aggregate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m1\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe Number of gradients to collect before updating params.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--moving_average_decay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe decay to use for the moving average.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33m If left as None, then moving averages are not used.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m#####################\u001b[39;49;00m\r\n",
      "    \u001b[37m# Dataset Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m#####################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dataset_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[33m'\u001b[39;49;00m\u001b[33mimagenet\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe name of the dataset to load.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[37m# parser.add_argument('--dataset_split_name', type=str,\u001b[39;49;00m\r\n",
      "    \u001b[37m#                     default='train', help='The name of the train/test split.')\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--labels_offset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mAn offset for the labels in the dataset.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mThis flag is primarily used to evaluate the VGG and ResNet \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33marchitectures which do not use a background class for the ImageNet dataset.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[33m'\u001b[39;49;00m\u001b[33minception_v3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe name of the architecture to train.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--preprocessing_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe name of the preprocessing to use.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mIf left as `None`, then the model_name flag is used.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m32\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of samples in each batch.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--image_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mTrain image size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_number_of_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe maximum number of training steps.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_grayscale\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhether to convert input images to grayscale.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m#########################\u001b[39;49;00m\r\n",
      "    \u001b[37m# Fine-Tuning Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m#########################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--finetune_checkpoint_path\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe path to a checkpoint from which to fine-tune.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--checkpoint_exclude_scopes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of scopes of variables\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mto exclude when restoring from a checkpoint.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--trainable_scopes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of scopes to filter the set\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mof variables to train. By default, None would train all the variables.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ignore_missing_vars\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhen restoring a checkpoint would ignore missing variables.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m#########################\u001b[39;49;00m\r\n",
      "    \u001b[37m# evaluation Arguments #\u001b[39;49;00m\r\n",
      "    \u001b[37m#########################\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m100\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of samples in each batch in evaluation.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_num_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of train samples\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_num_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of test samples\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_eval_num_batches\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mMax number of batches to evaluate by default use all.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[37m# parser.add_argument('--num_preprocessing_threads', type=int,\u001b[39;49;00m\r\n",
      "    \u001b[37m#                     default=4, help='The number of threads used to create the batches.')\u001b[39;49;00m\r\n",
      "    \u001b[37m# parser.add_argument('--eval_image_size', type=int,\u001b[39;49;00m\r\n",
      "    \u001b[37m#                     default=None, help='Eval image size')\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--quantize\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mEval image size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--is_training\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhether to save out a training-focused version of the model.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--is_video_model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mwhether to use 5-D inputs for video model.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_frames\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of frames to use. Only used if is_video_model is True.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--write_text_graphdef\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhether to write a text version of graphdef.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    return_value = parser.parse_known_args()\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mparser.parse_known_args() : \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(return_value))\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m return_value\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_configure_learning_rate\u001b[39;49;00m(args, num_samples_per_epoch, global_step):\r\n",
      "    \u001b[33m\"\"\"Configures the learning rate.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m      num_samples_per_epoch: The number of samples in each epoch of training.\u001b[39;49;00m\r\n",
      "\u001b[33m      global_step: The global_step tensor.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m      A `Tensor` representing the learning rate.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Raises:\u001b[39;49;00m\r\n",
      "\u001b[33m      ValueError: if\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[37m# Note: when num_clones is > 1, this will actually have each clone to go\u001b[39;49;00m\r\n",
      "    \u001b[37m# over each epoch args.num_epochs_per_decay times. This is different\u001b[39;49;00m\r\n",
      "    \u001b[37m# behavior from sync replicas and is expected to produce different results.\u001b[39;49;00m\r\n",
      "    steps_per_epoch = num_samples_per_epoch / args.batch_size\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.sync_replicas:\r\n",
      "        steps_per_epoch /= args.replicas_to_aggregate\r\n",
      "\r\n",
      "    decay_steps = \u001b[36mint\u001b[39;49;00m(steps_per_epoch * args.num_epochs_per_decay)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.learning_rate_decay_type == \u001b[33m'\u001b[39;49;00m\u001b[33mexponential\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        learning_rate = tf.train.exponential_decay(\r\n",
      "            args.learning_rate,\r\n",
      "            global_step,\r\n",
      "            decay_steps,\r\n",
      "            args.learning_rate_decay_factor,\r\n",
      "            staircase=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "            name=\u001b[33m'\u001b[39;49;00m\u001b[33mexponential_decay_learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.learning_rate_decay_type == \u001b[33m'\u001b[39;49;00m\u001b[33mfixed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        learning_rate = tf.constant(\r\n",
      "            args.learning_rate, name=\u001b[33m'\u001b[39;49;00m\u001b[33mfixed_learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.learning_rate_decay_type == \u001b[33m'\u001b[39;49;00m\u001b[33mpolynomial\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        learning_rate = tf.train.polynomial_decay(\r\n",
      "            args.learning_rate,\r\n",
      "            global_step,\r\n",
      "            decay_steps,\r\n",
      "            args.end_learning_rate,\r\n",
      "            power=\u001b[34m1.0\u001b[39;49;00m,\r\n",
      "            cycle=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "            name=\u001b[33m'\u001b[39;49;00m\u001b[33mpolynomial_decay_learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate_decay_type [\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m] was not recognized\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m %\r\n",
      "                         args.learning_rate_decay_type)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.warmup_epochs:\r\n",
      "        warmup_lr = (\r\n",
      "            args.learning_rate * tf.cast(global_step, tf.float32) /\r\n",
      "            (steps_per_epoch * args.warmup_epochs))\r\n",
      "        learning_rate = tf.minimum(warmup_lr, learning_rate)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m learning_rate\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_configure_optimizer\u001b[39;49;00m(args, learning_rate):\r\n",
      "    \u001b[33m\"\"\"Configures the optimizer used for training.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m      learning_rate: A scalar or `Tensor` learning rate.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m      An instance of an optimizer.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Raises:\u001b[39;49;00m\r\n",
      "\u001b[33m      ValueError: if args.optimizer is not recognized.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33madadelta\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.AdadeltaOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            rho=args.adadelta_rho,\r\n",
      "            epsilon=args.opt_epsilon)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33madagrad\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.AdagradOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            initial_accumulator_value=args.adagrad_initial_accumulator_value)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.AdamOptimizer(\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            learning_rate,\r\n",
      "            beta1=args.adam_beta1,\r\n",
      "            beta2=args.adam_beta2,\r\n",
      "            epsilon=args.opt_epsilon)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33mftrl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.FtrlOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            learning_rate_power=args.ftrl_learning_rate_power,\r\n",
      "            initial_accumulator_value=args.ftrl_initial_accumulator_value,\r\n",
      "            l1_regularization_strength=args.ftrl_l1,\r\n",
      "            l2_regularization_strength=args.ftrl_l2)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33mmomentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.MomentumOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            momentum=args.momentum,\r\n",
      "            name=\u001b[33m'\u001b[39;49;00m\u001b[33mMomentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33mrmsprop\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.RMSPropOptimizer(\r\n",
      "            learning_rate,\r\n",
      "            decay=args.rmsprop_decay,\r\n",
      "            momentum=args.rmsprop_momentum,\r\n",
      "            epsilon=args.opt_epsilon)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer == \u001b[33m'\u001b[39;49;00m\u001b[33msgd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mOptimizer [\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m] was not recognized\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % args.optimizer)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m optimizer\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_init_fn\u001b[39;49;00m(args):\r\n",
      "    \u001b[33m\"\"\"Returns a function run by the chief worker to warm-start the training.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Note that the init_fn is only run when initializing the model during the very\u001b[39;49;00m\r\n",
      "\u001b[33m    first global step.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m      An init function run by the supervisor.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.finetune_checkpoint_path \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Warn the user if a checkpoint exists in the train_dir. Then we'll be\u001b[39;49;00m\r\n",
      "    \u001b[37m# ignoring the checkpoint anyway.\u001b[39;49;00m\r\n",
      "    \u001b[37m# if tf.train.latest_checkpoint(args.finetune_checkpoint_path):\u001b[39;49;00m\r\n",
      "    \u001b[37m#     tf.logging.info(\u001b[39;49;00m\r\n",
      "    \u001b[37m#         'Ignoring --checkpoint_path because a checkpoint already exists in %s'\u001b[39;49;00m\r\n",
      "    \u001b[37m#         % args.finetune_checkpoint_path)\u001b[39;49;00m\r\n",
      "    \u001b[37m#     return None\u001b[39;49;00m\r\n",
      "\r\n",
      "    exclusions = []\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.checkpoint_exclude_scopes:\r\n",
      "        exclusions = [scope.strip()\r\n",
      "                      \u001b[34mfor\u001b[39;49;00m scope \u001b[35min\u001b[39;49;00m args.checkpoint_exclude_scopes.split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)]\r\n",
      "\r\n",
      "    \u001b[37m# TODO(sguada) variables.filter_variables()\u001b[39;49;00m\r\n",
      "    variables_to_restore = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m var \u001b[35min\u001b[39;49;00m slim.get_model_variables():\r\n",
      "        \u001b[34mfor\u001b[39;49;00m exclusion \u001b[35min\u001b[39;49;00m exclusions:\r\n",
      "            \u001b[34mif\u001b[39;49;00m var.op.name.startswith(exclusion):\r\n",
      "                \u001b[34mbreak\u001b[39;49;00m\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            variables_to_restore.append(var)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m tf.gfile.IsDirectory(args.finetune_checkpoint_path):\r\n",
      "        checkpoint_path = tf.train.latest_checkpoint(\r\n",
      "            args.finetune_checkpoint_path)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        checkpoint_path = args.finetune_checkpoint_path\r\n",
      "\r\n",
      "    tf.logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mFine-tuning from \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % checkpoint_path)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m slim.assign_from_checkpoint_fn(\r\n",
      "        checkpoint_path,\r\n",
      "        variables_to_restore,\r\n",
      "        ignore_missing_vars=args.ignore_missing_vars)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_variables_to_train\u001b[39;49;00m(args):\r\n",
      "    \u001b[33m\"\"\"Returns a list of variables to train.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m      A list of variables to train by the optimizer.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.trainable_scopes \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m tf.trainable_variables()\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        scopes = [scope.strip() \u001b[34mfor\u001b[39;49;00m scope \u001b[35min\u001b[39;49;00m args.trainable_scopes.split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)]\r\n",
      "\r\n",
      "    variables_to_train = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m scope \u001b[35min\u001b[39;49;00m scopes:\r\n",
      "        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\r\n",
      "        variables_to_train.extend(variables)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m variables_to_train\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mevaluation\u001b[39;49;00m(args):\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.dataset_dir:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mYou must supply the dataset directory with --dataset_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    tf.logging.set_verbosity(tf.logging.INFO)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.Graph().as_default():\r\n",
      "        tf_global_step = slim.get_or_create_global_step()\r\n",
      "\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the dataset #\u001b[39;49;00m\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        dataset = dataset_factory.get_dataset(\r\n",
      "            args.dataset_name, \u001b[33m'\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.dataset_dir, args.test_num_data)\r\n",
      "\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the model #\u001b[39;49;00m\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        network_fn = nets_factory.get_network_fn(\r\n",
      "            args.model_name,\r\n",
      "            num_classes=(dataset.num_classes - args.labels_offset),\r\n",
      "            is_training=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m##############################################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Create a dataset provider that loads data from the dataset #\u001b[39;49;00m\r\n",
      "        \u001b[37m##############################################################\u001b[39;49;00m\r\n",
      "        provider = slim.dataset_data_provider.DatasetDataProvider(\r\n",
      "            dataset,\r\n",
      "            shuffle=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "            common_queue_capacity=\u001b[34m2\u001b[39;49;00m * args.eval_batch_size,\r\n",
      "            common_queue_min=args.eval_batch_size)\r\n",
      "        [image, label] = provider.get([\u001b[33m'\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "        label -= args.labels_offset\r\n",
      "\r\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the preprocessing function #\u001b[39;49;00m\r\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\r\n",
      "        preprocessing_name = args.preprocessing_name \u001b[35mor\u001b[39;49;00m args.model_name\r\n",
      "        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33meval_args.use_grayscale : \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.use_grayscale))\r\n",
      "        \r\n",
      "        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\r\n",
      "            preprocessing_name,\r\n",
      "            is_training=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "            use_grayscale=args.use_grayscale)\r\n",
      "\r\n",
      "        eval_image_size = args.image_size \u001b[35mor\u001b[39;49;00m network_fn.default_image_size\r\n",
      "\r\n",
      "        image = image_preprocessing_fn(image, eval_image_size, eval_image_size)\r\n",
      "\r\n",
      "        images, labels = tf.train.batch(\r\n",
      "            [image, label],\r\n",
      "            batch_size=args.eval_batch_size,\r\n",
      "            num_threads=args.num_preprocessing_threads,\r\n",
      "            capacity=\u001b[34m5\u001b[39;49;00m * args.eval_batch_size)\r\n",
      "\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Define the model #\u001b[39;49;00m\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        logits, _ = network_fn(images)\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.quantize:\r\n",
      "            contrib_quantize.create_eval_graph()\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.moving_average_decay:\r\n",
      "            variable_averages = tf.train.ExponentialMovingAverage(\r\n",
      "                args.moving_average_decay, tf_global_step)\r\n",
      "            variables_to_restore = variable_averages.variables_to_restore(\r\n",
      "                slim.get_model_variables())\r\n",
      "            variables_to_restore[tf_global_step.op.name] = tf_global_step\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            variables_to_restore = slim.get_variables_to_restore()\r\n",
      "\r\n",
      "        predictions = tf.argmax(logits, \u001b[34m1\u001b[39;49;00m)\r\n",
      "        labels = tf.squeeze(labels)\r\n",
      "\r\n",
      "        \u001b[37m# Define the metrics:\u001b[39;49;00m\r\n",
      "        names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mAccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: slim.metrics.streaming_accuracy(predictions, labels),\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mRecall_5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: slim.metrics.streaming_recall_at_k(\r\n",
      "                logits, labels, \u001b[34m5\u001b[39;49;00m),\r\n",
      "        })\r\n",
      "\r\n",
      "        \u001b[37m# Print the summaries to screen.\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m name, value \u001b[35min\u001b[39;49;00m names_to_values.items():\r\n",
      "            summary_name = \u001b[33m'\u001b[39;49;00m\u001b[33meval/\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % name\r\n",
      "            op = tf.summary.scalar(summary_name, value, collections=[])\r\n",
      "            op = tf.Print(op, [value], summary_name)\r\n",
      "            tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\r\n",
      "\r\n",
      "        \u001b[37m# TODO(sguada) use num_epochs=1\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.max_eval_num_batches:\r\n",
      "            num_batches = args.max_eval_num_batches\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[37m# This ensures that we make a single pass over all of the data.\u001b[39;49;00m\r\n",
      "            num_batches = math.ceil(\r\n",
      "                dataset.num_samples / \u001b[36mfloat\u001b[39;49;00m(args.eval_batch_size))\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m tf.gfile.IsDirectory(args.train_dir):\r\n",
      "            checkpoint_path = tf.train.latest_checkpoint(args.train_dir)\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            checkpoint_path = args.train_dir\r\n",
      "\r\n",
      "        tf.logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mEvaluating \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % checkpoint_path)\r\n",
      "\r\n",
      "        slim.evaluation.evaluate_once(\r\n",
      "            master=args.master,\r\n",
      "            checkpoint_path=checkpoint_path,\r\n",
      "            logdir=args.train_dir,\r\n",
      "            num_evals=num_batches,\r\n",
      "            eval_op=\u001b[36mlist\u001b[39;49;00m(names_to_updates.values()),\r\n",
      "            variables_to_restore=variables_to_restore)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mexport_inference_graph\u001b[39;49;00m(args):\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.train_dir:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mYou must supply the path to save to with --train_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.is_video_model \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.num_frames:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mNumber of frames must be specified for video models with --num_frames\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    tf.logging.set_verbosity(tf.logging.INFO)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.Graph().as_default() \u001b[34mas\u001b[39;49;00m graph:\r\n",
      "        dataset = dataset_factory.get_dataset(args.dataset_name, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                              args.dataset_dir, args.train_num_data)\r\n",
      "        network_fn = nets_factory.get_network_fn(\r\n",
      "            args.model_name,\r\n",
      "            num_classes=(dataset.num_classes - args.labels_offset),\r\n",
      "            is_training=args.is_training)\r\n",
      "        image_size = args.image_size \u001b[35mor\u001b[39;49;00m network_fn.default_image_size\r\n",
      "        num_channels = \u001b[34m1\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m args.use_grayscale \u001b[34melse\u001b[39;49;00m \u001b[34m3\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.is_video_model:\r\n",
      "            input_shape = [\r\n",
      "                \u001b[34m1\u001b[39;49;00m, args.num_frames, image_size, image_size,\r\n",
      "                num_channels\r\n",
      "            ]\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            input_shape = [\u001b[34m1\u001b[39;49;00m,\r\n",
      "                           image_size, image_size, num_channels]\r\n",
      "        placeholder = tf.placeholder(name=\u001b[33m'\u001b[39;49;00m\u001b[33minput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, dtype=tf.float32,\r\n",
      "                                     shape=input_shape)\r\n",
      "        network_fn(placeholder)\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.quantize:\r\n",
      "            contrib_quantize.create_eval_graph()\r\n",
      "\r\n",
      "        graph_def = graph.as_graph_def()\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.write_text_graphdef:\r\n",
      "            tf.io.write_graph(\r\n",
      "                graph_def,\r\n",
      "                os.path.dirname(args.train_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/inference_graph.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "                os.path.basename(args.train_dir),\r\n",
      "                as_text=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[34mwith\u001b[39;49;00m gfile.GFile(args.train_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/inference_graph.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "                f.write(graph_def.SerializeToString())\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mfreeze_graph\u001b[39;49;00m(args):\r\n",
      "    checkpoint_version = saver_pb2.SaverDef.V2\r\n",
      "    input_graph = args.train_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/inference_graph.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    input_checkpoint = tf.train.latest_checkpoint(args.train_dir)\r\n",
      "    input_binary = \u001b[34mTrue\u001b[39;49;00m\r\n",
      "    output_graph = args.train_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/inference_graph_frozen.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    output_node_names = \u001b[33m'\u001b[39;49;00m\u001b[33mMobilenetV1/Predictions/Reshape_1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    input_saved_model_dir = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    saved_model_tags = \u001b[33m\"\u001b[39;49;00m\u001b[33mserve\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    input_meta_graph = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    variable_names_blacklist = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    variable_names_whitelist = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    initializer_nodes = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    clear_devices = \u001b[34mTrue\u001b[39;49;00m\r\n",
      "    filename_tensor_name = \u001b[33m\"\u001b[39;49;00m\u001b[33msave/Const:0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    restore_op_name = \u001b[33m\"\u001b[39;49;00m\u001b[33msave/restore_all\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    input_saver = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mfreeze_graph input_checkpoint : \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(input_checkpoint))\r\n",
      "\r\n",
      "    fg.freeze_graph(input_graph, input_saver, input_binary,\r\n",
      "                    input_checkpoint, output_node_names,\r\n",
      "                    restore_op_name, filename_tensor_name,\r\n",
      "                    output_graph, clear_devices, initializer_nodes,\r\n",
      "                    variable_names_whitelist, variable_names_blacklist,\r\n",
      "                    input_meta_graph, input_saved_model_dir,\r\n",
      "                    saved_model_tags, checkpoint_version)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\r\n",
      "    args, unknown = parse_args()\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.dataset_dir:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mYou must supply the dataset directory with --dataset_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    tf.logging.set_verbosity(tf.logging.INFO)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.Graph().as_default():\r\n",
      "        \u001b[37m#######################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Config model_deploy #\u001b[39;49;00m\r\n",
      "        \u001b[37m#######################\u001b[39;49;00m\r\n",
      "        deploy_config = model_deploy.DeploymentConfig(\r\n",
      "            num_clones=args.num_clones,\r\n",
      "            clone_on_cpu=args.clone_on_cpu,\r\n",
      "            replica_id=args.task,\r\n",
      "            num_replicas=args.worker_replicas,\r\n",
      "            num_ps_tasks=args.num_ps_tasks)\r\n",
      "\r\n",
      "        \u001b[37m# Create global_step\u001b[39;49;00m\r\n",
      "        \u001b[34mwith\u001b[39;49;00m tf.device(deploy_config.variables_device()):\r\n",
      "            global_step = slim.create_global_step()\r\n",
      "\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the dataset #\u001b[39;49;00m\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        dataset = dataset_factory.get_dataset(\r\n",
      "            args.dataset_name, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.dataset_dir, args.train_num_data)\r\n",
      "\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the network #\u001b[39;49;00m\r\n",
      "        \u001b[37m######################\u001b[39;49;00m\r\n",
      "        network_fn = nets_factory.get_network_fn(\r\n",
      "            args.model_name,\r\n",
      "            num_classes=(dataset.num_classes - args.labels_offset),\r\n",
      "            weight_decay=args.weight_decay,\r\n",
      "            is_training=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Select the preprocessing function #\u001b[39;49;00m\r\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\r\n",
      "        preprocessing_name = args.preprocessing_name \u001b[35mor\u001b[39;49;00m args.model_name\r\n",
      "        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_args.use_grayscale : \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.use_grayscale))\r\n",
      "        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\r\n",
      "            preprocessing_name,\r\n",
      "            is_training=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "            use_grayscale=args.use_grayscale)\r\n",
      "\r\n",
      "        \u001b[37m##############################################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Create a dataset provider that loads data from the dataset #\u001b[39;49;00m\r\n",
      "        \u001b[37m##############################################################\u001b[39;49;00m\r\n",
      "        \u001b[34mwith\u001b[39;49;00m tf.device(deploy_config.inputs_device()):\r\n",
      "            provider = slim.dataset_data_provider.DatasetDataProvider(\r\n",
      "                dataset,\r\n",
      "                num_readers=args.num_readers,\r\n",
      "                common_queue_capacity=\u001b[34m20\u001b[39;49;00m * args.batch_size,\r\n",
      "                common_queue_min=\u001b[34m10\u001b[39;49;00m * args.batch_size)\r\n",
      "            [image, label] = provider.get([\u001b[33m'\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "            label -= args.labels_offset\r\n",
      "\r\n",
      "            train_image_size = args.image_size \u001b[35mor\u001b[39;49;00m network_fn.default_image_size\r\n",
      "\r\n",
      "            image = image_preprocessing_fn(\r\n",
      "                image, train_image_size, train_image_size)\r\n",
      "\r\n",
      "            images, labels = tf.train.batch(\r\n",
      "                [image, label],\r\n",
      "                batch_size=args.batch_size,\r\n",
      "                num_threads=args.num_preprocessing_threads,\r\n",
      "                capacity=\u001b[34m5\u001b[39;49;00m * args.batch_size)\r\n",
      "            labels = slim.one_hot_encoding(\r\n",
      "                labels, dataset.num_classes - args.labels_offset)\r\n",
      "            batch_queue = slim.prefetch_queue.prefetch_queue(\r\n",
      "                [images, labels], capacity=\u001b[34m2\u001b[39;49;00m * deploy_config.num_clones)\r\n",
      "\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Define the model #\u001b[39;49;00m\r\n",
      "        \u001b[37m####################\u001b[39;49;00m\r\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32mclone_fn\u001b[39;49;00m(args, batch_queue):\r\n",
      "            \u001b[33m\"\"\"Allows data parallelism by creating multiple clones of network_fn.\"\"\"\u001b[39;49;00m\r\n",
      "            images, labels = batch_queue.dequeue()\r\n",
      "            logits, end_points = network_fn(images)\r\n",
      "\r\n",
      "            \u001b[37m#############################\u001b[39;49;00m\r\n",
      "            \u001b[37m# Specify the loss function #\u001b[39;49;00m\r\n",
      "            \u001b[37m#############################\u001b[39;49;00m\r\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mAuxLogits\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m end_points:\r\n",
      "                slim.losses.softmax_cross_entropy(\r\n",
      "                    end_points[\u001b[33m'\u001b[39;49;00m\u001b[33mAuxLogits\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], labels,\r\n",
      "                    label_smoothing=args.label_smoothing, weights=\u001b[34m0.4\u001b[39;49;00m,\r\n",
      "                    scope=\u001b[33m'\u001b[39;49;00m\u001b[33maux_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            slim.losses.softmax_cross_entropy(\r\n",
      "                logits, labels, label_smoothing=args.label_smoothing, weights=\u001b[34m1.0\u001b[39;49;00m)\r\n",
      "            \u001b[34mreturn\u001b[39;49;00m end_points\r\n",
      "\r\n",
      "        \u001b[37m# Gather initial summaries.\u001b[39;49;00m\r\n",
      "        summaries = \u001b[36mset\u001b[39;49;00m(tf.get_collection(tf.GraphKeys.SUMMARIES))\r\n",
      "\r\n",
      "        clones = model_deploy.create_clones(\r\n",
      "            deploy_config, clone_fn, [args, batch_queue])\r\n",
      "        first_clone_scope = deploy_config.clone_scope(\u001b[34m0\u001b[39;49;00m)\r\n",
      "        \u001b[37m# Gather update_ops from the first clone. These contain, for example,\u001b[39;49;00m\r\n",
      "        \u001b[37m# the updates for the batch_norm variables created by network_fn.\u001b[39;49;00m\r\n",
      "        update_ops = tf.get_collection(\r\n",
      "            tf.GraphKeys.UPDATE_OPS, first_clone_scope)\r\n",
      "\r\n",
      "        \u001b[37m# Add summaries for end_points.\u001b[39;49;00m\r\n",
      "        end_points = clones[\u001b[34m0\u001b[39;49;00m].outputs\r\n",
      "        \u001b[34mfor\u001b[39;49;00m end_point \u001b[35min\u001b[39;49;00m end_points:\r\n",
      "            x = end_points[end_point]\r\n",
      "            summaries.add(tf.summary.histogram(\u001b[33m'\u001b[39;49;00m\u001b[33mactivations/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + end_point, x))\r\n",
      "            summaries.add(tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33msparsity/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + end_point,\r\n",
      "                                            tf.nn.zero_fraction(x)))\r\n",
      "\r\n",
      "        \u001b[37m# Add summaries for losses.\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m loss \u001b[35min\u001b[39;49;00m tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\r\n",
      "            summaries.add(tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mlosses/\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % loss.op.name, loss))\r\n",
      "\r\n",
      "        \u001b[37m# Add summaries for variables.\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m variable \u001b[35min\u001b[39;49;00m slim.get_model_variables():\r\n",
      "            summaries.add(tf.summary.histogram(variable.op.name, variable))\r\n",
      "\r\n",
      "        \u001b[37m#################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Configure the moving averages #\u001b[39;49;00m\r\n",
      "        \u001b[37m#################################\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.moving_average_decay:\r\n",
      "            moving_average_variables = slim.get_model_variables()\r\n",
      "            variable_averages = tf.train.ExponentialMovingAverage(\r\n",
      "                args.moving_average_decay, global_step)\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            moving_average_variables, variable_averages = \u001b[34mNone\u001b[39;49;00m, \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.quantize_delay >= \u001b[34m0\u001b[39;49;00m:\r\n",
      "            contrib_quantize.create_training_graph(\r\n",
      "                quant_delay=args.quantize_delay)\r\n",
      "\r\n",
      "        \u001b[37m#########################################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Configure the optimization procedure. #\u001b[39;49;00m\r\n",
      "        \u001b[37m#########################################\u001b[39;49;00m\r\n",
      "        \u001b[34mwith\u001b[39;49;00m tf.device(deploy_config.optimizer_device()):\r\n",
      "            learning_rate = _configure_learning_rate(args,\r\n",
      "                                                     dataset.num_samples, global_step)\r\n",
      "            optimizer = _configure_optimizer(args, learning_rate)\r\n",
      "            summaries.add(tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, learning_rate))\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m args.sync_replicas:\r\n",
      "            \u001b[37m# If sync_replicas is enabled, the averaging will be done in the chief\u001b[39;49;00m\r\n",
      "            \u001b[37m# queue runner.\u001b[39;49;00m\r\n",
      "            optimizer = tf.train.SyncReplicasOptimizer(\r\n",
      "                opt=optimizer,\r\n",
      "                replicas_to_aggregate=args.replicas_to_aggregate,\r\n",
      "                total_num_replicas=args.worker_replicas,\r\n",
      "                variable_averages=variable_averages,\r\n",
      "                variables_to_average=moving_average_variables)\r\n",
      "        \u001b[34melif\u001b[39;49;00m args.moving_average_decay:\r\n",
      "            \u001b[37m# Update ops executed locally by trainer.\u001b[39;49;00m\r\n",
      "            update_ops.append(variable_averages.apply(\r\n",
      "                moving_average_variables))\r\n",
      "\r\n",
      "        \u001b[37m# Variables to train.\u001b[39;49;00m\r\n",
      "        variables_to_train = _get_variables_to_train(args)\r\n",
      "\r\n",
      "        \u001b[37m#  and returns a train_tensor and summary_op\u001b[39;49;00m\r\n",
      "        total_loss, clones_gradients = model_deploy.optimize_clones(\r\n",
      "            clones,\r\n",
      "            optimizer,\r\n",
      "            var_list=variables_to_train)\r\n",
      "        \u001b[37m# Add total_loss to summary.\u001b[39;49;00m\r\n",
      "        summaries.add(tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mtotal_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, total_loss))\r\n",
      "\r\n",
      "        \u001b[37m# Create gradient updates.\u001b[39;49;00m\r\n",
      "        grad_updates = optimizer.apply_gradients(clones_gradients,\r\n",
      "                                                 global_step=global_step)\r\n",
      "        update_ops.append(grad_updates)\r\n",
      "\r\n",
      "        update_op = tf.group(*update_ops)\r\n",
      "        \u001b[34mwith\u001b[39;49;00m tf.control_dependencies([update_op]):\r\n",
      "            train_tensor = tf.identity(total_loss, name=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_op\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m# Add the summaries from the first clone. These contain the summaries\u001b[39;49;00m\r\n",
      "        \u001b[37m# created by model_fn and either optimize_clones() or _gather_clone_loss().\u001b[39;49;00m\r\n",
      "        summaries |= \u001b[36mset\u001b[39;49;00m(tf.get_collection(tf.GraphKeys.SUMMARIES,\r\n",
      "                                           first_clone_scope))\r\n",
      "\r\n",
      "        \u001b[37m# Merge all summaries together.\u001b[39;49;00m\r\n",
      "        summary_op = tf.summary.merge(\u001b[36mlist\u001b[39;49;00m(summaries), name=\u001b[33m'\u001b[39;49;00m\u001b[33msummary_op\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m###########################\u001b[39;49;00m\r\n",
      "        \u001b[37m# Kicks off the training. #\u001b[39;49;00m\r\n",
      "        \u001b[37m###########################\u001b[39;49;00m\r\n",
      "        slim.learning.train(\r\n",
      "            train_tensor,\r\n",
      "            logdir=args.train_dir,\r\n",
      "            master=args.master,\r\n",
      "            is_chief=(args.task == \u001b[34m0\u001b[39;49;00m),\r\n",
      "            init_fn=_get_init_fn(args),\r\n",
      "            summary_op=summary_op,\r\n",
      "            number_of_steps=args.max_number_of_steps,\r\n",
      "            log_every_n_steps=args.log_every_n_steps,\r\n",
      "            save_summaries_secs=args.save_summaries_secs,\r\n",
      "            save_interval_secs=args.save_interval_secs,\r\n",
      "            sync_optimizer=optimizer \u001b[34mif\u001b[39;49;00m args.sync_replicas \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m)\r\n",
      "\r\n",
      "    evaluation(args)\r\n",
      "    export_inference_graph(args)\r\n",
      "    freeze_graph(args)\r\n",
      "\r\n",
      "    converter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n",
      "        args.train_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/inference_graph_frozen.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33minput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], [\u001b[33m'\u001b[39;49;00m\u001b[33mMobilenetV1/Predictions/Reshape_1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    tflite_model = converter.convert()\r\n",
      "    \u001b[36mopen\u001b[39;49;00m(args.train_dir + \u001b[33m\"\u001b[39;49;00m\u001b[33m/mobilenetv1_model.tflite\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "         \u001b[33m\"\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).write(tflite_model)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize './src_dir/image_classifier.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. `TensorFlow` estimator를 이용한 training job 생성하기\n",
    "\n",
    "\n",
    "<p><strong><code>sagemaker.tensorflow.TensorFlow</code></strong> estimator는 처음 실행하는 스크립트 위치와 다양한 연계 코드들이 위치한 디렉토리 정보를 찾아서 스크립트를 S3에 upload하고 SageMaker의 training job을 수행하게 됩니다. training job은 학습을 수행한 단위입니다. 학습을 1번 돌리면 training job이 1개 생성됩니다. 몇 가지 중요 파라미터를 아래와 같이 설명드립니다. </p>\n",
    "\n",
    "- **entry_point** : 학습을 처음 실행하는 Python 소스 파일의 절대 또는 상대 경로이며, source_dir이 지정된 경우 entry_point는 source_dir 내 파일이 됩니다.\n",
    "- **source_dir** : 학습에 연계되는 다양한 소스코드 파일이 들어 있는 디렉토리 위치이며, 절대, 상대 경로 또는 S3 URI가 모두 가능하며,source_dir이 S3 URI 인 경우 tar.gz 파일이 됩니다.\n",
    "- **role** : Amazon SageMaker가 사용자를 대신해 작업(예: S3 버킷에서 모델 결과물이라고 하는 훈련 결과 읽기 및 Amazon S3에 훈련 결과 쓰기)을 수행하는 AWS Identity and Access Management(IAM) 역할입니다.\n",
    "- **train_instance_count** : 학습을 수행하는 instance 개수를 정의할 수 있습니다.\n",
    "- **train_instance_type** : 학습을 수행하는 instance 타입을 정의할 수 있습니다.\n",
    "- **train_volume_size** : 학습 인스턴스에 연결할 Amazon Elastic Block Store(Amazon EBS) 스토리지 볼륨의 크기(GB)입니다. File 모드를 사용할 경우 이 값이 훈련 데이터를 충분히 저장할 수 있는 크기여야 합니다(File 모드가 기본값)\n",
    "- **train_use_spot_instances** : 학습에서 SageMaker Managed Spot 인스턴스를 사용할지 여부를 지정합니다. 활성화되면 train_max_wait도 설정해야 합니다.\n",
    "- **train_max_run** : 최대 학습 시간을 설정할 수 있으며, 이 시간이 지나면 Amazon SageMaker는 현재 상태에 관계없이 작업을 종료합니다. (기본값 : 24 * 60 * 60)\n",
    "- **train_max_wait** : SageMaker Managed Spot 인스턴스를 기다리는 초 단위의 시간을 의미하는 것으로, 이 시간이 지나면 Amazon SageMaker는 스팟 인스턴스가 사용 가능해지기를 기다리는 것을 중지하며 결과는 fail이 됩니다.\n",
    "- **framework_version** : 학습에 사용될 특정 Tensorflow 버전을 정의할 수 있습니다.\n",
    "- **py_version** : 컨테이너 환경이 python3일 경우 py3, python2일 경우 py2로 설정하면 됩니다. python2는 지원이 중단되었지만 기존 python2로 구성된 파일들을 지원하기 위해 현재 계속 사용할 수 있습니다.\n",
    "- **hyperparameters** : 학습에 사용할 하이퍼 파라미터를 정의할 수 있으며, 정의된 하이퍼 파라미터 값들은 모두 학습 컨테이너로 전송이 됩니다.\n",
    "\n",
    "<p> 추가적으로 분산/ 멀티 GPU 학습도 가능합니다. SageMaker는 <strong><a href=\"https://github.com/horovod/horovod\" target=\"_blank\" class ='btn-default'>Horovod</a></strong>에 최적화된 환경을 제공하고 있으며, 자세한 내용은 <strong><a href=\"https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/tensorflow#distributed-training\" target=\"_blank\" class ='btn-default'>여기</a></strong>에서 확인이 가능합니다. 이번 학습에서는 분산/멀티 GPU 학습은 제외하였습니다.(단, 기존과 동일하게 python 소스코드에 분산/멀티 학습이 가능하도록 구성 필요) </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>S3에 저장된 TFRecord 파일의 위치를 다시 확인합니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-experiments-us-east-2-322537213286/captured_data/tfrecord'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Dataset 위치\n",
    "inputs= 's3://{}/{}'.format(data_bucket, prefix)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        'dataset_name' : 'captured_dataset',\n",
    "        'model_name' : 'mobilenet_v1_025',\n",
    "        'preprocessing_name' : 'mobilenet_v1',\n",
    "        'learning_rate_decay_type' : 'exponential',    ## \"fixed\", \"exponential\" or \"polynomial\"\n",
    "        'learning_rate_decay_factor' : 0.98,          ## in case of exponential\n",
    "        'learning_rate' : 0.0001,\n",
    "        'image_size' : 224,\n",
    "        'save_summaries_secs' : 300,\n",
    "        'num_epochs_per_decay' : 2.5,\n",
    "        'moving_average_decay' : 0.9999,\n",
    "        'batch_size' : 128,\n",
    "        'max_number_of_steps' : 1000,\n",
    "        'eval_batch_size' : 1000,\n",
    "        'train_num_data' : train_num_data,\n",
    "        'test_num_data': test_num_data,\n",
    "        'finetune_checkpoint_path' : 'fine_tune_checkpoint/model.ckpt-25000',\n",
    "#         'finetune_checkpoint_path' : 'fine_tune_checkpoint/mobilenet_v1_0.25_128.ckpt',\n",
    "#         'checkpoint_exclude_scopes' : 'MobilenetV1/Logits,MobilenetV1/AuxLogits',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"sagemaker\"\n"
     ]
    }
   ],
   "source": [
    "training_job_name = \"{}-img-classifier-training-job\".format(int(time.time()))\n",
    "estimator = TensorFlow(entry_point='image_classifier.py',\n",
    "                       source_dir='src_dir',\n",
    "                       role=role,\n",
    "                       train_instance_count=1,\n",
    "                       train_instance_type='ml.p3.2xlarge',\n",
    "                       train_use_spot_instances=True,  # spot instance 활용\n",
    "                       train_volume_size=400,\n",
    "                       train_max_run=12*60*60,\n",
    "                       train_max_wait=12*60*60,\n",
    "#                        train_instance_type='local_gpu',\n",
    "                       framework_version='1.14.0',\n",
    "                       py_version='py2',\n",
    "                       hyperparameters=hyperparameters\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fit 함수로 학습 시작하기 \n",
    "\n",
    "<p>학습을 시작하는 것은 <strong><code>estimator.fit (training_data_uri)</code></strong>이 호출되는 경우입니다. 여기에서 실제 데이터가 있는 S3의 위치가 입력으로 사용됩니다. <code>fit</code>에서는 <code>training</code>라는 기본 채널을 생성하며, 이 위치의 데이터는 S3에서 실제 컨테이너 환경에서는 <code>SM_CHANNEL_TRAINING</code> 위치로 복사되어 학습에 활용이 가능합니다. <code>fit</code>은 몇 가지 다른 유형의 입력도 허용하는데 자세한 내용은 <strong><a href=\"https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.EstimatorBase.fit\" target=\"_blank\" class ='btn-default'>API 문서</a></strong>를 참고하실 수 있습니다.</p>\n",
    "<p> 학습이 시작되면 Tensorflow 컨테이너에서는 <code>image_classifier.py</code>를 실행되며, <code>Estimator</code>에서 <code>hyperparameters</code> 와 <code>model_dir</code>을 스크립트의 파라미터로 전달합니다. <code>model_dir</code>을 별도로 전달하지 않으며, 기본값은<strong>s3://[DEFAULT_BUCKET]/[TRAINING_JOB_NAME] </strong>이 되며 실제 스크립트 실행은 다음과 같습니다. </p>\n",
    "    \n",
    "```bash\n",
    "python image_classifier.py --model_dir s3://[DEFAULT_BUCKET]/[TRAINING_JOB_NAME]\n",
    "```\n",
    "<p>학습이 완료되면 training job은 Tensorflow serving을 위해 saved model을 S3에 upload합니다.</p>\n",
    "<p><code>fit</code>에서 <strong>wait=True</strong>로 설정할 경우 <strong>Synchronous</strong> 방식으로 동직하게 되며, <strong>wait=False</strong>일 경우 <strong>Aynchronous</strong> 방식으로 동작되어 여러 개의 Training job을 동시에 실행할 수 있습니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_job_name : 1593527851-img-classifier-training-job\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(\n",
    "    inputs = {'training': inputs},\n",
    "    job_name=training_job_name,\n",
    "    logs='All',\n",
    "    wait=False\n",
    ")\n",
    "print(\"training_job_name : {}\".format(training_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Aynchronous</strong>로 진행된 Training job은 아래와 같은 방법으로 진행상황을 실시간으로 확인할 수 있습니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30 14:37:33 Starting - Starting the training job...\n",
      "2020-06-30 14:37:36 Starting - Launching requested ML instances......\n",
      "2020-06-30 14:38:40 Starting - Preparing the instances for training......\n",
      "2020-06-30 14:39:42 Downloading - Downloading input data\n",
      "2020-06-30 14:39:42 Training - Downloading the training image...\n",
      "2020-06-30 14:40:26 Training - Training image download completed. Training in progress..\u001b[34mparser.parse_known_args() : (Namespace(adadelta_rho=0.95, adagrad_initial_accumulator_value=0.1, adam_beta1=0.9, adam_beta2=0.999, batch_size=128, checkpoint_exclude_scopes=None, clone_on_cpu=False, current_host='algo-1', data_config={u'training': {u'TrainingInputMode': u'File', u'RecordWrapperType': u'None', u'S3DistributionType': u'FullyReplicated'}}, dataset_dir='/opt/ml/input/data/training', dataset_name='captured_dataset', end_learning_rate=0.01, eval_batch_size=1000, finetune_checkpoint_path='fine_tune_checkpoint/model.ckpt-25000', ftrl_initial_accumulator_value=0.1, ftrl_l1=0.0, ftrl_l2=0.0, ftrl_learning_rate_power=-0.5, fw_params={}, hosts=[u'algo-1'], ignore_missing_vars=False, image_size=224, is_training=False, is_video_model=False, label_smoothing=0.1, labels_offset=0, learning_rate=0.0001, learning_rate_decay_factor=0.98, learning_rate_decay_type='exponential', log_every_n_steps=10, master='', max_eval_num_batches=None, max_number_of_steps=1000, model_dir='s3://sagemaker-us-east-2-322537213286/1593527851-img-classifier-training-job/model', model_name='mobilenet_v1_025', momentum=0.9, moving_average_decay=0.9999, num_clones=1, num_epochs_per_decay=2.5, num_frames=None, num_gpus='1', num_preprocessing_threads=4, num_ps_tasks=0, num_readers=4, opt_epsilon=1.0, optimizer='rmsprop', output_data_dir='/opt/ml/output/data', output_dir='/opt/ml/output', preprocessing_name='mobilenet_v1', quantize=False, quantize_delay=-1, replicas_to_aggregate=1, rmsprop_decay=0.9, rmsprop_momentum=0.9, save_interval_secs=600, save_summaries_secs=300, sync_replicas=False, task=0, test_num_data=61, train_dir='/opt/ml/model', train_num_data=307, trainable_scopes=None, use_grayscale=False, warmup_epochs=0, weight_decay=4e-05, worker_replicas=1, write_text_graphdef=False), [])\u001b[0m\n",
      "\u001b[34mWARNING: Logging before flag parsing goes to stderr.\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.625874 140082486339328 deprecation_wrapper.py:119] From image_classifier.py:603: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.626128 140082486339328 deprecation_wrapper.py:119] From image_classifier.py:603: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.628395 140082486339328 deprecation.py:323] From image_classifier.py:617: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease switch to tf.train.create_global_step\u001b[0m\n",
      "\u001b[34mfile_pattern : None\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.634073 140082486339328 deprecation_wrapper.py:119] From /opt/ml/code/datasets/captured_dataset.py:72: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.634833 140082486339328 deprecation_wrapper.py:119] From /opt/ml/code/datasets/captured_dataset.py:88: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.635003 140082486339328 deprecation_wrapper.py:119] From /opt/ml/code/datasets/dataset_utils.py:206: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\u001b[0m\n",
      "\u001b[34mlabels_to_names : {0: u'japanese_chin', 1: u'yorkshire_terrier', 2: u'beagle', 3: u'Maine_Coon', 4: u'British_Shorthair', 5: u'Birman', 6: u'english_cocker_spaniel', 7: u'pug', 8: u'boxer', 9: u'Sphynx', 10: u'miniature_pinscher', 11: u'staffordshire_bull_terrier', 12: u'Persian', 13: u'Bombay', 14: u'shiba_inu', 15: u'german_shorthaired', 16: u'keeshond', 17: u'havanese', 18: u'samoyed', 19: u'Egyptian_Mau', 20: u'pomeranian', 21: u'basset_hound', 22: u'chihuahua', 23: u'Siamese', 24: u'leonberger', 25: u'Bengal', 26: u'Ragdoll', 27: u'wheaten_terrier', 28: u'scottish_terrier', 29: u'english_setter', 30: u'american_bulldog', 31: u'newfoundland', 32: u'Russian_Blue', 33: u'saint_bernard', 34: u'american_pit_bull_terrier', 35: u'Abyssinian', 36: u'great_pyrenees'}\u001b[0m\n",
      "\u001b[34mtrain_args.use_grayscale : False\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.635750 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py:246: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.641051 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.641834 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.643356 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:199: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.644376 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.651743 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py:95: __init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.755543 140082486339328 deprecation_wrapper.py:119] From /opt/ml/code/preprocessing/inception_preprocessing.py:206: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.757597 140082486339328 deprecation.py:323] From /opt/ml/code/preprocessing/inception_preprocessing.py:148: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34m`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.762901 140082486339328 deprecation_wrapper.py:119] From /opt/ml/code/preprocessing/inception_preprocessing.py:38: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.766834 140082486339328 deprecation_wrapper.py:119] From /opt/ml/code/preprocessing/inception_preprocessing.py:230: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.806149 140082486339328 deprecation.py:323] From image_classifier.py:666: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.823250 140082486339328 deprecation_wrapper.py:119] From image_classifier.py:693: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.823390 140082486339328 deprecation_wrapper.py:119] From image_classifier.py:693: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:32.823755 140082486339328 deprecation_wrapper.py:119] From /opt/ml/code/deployment/model_deploy.py:192: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:34.434957 140082486339328 deprecation.py:323] From image_classifier.py:689: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\u001b[0m\n",
      "\u001b[34mW0630 14:40:34.442406 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:373: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\n",
      "\u001b[0m\n",
      "\u001b[34mFuture major versions of TensorFlow will allow gradients to flow\u001b[0m\n",
      "\u001b[34minto the labels input on backprop by default.\n",
      "\u001b[0m\n",
      "\u001b[34mSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:34.475625 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:374: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.losses.compute_weighted_loss instead.\u001b[0m\n",
      "\u001b[34mW0630 14:40:34.484251 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:152: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mDeprecated in favor of operator or tf.math.divide.\u001b[0m\n",
      "\u001b[34mW0630 14:40:34.486048 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:154: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.where in 2.0, which has the same broadcast rule as np.where\u001b[0m\n",
      "\u001b[34mW0630 14:40:34.493993 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:121: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.losses.add_loss instead.\u001b[0m\n",
      "\u001b[34mW0630 14:40:34.494479 140082486339328 deprecation_wrapper.py:119] From image_classifier.py:707: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:34.495693 140082486339328 deprecation_wrapper.py:119] From image_classifier.py:708: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:35.187357 140082486339328 deprecation_wrapper.py:119] From image_classifier.py:262: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:35.192728 140082486339328 deprecation_wrapper.py:119] From image_classifier.py:333: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:35.194066 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/moving_averages.py:433: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\u001b[0m\n",
      "\u001b[34mW0630 14:40:38.920543 140082486339328 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/rmsprop.py:119: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m\n",
      "\u001b[34mW0630 14:40:39.885220 140082486339328 deprecation_wrapper.py:119] From image_classifier.py:782: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:39.891866 140082486339328 deprecation_wrapper.py:119] From image_classifier.py:379: The name tf.gfile.IsDirectory is deprecated. Please use tf.io.gfile.isdir instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0630 14:40:39.892062 140082486339328 deprecation_wrapper.py:119] From image_classifier.py:385: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\u001b[0m\n",
      "\u001b[34mI0630 14:40:39.892149 140082486339328 image_classifier.py:385] Fine-tuning from fine_tune_checkpoint/model.ckpt-25000\u001b[0m\n",
      "\u001b[34mW0630 14:40:40.714620 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:742: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease switch to tf.train.MonitoredTrainingSession\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-06-30 14:40:43.988708: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34mW0630 14:40:44.446440 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1282: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse standard file APIs to check for files with this prefix.\u001b[0m\n",
      "\u001b[34mI0630 14:40:44.448508 140082486339328 saver.py:1286] Restoring parameters from fine_tune_checkpoint/model.ckpt-25000\u001b[0m\n",
      "\u001b[34mI0630 14:40:44.715303 140082486339328 session_manager.py:500] Running local_init_op.\u001b[0m\n",
      "\u001b[34mI0630 14:40:44.798985 140082486339328 session_manager.py:502] Done running local_init_op.\u001b[0m\n",
      "\u001b[34mI0630 14:40:48.884896 140082486339328 learning.py:754] Starting Session.\u001b[0m\n",
      "\u001b[34mI0630 14:40:48.983082 140076591867648 supervisor.py:1117] Saving checkpoint to path /opt/ml/model/model.ckpt\u001b[0m\n",
      "\u001b[34mI0630 14:40:48.988342 140082486339328 learning.py:768] Starting Queues.\u001b[0m\n",
      "\u001b[34mI0630 14:40:49.819207 140076583474944 supervisor.py:1099] global_step/sec: 0\u001b[0m\n",
      "\u001b[34mI0630 14:40:57.913923 140076575082240 supervisor.py:1050] Recording summary at step 1.\u001b[0m\n",
      "\u001b[34mI0630 14:41:00.043952 140082486339328 learning.py:507] global step 10: loss = 1.6374 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:02.392544 140082486339328 learning.py:507] global step 20: loss = 1.6063 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:04.699541 140082486339328 learning.py:507] global step 30: loss = 1.5311 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:06.982454 140082486339328 learning.py:507] global step 40: loss = 1.5002 (0.220 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:09.305571 140082486339328 learning.py:507] global step 50: loss = 1.6618 (0.222 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:11.601419 140082486339328 learning.py:507] global step 60: loss = 1.5942 (0.221 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:13.916731 140082486339328 learning.py:507] global step 70: loss = 1.6709 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:16.261409 140082486339328 learning.py:507] global step 80: loss = 1.5874 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:18.585613 140082486339328 learning.py:507] global step 90: loss = 1.5423 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:20.938309 140082486339328 learning.py:507] global step 100: loss = 1.5892 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:23.258963 140082486339328 learning.py:507] global step 110: loss = 1.5713 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:25.563843 140082486339328 learning.py:507] global step 120: loss = 1.6738 (0.232 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:27.885701 140082486339328 learning.py:507] global step 130: loss = 1.5046 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:30.212833 140082486339328 learning.py:507] global step 140: loss = 1.6415 (0.226 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:32.569590 140082486339328 learning.py:507] global step 150: loss = 1.6079 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:34.926152 140082486339328 learning.py:507] global step 160: loss = 1.3854 (0.245 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:37.244538 140082486339328 learning.py:507] global step 170: loss = 1.6407 (0.242 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:39.596263 140082486339328 learning.py:507] global step 180: loss = 1.5134 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:41.916563 140082486339328 learning.py:507] global step 190: loss = 1.5805 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:44.225821 140082486339328 learning.py:507] global step 200: loss = 1.5567 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:46.548387 140082486339328 learning.py:507] global step 210: loss = 1.5965 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:48.895565 140082486339328 learning.py:507] global step 220: loss = 1.5604 (0.242 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:51.291641 140082486339328 learning.py:507] global step 230: loss = 1.6167 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:53.632936 140082486339328 learning.py:507] global step 240: loss = 1.5801 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:55.963721 140082486339328 learning.py:507] global step 250: loss = 1.4677 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:41:58.305529 140082486339328 learning.py:507] global step 260: loss = 1.5173 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:00.629092 140082486339328 learning.py:507] global step 270: loss = 1.4973 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:03.003674 140082486339328 learning.py:507] global step 280: loss = 1.5045 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:05.328855 140082486339328 learning.py:507] global step 290: loss = 1.7900 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:07.635957 140082486339328 learning.py:507] global step 300: loss = 1.5482 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:09.986495 140082486339328 learning.py:507] global step 310: loss = 1.5848 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:12.306786 140082486339328 learning.py:507] global step 320: loss = 1.4330 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:14.627557 140082486339328 learning.py:507] global step 330: loss = 1.4477 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:16.950685 140082486339328 learning.py:507] global step 340: loss = 1.5925 (0.242 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:19.296969 140082486339328 learning.py:507] global step 350: loss = 1.5870 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:21.637556 140082486339328 learning.py:507] global step 360: loss = 1.6486 (0.223 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:23.967299 140082486339328 learning.py:507] global step 370: loss = 1.5358 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:26.300816 140082486339328 learning.py:507] global step 380: loss = 1.4590 (0.221 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:28.618804 140082486339328 learning.py:507] global step 390: loss = 1.5677 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:30.947973 140082486339328 learning.py:507] global step 400: loss = 1.5709 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:33.305541 140082486339328 learning.py:507] global step 410: loss = 1.5973 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:35.603872 140082486339328 learning.py:507] global step 420: loss = 1.4743 (0.226 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:37.959235 140082486339328 learning.py:507] global step 430: loss = 1.5165 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:40.303167 140082486339328 learning.py:507] global step 440: loss = 1.5618 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:42.616441 140082486339328 learning.py:507] global step 450: loss = 1.5395 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:44.944564 140082486339328 learning.py:507] global step 460: loss = 1.6587 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:47.297128 140082486339328 learning.py:507] global step 470: loss = 1.5309 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:49.626976 140082486339328 learning.py:507] global step 480: loss = 1.5343 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:51.986641 140082486339328 learning.py:507] global step 490: loss = 1.4684 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:54.299154 140082486339328 learning.py:507] global step 500: loss = 1.5192 (0.232 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:56.640742 140082486339328 learning.py:507] global step 510: loss = 1.3962 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:42:58.992089 140082486339328 learning.py:507] global step 520: loss = 1.5835 (0.232 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:01.369877 140082486339328 learning.py:507] global step 530: loss = 1.6568 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:03.692312 140082486339328 learning.py:507] global step 540: loss = 1.5479 (0.232 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:06.048180 140082486339328 learning.py:507] global step 550: loss = 1.5104 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:08.385900 140082486339328 learning.py:507] global step 560: loss = 1.6381 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:10.741489 140082486339328 learning.py:507] global step 570: loss = 1.5073 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:13.081311 140082486339328 learning.py:507] global step 580: loss = 1.6862 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:15.401196 140082486339328 learning.py:507] global step 590: loss = 1.5771 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:17.716711 140082486339328 learning.py:507] global step 600: loss = 1.5686 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:20.048244 140082486339328 learning.py:507] global step 610: loss = 1.5022 (0.240 sec/step)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0630 14:43:22.377265 140082486339328 learning.py:507] global step 620: loss = 1.5918 (0.232 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:24.761236 140082486339328 learning.py:507] global step 630: loss = 1.3972 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:27.075521 140082486339328 learning.py:507] global step 640: loss = 1.4541 (0.227 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:29.395348 140082486339328 learning.py:507] global step 650: loss = 1.4633 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:31.693792 140082486339328 learning.py:507] global step 660: loss = 1.5071 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:34.008377 140082486339328 learning.py:507] global step 670: loss = 1.5890 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:36.325944 140082486339328 learning.py:507] global step 680: loss = 1.4902 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:38.637083 140082486339328 learning.py:507] global step 690: loss = 1.5437 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:40.972358 140082486339328 learning.py:507] global step 700: loss = 1.5472 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:43.321620 140082486339328 learning.py:507] global step 710: loss = 1.4978 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:45.625113 140082486339328 learning.py:507] global step 720: loss = 1.5666 (0.224 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:47.968605 140082486339328 learning.py:507] global step 730: loss = 1.5937 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:50.289273 140082486339328 learning.py:507] global step 740: loss = 1.5310 (0.232 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:52.621560 140082486339328 learning.py:507] global step 750: loss = 1.3607 (0.242 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:54.962044 140082486339328 learning.py:507] global step 760: loss = 1.4370 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:57.286806 140082486339328 learning.py:507] global step 770: loss = 1.5312 (0.229 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:43:59.614504 140082486339328 learning.py:507] global step 780: loss = 1.4409 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:01.943947 140082486339328 learning.py:507] global step 790: loss = 1.6055 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:04.278285 140082486339328 learning.py:507] global step 800: loss = 1.6177 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:06.610661 140082486339328 learning.py:507] global step 810: loss = 1.4485 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:08.944931 140082486339328 learning.py:507] global step 820: loss = 1.5908 (0.231 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:11.257390 140082486339328 learning.py:507] global step 830: loss = 1.3951 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:13.604515 140082486339328 learning.py:507] global step 840: loss = 1.4209 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:15.930985 140082486339328 learning.py:507] global step 850: loss = 1.4721 (0.242 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:18.227602 140082486339328 learning.py:507] global step 860: loss = 1.6154 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:20.580378 140082486339328 learning.py:507] global step 870: loss = 1.3368 (0.227 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:22.895615 140082486339328 learning.py:507] global step 880: loss = 1.5331 (0.233 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:25.228341 140082486339328 learning.py:507] global step 890: loss = 1.6160 (0.237 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:27.577024 140082486339328 learning.py:507] global step 900: loss = 1.5715 (0.230 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:29.869291 140082486339328 learning.py:507] global step 910: loss = 1.5596 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:32.219052 140082486339328 learning.py:507] global step 920: loss = 1.5254 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:34.551369 140082486339328 learning.py:507] global step 930: loss = 1.4809 (0.238 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:36.882036 140082486339328 learning.py:507] global step 940: loss = 1.5500 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:39.230170 140082486339328 learning.py:507] global step 950: loss = 1.4904 (0.236 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:41.540707 140082486339328 learning.py:507] global step 960: loss = 1.5535 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:43.865060 140082486339328 learning.py:507] global step 970: loss = 1.5827 (0.234 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:46.195214 140082486339328 learning.py:507] global step 980: loss = 1.5177 (0.225 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:48.520095 140082486339328 learning.py:507] global step 990: loss = 1.4304 (0.235 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:50.833445 140082486339328 learning.py:507] global step 1000: loss = 1.5439 (0.228 sec/step)\u001b[0m\n",
      "\u001b[34mI0630 14:44:50.834151 140082486339328 learning.py:777] Stopping Training.\u001b[0m\n",
      "\u001b[34mI0630 14:44:50.834377 140082486339328 learning.py:785] Finished training! Saving model to disk.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/writer/writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.\n",
      "  warnings.warn(\"Attempting to use a closed FileWriter. \"\u001b[0m\n",
      "\u001b[34mW0630 14:44:51.811903 140082486339328 deprecation.py:323] From image_classifier.py:418: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease switch to tf.train.get_or_create_global_step\u001b[0m\n",
      "\u001b[34mfile_pattern : None\u001b[0m\n",
      "\u001b[34mlabels_to_names : {0: u'japanese_chin', 1: u'yorkshire_terrier', 2: u'beagle', 3: u'Maine_Coon', 4: u'British_Shorthair', 5: u'Birman', 6: u'english_cocker_spaniel', 7: u'pug', 8: u'boxer', 9: u'Sphynx', 10: u'miniature_pinscher', 11: u'staffordshire_bull_terrier', 12: u'Persian', 13: u'Bombay', 14: u'shiba_inu', 15: u'german_shorthaired', 16: u'keeshond', 17: u'havanese', 18: u'samoyed', 19: u'Egyptian_Mau', 20: u'pomeranian', 21: u'basset_hound', 22: u'chihuahua', 23: u'Siamese', 24: u'leonberger', 25: u'Bengal', 26: u'Ragdoll', 27: u'wheaten_terrier', 28: u'scottish_terrier', 29: u'english_setter', 30: u'american_bulldog', 31: u'newfoundland', 32: u'Russian_Blue', 33: u'saint_bernard', 34: u'american_pit_bull_terrier', 35: u'Abyssinian', 36: u'great_pyrenees'}\u001b[0m\n",
      "\u001b[34meval_args.use_grayscale : False\u001b[0m\n",
      "\u001b[34mW0630 14:44:51.929450 140082486339328 deprecation_wrapper.py:119] From /opt/ml/code/preprocessing/inception_preprocessing.py:301: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\n",
      "\u001b[0m\n",
      "\u001b[34mI0630 14:44:51.941334 140082486339328 regularizers.py:98] Scale of 0 disables regularizer.\u001b[0m\n",
      "\u001b[34mW0630 14:44:53.000659 140082486339328 deprecation.py:323] From image_classifier.py:489: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.\u001b[0m\n",
      "\u001b[34mW0630 14:44:53.015746 140082486339328 deprecation.py:323] From image_classifier.py:491: streaming_recall_at_k (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed after 2016-11-08.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease use `streaming_sparse_recall_at_k`, and reshape labels from [batch_size] to [batch_size, 1].\u001b[0m\n",
      "\u001b[34mW0630 14:44:53.017632 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py:2166: streaming_mean (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease switch to tf.metrics.mean\u001b[0m\n",
      "\u001b[34mW0630 14:44:53.032411 140082486339328 deprecation.py:323] From image_classifier.py:498: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\u001b[0m\n",
      "\u001b[34m```python\n",
      "    sess = tf.compat.v1.Session()\n",
      "    with sess.as_default():\n",
      "        tensor = tf.range(10)\n",
      "        print_op = tf.print(tensor)\n",
      "        with tf.control_dependencies([print_op]):\n",
      "          out = tf.add(tensor, tensor)\n",
      "        sess.run(out)\n",
      "    ```\u001b[0m\n",
      "\u001b[34mAdditionally, to use tf.print in python 2.7, users must make sure to import\u001b[0m\n",
      "\u001b[34mthe following:\n",
      "\n",
      "  `from __future__ import print_function`\n",
      "\u001b[0m\n",
      "\u001b[34mI0630 14:44:53.037187 140082486339328 image_classifier.py:514] Evaluating /opt/ml/model/model.ckpt-1000\u001b[0m\n",
      "\u001b[34mI0630 14:44:53.167807 140082486339328 evaluation.py:255] Starting evaluation at 2020-06-30T14:44:53Z\u001b[0m\n",
      "\u001b[34mI0630 14:44:53.403904 140082486339328 monitored_session.py:240] Graph was finalized.\u001b[0m\n",
      "\u001b[34mI0630 14:44:53.407424 140082486339328 saver.py:1286] Restoring parameters from /opt/ml/model/model.ckpt-1000\u001b[0m\n",
      "\u001b[34mI0630 14:44:53.860552 140082486339328 session_manager.py:500] Running local_init_op.\u001b[0m\n",
      "\u001b[34mI0630 14:44:53.889167 140082486339328 session_manager.py:502] Done running local_init_op.\u001b[0m\n",
      "\u001b[34mW0630 14:44:54.086289 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mI0630 14:44:55.992909 140082486339328 evaluation.py:167] Evaluation [1/1]\u001b[0m\n",
      "\u001b[34meval/Recall_5[0.966]\u001b[0m\n",
      "\u001b[34meval/Accuracy[0.884]\u001b[0m\n",
      "\u001b[34mI0630 14:44:56.146529 140082486339328 evaluation.py:275] Finished evaluation at 2020-06-30-14:44:56\u001b[0m\n",
      "\u001b[34mfile_pattern : None\u001b[0m\n",
      "\u001b[34mlabels_to_names : {0: u'japanese_chin', 1: u'yorkshire_terrier', 2: u'beagle', 3: u'Maine_Coon', 4: u'British_Shorthair', 5: u'Birman', 6: u'english_cocker_spaniel', 7: u'pug', 8: u'boxer', 9: u'Sphynx', 10: u'miniature_pinscher', 11: u'staffordshire_bull_terrier', 12: u'Persian', 13: u'Bombay', 14: u'shiba_inu', 15: u'german_shorthaired', 16: u'keeshond', 17: u'havanese', 18: u'samoyed', 19: u'Egyptian_Mau', 20: u'pomeranian', 21: u'basset_hound', 22: u'chihuahua', 23: u'Siamese', 24: u'leonberger', 25: u'Bengal', 26: u'Ragdoll', 27: u'wheaten_terrier', 28: u'scottish_terrier', 29: u'english_setter', 30: u'american_bulldog', 31: u'newfoundland', 32: u'Russian_Blue', 33: u'saint_bernard', 34: u'american_pit_bull_terrier', 35: u'Abyssinian', 36: u'great_pyrenees'}\u001b[0m\n",
      "\u001b[34mI0630 14:44:56.149089 140082486339328 regularizers.py:98] Scale of 0 disables regularizer.\u001b[0m\n",
      "\u001b[34mfreeze_graph input_checkpoint : /opt/ml/model/model.ckpt-1000\u001b[0m\n",
      "\u001b[34mW0630 14:44:57.126383 140082486339328 deprecation_wrapper.py:119] From /opt/ml/code/freeze_graph.py:165: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
      "\u001b[0m\n",
      "\u001b[34mI0630 14:44:57.262871 140082486339328 saver.py:1286] Restoring parameters from /opt/ml/model/model.ckpt-1000\u001b[0m\n",
      "\u001b[34mW0630 14:44:57.491439 140082486339328 deprecation.py:323] From /opt/ml/code/freeze_graph.py:234: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.compat.v1.graph_util.convert_variables_to_constants`\u001b[0m\n",
      "\u001b[34mW0630 14:44:57.491636 140082486339328 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.compat.v1.graph_util.extract_sub_graph`\u001b[0m\n",
      "\u001b[34mI0630 14:44:57.608764 140082486339328 graph_util_impl.py:311] Froze 137 variables.\u001b[0m\n",
      "\u001b[34mI0630 14:44:57.628484 140082486339328 graph_util_impl.py:364] Converted 137 variables to const ops.\u001b[0m\n",
      "\u001b[34mWARNING: Logging before flag parsing goes to stderr.\u001b[0m\n",
      "\u001b[34mW0630 14:45:01.467588 139706723469056 training.py:183] Your model will NOT be servable with SageMaker TensorFlow Serving container.The model artifact was not saved in the TensorFlow SavedModel directory structure:\u001b[0m\n",
      "\u001b[34mhttps://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\u001b[0m\n",
      "\n",
      "2020-06-30 14:45:36 Uploading - Uploading generated training model\n",
      "2020-06-30 14:45:36 Completed - Training job completed\n",
      "Training seconds: 361\n",
      "Billable seconds: 108\n",
      "Managed Spot Training savings: 70.1%\n"
     ]
    }
   ],
   "source": [
    "sm_sess = sagemaker.Session()\n",
    "sm_sess.logs_for_job(estimator.latest_training_job.name, wait=True, log_type='All')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>학습이 모두 완료된 다음에 S3에서 모델 산출물을 SageMaker Notebook 환경으로 내려받습니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-322537213286/1593527851-img-classifier-training-job/\n",
      "                           PRE debug-output/\n",
      "                           PRE output/\n",
      "                           PRE source/\n"
     ]
    }
   ],
   "source": [
    "artifacts_dir = estimator.model_dir.replace('model','')\n",
    "print(artifacts_dir)\n",
    "!aws s3 ls --human-readable {artifacts_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-322537213286/1593527851-img-classifier-training-job/output/\n",
      "2020-06-30 14:45:34    7.8 MiB model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_dir=artifacts_dir+'output/'\n",
    "print(model_dir)\n",
    "!aws s3 ls --human-readable {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./model_result/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-2-322537213286/1593527851-img-classifier-training-job/output/model.tar.gz to model_result/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import json , os\n",
    "\n",
    "path = './model_result'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "!aws s3 cp {model_dir}model.tar.gz {path}/model.tar.gz\n",
    "!tar -xzf {path}/model.tar.gz -C {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>최종 결과물에는 tflite를 생성할 수 있도록 했습니다. 압축을 푼 다음 tflite 를 다시 활용하기 위해 S3에 파일을 upload 합니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: img_datasets/labels.txt to s3://sagemaker-us-east-2-322537213286/workshop_final_result/labels.txt\n",
      "upload: model_result/mobilenetv1_model.tflite to s3://sagemaker-us-east-2-322537213286/workshop_final_result/mobilenetv1_model.tflite\n"
     ]
    }
   ],
   "source": [
    "final_result = 's3://{}/{}'.format(bucket, 'workshop_final_result')\n",
    "\n",
    "!aws s3 cp ./img_datasets/labels.txt {final_result}/labels.txt\n",
    "!aws s3 cp {path}/mobilenetv1_model.tflite {final_result}/mobilenetv1_model.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p></p>\n",
    "<p>Amazon SageMaker에서 모든 학습을 완료하였습니다. 이제 tflite를 이용하여 AI Chip에서 활용할 수 있도록 Convertor를 수행합니다. 이 작업은 Cloud9에서 수행합니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
